{
   "texts": [
      "6YC INFORMATION \nSTATION\nOPERATION MANUAL\n\u0003 Read this manual carefully before operation. 6YC-2819U-70-E0\n00_6YC_owners_En.book  Page -4 Sunday, March 1, 2015  3:36 PMRead this manual carefully before operating the meter. Keep this manual onboard in a\nwaterproof bag when boating. This manual should stay with the product if it is sold.\n00_6YC_owners_En.book  Page -3 Sunday, March 1, 2015  3:36 PMTo the owner\nTo the owner\nThank you for selecting this product. This Operation Manual contains information needed for\nproper operation. A thorough understanding of these simple instructions will help you obtain\nmaximum enjoyment from your new Yamaha. If you have any question about the operation\nof the meter, please consult a Yamaha dealer.\nIn this Operation Manual particularly import ant information is distinguished in the following\nways.\n\u0003 This is the safety alert symbol. It is used to alert you to potential personal injury hazards.\nObey all safety messages that follow this symbol to avoid possible injury or death.\nWARNING\nA WARNING indicates a hazardous situation which, if not avoid ed, could result in\ndeath or serious injury.\nNOTICE\nA NOTICE indicates special pr ecautions that must be take n to avoid damage to the\noutboard motor or other property.\nTIP:\nA TIP provides key information to make procedures easier or clearer.\nInformation about the Operation Manual\nYamaha continually seeks advancements in product design and quality. Therefore, while this\nmanual contains the most current product information available at the time of printing, there\nmay be minor discrepancies between the product and this manual.\n6YC Information Station\nOPERATION MANUAL\n\u00a92015 by Yamaha Motor Co., LTD.\n1st Edition, March 2015\nAll rights reserved.\nAny reprinting or unauthorized use without the \nwritten permission of \nYamaha Motor Co., LTD. \nis expressly prohibited.\nPrinted in 8\u0011S\u0011A\u0011\n00_6YC_owners_En.book  Page -2 Sunday, March 1, 2015  3:36 PMTo the owner\nThe 6YC Information Station  e n a b l e s  t he \ndisplay of information through digital commu-\nnication with the engine.\nThe description of optional parts in this man-\nual, such as the speed sensor, cooling water\npressure sensor, Multi sensor, GPS, and\nY-COP is based on the assumption that the\nparts are installed and set up on the out-\nboard motor.\nThis product is compatible with the following\noutboard motors.\n4-stroke models:\nFuel injection models only\n00_6YC_owners_En.book  Page -1 Sunday, March 1, 2015  3:36 PMContents\nAlert Notifications ...................1\nAlert Notifications ................................. 1\nMaintenance Notifications .................... 2\nGeneral information................3\nList of abbreviations ............................. 3\nMeter unit.................................4\nInitial Setting ...........................6\nConfiguring the number of outboard \nmotors ............................................... 6\nConfiguring the fuel tank sensors......... 7\nCalibrating the fuel tank sensors .......... 7\nAdjusting the trim level to zero ........... 10\nMain screen ...........................11\nSwitching the screens ........................ 11\nCOMBO.............................................. 12\nENGINE.............................................. 13\nBOAT.................................................. 13\nTROLL................................................ 13\nAlert display area................................ 14\nOverheat alert ....................................... 14\nLow oil pressure alert ............................ 14\nWater in fuel alert .................................. 15\nLow battery voltage alert ....................... 15\nCheck engine alert/maintenance \nalert ................................................... 15\nY-COP display area............................ 16\nBasic display area .............................. 16\nTachometer ........................................... 16\nTrim meter............................................. 16\nFuel gauge ............................................ 16\nSpecific selection area ....................... 17\nAdjusting the trolling speed ................ 17\nMENU screen.........................19\nSwitching the screens......................... 19\nFeatures ............................................. 20\nMenu items ......................................... 21\nResetting \u201cTrip\u201d information (Trip)......... 21\nManaging the maintenance schedule \n(Maintenance) ................................... 21\nChanging backlight settings \n(Brightness)....................................... 22\nCustomizing the screen (Favorites) ...... 23\nSetting the background color (Color) .... 31\nAdjusting the clock (Clock).................... 31\nSetting the displayed units (Units)......... 32\nSetting the tanks (Tank Set).................. 33\nInitializing the meter (Reset) ................. 34\nAdjusting the trim level to zero \n(Trim level) ........................................ 34\nCalibration of fuel consumption \n(Fuel Flow) ........................................ 35\nTrouble Codes....................................... 35\nAppendix ...............................37\nRequirements for installation .............. 37\nProper care of the instrument ............. 37\nTemplate (actual size) ........................ 38\n00_6YC_owners_En.book  Page 0  Sunday, March 1, 2015  3:36 PM1\nAlert Notifications\nThe multifunction display will notify the operator when engine abnormalities occur by display-\ning a pop-up window and alert icon. A pop-up window will also be displayed when specific\nalert conditions occur.\nWhen events requiring multiple pop-up windows occur, the pop-up window with the highest\ndegree of urgency is displayed first. Press the [SET] button to display pop-up windows in the\norder of urgency, from a higher degree of urgency to a lower degree of urgency.\nThere are 2 types of pop-up win dows, \u201cAlert Notifications\u201d and \u201cMaintenance Notifications\u201d,\neach displayed with its own color.\nAlert Notifications: Red\nMaintenance Notifications: Yellow\nAlert Notifications\nFollow the instructions for responding to each specific alert. For further information, see \u201cAlert\ndisplay area\u201d (page 14).\nOverheat alert\nDisplayed when the engine tem-\nperature rises too high.\nLow oil pressure alert\nDisplayed when the oil pressure \ndrops too low.\nWater in fuel alert\nDisplayed when water has accu-\nmulated in the water separator \n(fuel filter).\nLow battery voltage alert\nDisplayed when the battery volt-\nage drops low.\nCheck engine alert\nDisplayed when the engine mal-\nfunctions.\nAlert Notifications\n00_6YC_owners_En.book  Page 1  Sunday, March 1, 2015  3:36 PM2\nMaintenance Notifications\nIf this alert is displayed, carry out the maintenance and reset the maintenance schedule. See\n\u201cManaging the maintenance schedule (Maintenance)\u201d on how to reset the maintenance\nschedule (page 21).\nMaintenance alert\nDisplayed when a scheduled \nmaintenance is overdue.\nAlert Notifications\n00_6YC_owners_En.book  Page 2  Sunday, March 1, 2015  3:36 PM3\nGeneral information\nList of abbreviations\nThe following abbreviations are used in this manual.\nAbbreviations Descriptions\nABYC American Boat and Yacht Council\nEUR Europe\nF Forward\nGPS Global Positioning System\nN Neutral\nRR e v e r s e\nY-COP Yamaha Customer Outboard Protection\nGeneral information\n00_6YC_owners_En.book  Page 3  Sunday, March 1, 2015  3:36 PM4\nMeter unit\nSET\nMENU\nCANCEL\n2\n1\n3\n6\n5\n4\nName Explanation of function\n1S t a t u s  b a r\nDisplays the current time and the name of the currently \nselected main screen.\n(If no GPS is connected, the clock and the \u201cGPS\u201d icon will \nnot be displayed.)\n\u2022T h e  c l o c k  s h o w s  t h e  t i m e  i n  2 4 - h o u r  f o r m a t .\n2M u l t i f u n c t i o n  d i s p l a yDisplays engine information, boat/environment information, \nsetting menus, etc. \n3D i r e c t i o n a l  k e y p a d\n[LI] (Up/Down) buttons\n\u2022S w i t c h e s  t h e  m a i n  s c r e e n\n\u2022M o v e s  c u r s o r  ( s e l e c t i o n )  o n  t h e  \u201c M E N U \u201d  s c r e e n\n\u2022A d j u s t s  t h e  t r o l l i n g  s p e e d\n[HJ] (Left/Right) buttons\n\u2022S w i t c h e s  t h e  m a i n  s c r e e n\n\u2022M o v e s  c u r s o r  ( s e l e c t i o n )  o n  t h e  \u201c M E N U \u201d  s c r e e n\n4[ S E T ]  b u t t o n\n\u2022R e c o r d s  t h e  s e t t i n g\n\u2022M o v e s  t h e  \u201c M E N U \u201d  s c r e e n  c u r s o r  t o  t h e  r i g h t\n\u2022M o v e s  f r o m  t h e  m a i n  s c r e e n  t o  t h e  \u201c B R I G H T N E S S \u201d  s c r e e n\n5[ C A N C E L ]  b u t t o n\n\u2022C a n c e l s  t h e  s e t t i n g\n\u2022M o v e s  f r o m  a n y  s e t t i n g  s c r e e n  t o  t h e  \u201c M E N U \u201d  s c r e e n\n\u2022M o v e s  f r o m  t h e  \u201c M E N U \u201d  s c r e e n  t o  t h e  m a i n  s c r e e n\n\u2022M o v e s  t h e  \u201c M E N U \u201d  s c r e e n  c u r s o r  t o  t h e  l e f t\n\u2022C a n c e l s  t h e  t r o l l i n g  m o d e\n\u2022W h e n  o n  t h e  m a i n  s c r e e n ,  m o v e s  t o  t h e  \u201c T R I P \u201d  s c r e e n\nMeter unit\n00_6YC_owners_En.book  Page 4  Sunday, March 1, 2015  3:36 PM5\n6[ M E N U ]  b u t t o n\n\u2022M o v e s  f r o m  a n y  s c r e e n  t o  t h e  t o p  \u201c M E N U \u201d  s c r e e n\n\u2022M o v e s  f r o m  t h e  \u201c M E N U \u201d  s c r e e n  o r  s e t t i n g  s c r e e n  t o  t h e\nmain screen\nName Explanation of function\nMeter unit\n00_6YC_owners_En.book  Page 5  Sunday, March 1, 2015  3:36 PM6\nInitial Setting\nThe 6YC Information Station must be initial-\nized before its first use or after a reset oper-\nation.\nTIP:\nConsult a Yamaha dealer when you perform\nthe initial setting.\nConfiguring the number of outboard \nmotors\n1. Turn the engine start switch to ON. The\n\u201cINITIAL SETTING\u201d screen is displayed.\nTIP:\nFor a twin engine application, turn the PORT\nengine start switch to the ON position first.\nThe system will recognize the engine turned\non first as the PORT engine.\n2. Use the [ LI] (Up/Down) buttons on the\ndirectional keypad to  select the number\nof outboard motors on your boat.\n3. Set using the [SET] button.\n4. The main screen is displayed.\n1: Single-engine application\n2: Twin-engine application\nInitial Setting\n00_6YC_owners_En.book  Page 6  Sunday, March 1, 2015  3:36 PM7\nConfiguring the fuel tank sensors\nA fuel tank sensor calibration is required af-\nter configuring the fuel tank parameters.\nPerform a fuel tank sensor calibration for all\nthe tanks that are conf igured as \u201cFUEL\u201d in\nthe \u201cTank Type\u201d setting. Otherwise, Availa-\nble Fuel (AVAL) will not be displayed.\n1. Press the [MENU] button in the main\nscreen to display the \u201cMENU\u201d screen.\n2. Use the directional keypad and the [SET]\nbutton to display \u201cTank Set\u201d in the\n\u201cMENU\u201d screen.\n3. Use the [ HJ] (Left/Right) buttons on the\ndirectional keypad to select the tank\nnumber.\nSet using the [SET] button.\n4. Use the directional keypad to select from\noptions in the \u201cType\u201d and \u201cSensor\u201d lists.\nSet using the [SET] button.\nTIP:\nA fuel tank sensor calibration is required if\nthe tank is configured as \u201cFUEL\u201d in the\n\u201cType\u201d setting. For details, see \u201cCalibrating\nthe fuel tank sensors\u201d (page 7).\nCalibrating the fuel tank sensors\nPerform the fuel tank sensor calibration\nwhen the tank is empty.\nOld calibration data will be deleted if the sen-\nsor type or tank capacity is changed.\n1. After selecting item s from the \u201cType\u201d and\n\u201cSensor\u201d lists in the \u201cTank Set\u201d menu, use\nthe directional keypad to select \u201cCalibra-\ntion\u201d.\nPress the [SET] button to display the\nCalibration menu.\n2. Select \u201cIndividual Tank Capacity\u201d. Set\nusing the [SET] button.\nInitial Setting\n00_6YC_owners_En.book  Page 7  Sunday, March 1, 2015  3:36 PM8\n3. Use the directional keypad to configure\nthe fuel tank capacity.\nSet using the [SET] button.\nTIP:\n\u2022T h e  f u e l  t a n k  c a p a c i t y  m u s t  b e  c o n f i g u r e d\nfor each fuel tank.\n\u2022T h e  d i s p l a y e d  u n i t  f o r  t h e  f u e l  t a n k  c a p a c -\nity can be changed. For details on how to\nchange the displayed unit, see \u201cSetting the\ndisplayed units (Units)\u201d (page 32).\n4. Select \u201cCalibration\u201d. Set using the [SET]\nbutton.\nTIP:\n\u2022M u l t i  P o i n t  C a l i b r a t i o n  u s e s  5  c a l i b r a t i o n\npoints and Single Point Calibration uses\nonly 1 calibration point.\n\u2022S e l e c t  \u201c D e l e t e  C a l i b r a t i o n \u201d  t o  d e l e t e  o l d\ncalibration data.\n\u2022I t  i s  n o t  p o s s i b l e  t o  p r o c e e d  t o  \u201c C a l i b r a t i o n\nStep 1\u201d of \u201cMulti Point Calibration\u201d or \u201cSin-\ngle Point Calibration\u201d if any fuel remains in\nthe fuel tank.\n5. With no fuel remaining in the fuel tank\n(the fuel tank sensor should be posi-\ntioned at the bottom of the fuel tank),\npress the [SET] button.\n6. A confirmation screen will appear. Press\nthe [SET] button to confirm.\nTIP:\n\u2022T h e  S i n g l e  P o i n t  C a l i b r a t i o n  o f  t h e  f u e l\ntank sensor is complete.\n\u2022I n  c a s e  o f  t h e  M u l t i  P o i n t  c a l i b r a t i o n ,  t h e\ncalibration data will be saved at each point.\nAccordingly, the next time you perform a\ncalibration, it is possible to proceed with\nthe next point even if you abort the current\ncalibration process.\nInitial Setting\n00_6YC_owners_En.book  Page 8  Sunday, March 1, 2015  3:36 PM9\n7. Fill the fuel tank to 25% of its capacity\nwith fuel.\nTIP:\n\u2022T h e  q u a n t i t y  d i s p l a y e d  o n  t h e  s c r e e n  i s\n25% of the tank capacity.\n\u2022A n  e r r o r  m e s s a g e  w i l l  a p p e a r  a n d  c a l i b r a -\ntion will abort if the fuel tank sensor resist-\nance value is different from that of the\ncalibration point. Check the fuel quantity in\nthe fuel tank.\n8. Wait 5 seconds after filling the fuel tank\nwith fuel, and then press the [SET] but-\nton.\n9. A confirmation screen will appear. Press\nthe [SET] button to confirm and proceed\nwith calibration step 2.\n10. Fill the fuel tank to 50% of its capacity\nwith fuel.\n11. Wait 5 seconds after filling the fuel tank\nwith fuel, and then press the [SET] but-\nton.\n12. A confirmation screen will appear. Press\nthe [SET] button to confirm and proceed\nwith calibration step 3.\n13. Fill the fuel tank to 75% of its capacity\nwith fuel.\n14. Wait 5 seconds after filling the fuel tank\nwith fuel, and then press the [SET] but-\nton.\n15. A confirmation screen will appear. Press\nthe [SET] button to confirm and proceed\nwith calibration step 4.\n16. Fill the fuel tank to 100% of its capacity\nwith fuel.\n17. Wait 5 seconds after filling the fuel tank\nwith fuel, and then press the [SET] but-\nton.\n18. A confirmation screen will appear. Press\nthe [SET] button to confirm and proceed\nwith calibration step 5.\n19. The fuel tank sensor calibration is com-\nplete. Press the [SET] button.\nInitial Setting\n00_6YC_owners_En.book  Page 9  Sunday, March 1, 2015  3:36 PM10\nAdjusting the trim level to zero\n1. Fully tilt the outboard motor down.\n2. Press the [MENU] button in the main\nscreen to display the \u201cMENU\u201d screen.\n3. Use the directional keypad and the [SET]\nbutton to select \u201cTrim Level\u201d in the\n\u201cMENU\u201d screen. The \u201cTRIM LEVEL\u201d\nscreen is displayed.\n4. Check that the displayed trim level is\n\u201c0%\u201d.\n5. If a value other than \u201c0%\u201d is displayed,\npress the [SET] button to reset the set-\nting.\nInitial Setting\n00_6YC_owners_En.book  Page 10  Sunday, March 1, 2015  3:36 PM11\nMain screen\nThe main screens display various types of in-\nformation graphically and numerically. It is\npossible to switch between 4 types of main\nscreens: \u201cCOMBO\u201d, \u201cENGINE\u201d, \u201cBOAT\u201d, and\n\u201cTROLL\u201d.\nTIP:\nDepending on the model of the outboard mo-\ntor, some functions may not be available. For\ndetailed information, consult a Yamaha deal-\ner.\nSwitching the screens\nIt is possible to change the main screen de-\nsign to suit your preference by selecting from\nscreen type options.\nUse the [HJ] (Left/Right) buttons on the di-\nrectional keypad to switch between the main\nscreens.\nSingle-engine application\nTwin-engine application\nIt is possible to switch between 4 preregis-\ntered screen types (A\u2013D) for the following\nmain screens: \u201cCOMBO\u201d, \u201cENGINE\u201d,\n\u201cBOAT\u201d.\nUse the [LI] (Up/Down) buttons on the di-\nrectional keypad to switch between the\nscreen types.\n[/g74][/g74]\nTROLL\nCOMBO\n[/g74][/g74]\n[/g74][/g74]\nBOAT\n[/g74] [/g74][/g74]\nCOMBO\n[/g74][/g74]\nTROLL\nENGINE\n[/g74]\nMain screen\n00_6YC_owners_En.book  Page 11  Sunday, March 1, 2015  3:36 PM12\nUp to 4 screen types (A\u2013D) can be preregis-\ntered. For details on how to preregister the\nscreen types, see \u201cCustomizing the screen\n(Favorites)\u201d (page 23).\nSingle-engine application\nTwin-engine application\nENGINE\nBOAT\nCOMBO\nCOMBO\nThe \u201cCOMBO\u201d screen displays engine infor-\nmation and boat/environment information.\nSingle-engine application\nTwin-engine application\n1. Alert display area (icons)\n2. Y-COP display area\n3. Basic display area\n4. Specific selection area\n[/g73][/g73]\n[/g73][/g73]\n[/g73][/g73]\n[/g73][/g73]\n1\n2 4\n3\n1\n3\n4\n22\nMain screen\n00_6YC_owners_En.book  Page 12  Sunday, March 1, 2015  3:36 PM13\nENGINE\nThe \u201cENGINE\u201d screen displays engine infor-\nmation.\nThis screen is not displayed in a single-en-\ngine application.\n1. Alert display area (icons)\n2. Y-COP display area\n3. Basic display area\n4. Specific selection area\nBOAT\nThe \u201cBOAT\u201d screen displays boat/environ-\nment information.\nThis screen is not displayed in a single-en-\ngine application.\n1. Alert display area (icons)\n2. Basic display area\n3. Specific selection area\nTROLL\nThe \u201cTROLL\u201d screen allows you to switch to\na trolling mode and adjust the trolling speed.\nFor details on how to adjust the trolling\nspeed, see \u201cAdjusting the trolling speed\u201d\n(page 17).\nSingle-engine application\n1. Alert display area (icons)\n2. Basic display area\n3. Specific selection area\nTwin-engine application\n1. Alert display area (icons)\n2. Basic display area\n3. Specific selection area\n1\n3\n2\n4\n2\n2\n3\n3\n1\n3\n1\n2\n1\n2\n3\nMain screen\n00_6YC_owners_En.book  Page 13  Sunday, March 1, 2015  3:36 PM14\nAlert display area\nRed-colored alert indicators will be displayed\nin the alert display area for \u201cOverheat alert\u201d,\n\u201cLow oil pressure alert\u201d, \u201cWater in fuel alert\u201d,\n\u201cLow battery voltage alert\u201d, and \u201cCheck en-\ngine alert/maintenance alert\u201d. If an alert indi-\ncator is displayed, follow the instructions for\nresponding to each specific alert.\n\u25a0 Overheat alert\nThe cooling water temperature icon blinks\nwhen the engine temperature rises too high.\nAdditionally, the engine speed becomes lim-\nited and an alert buzzer sounds. Stop the en-\ngine and check that the cooling water inlet on\nthe lower case is not clogged.\nNOTICE\n\u2022D o  n o t  o p e r a t e  t h e  o u t b o a r d  m o t o r\nwhen the overheat alert is displayed.\nOtherwise, severe engine damage\ncould result.\n\u2022C o n s u l t  y o u r  Y a m a h a  d e a l e r  i f  t h e  p r o b -\nlem cannot be located and corrected.\n\u25a0 Low oil pressure alert\nThe oil pressure icon blinks when the oil\npressure drops below a specific value. Addi-\ntionally, the engine speed becomes limited\nand an alert buzzer sounds. Stop the engine\nand check the engine oil level. For details on\nhow to check the engine oil level, see the\nOwner\u2019s Manual of the outboard motor.\nNOTICE\n\u2022D o  n o t  o p e r a t e  t h e  e n g i n e  w i t h o u t  e n -\ngine oil. Otherwise, severe engine dam-\nage could result.\n\u2022C o n s u l t  y o u r  Y a m a h a  d e a l e r  i f  t h e  p r o b -\nlem cannot be located and corrected.\nMain screen\n00_6YC_owners_En.book  Page 14  Sunday, March 1, 2015  3:36 PM15\n\u25a0 Water in fuel alert\nThe water detection alert icon blinks when\nwater has accumulated in the water separa-\ntor (fuel filter) during cruising. Stop the en-\ngine and drain the water from the water\nseparator (fuel filter). For details on how to\ndrain the water, see the Owner\u2019s Manual of\nthe outboard motor.\nNOTICE\n\u2022W a t e r  i n  f u e l  c a n  c a u s e  e n g i n e  m a l -\nfunction.\n\u2022C o n s u l t  y o u r  Y a m a h a  d e a l e r  i f  t h e  p r o b -\nlem cannot be located and corrected.\n\u25a0 Low battery voltage alert\nThe battery voltage alert icon blinks when\nthe battery voltage drops low. Start the en-\ngine immediately to charge the battery when\nthe alert has activated. When the alert is con-\nstantly on even if the engine is running, get\nback to port immediately and consult a\nYamaha dealer to check the battery charging\nsystem.\n\u25a0 Check engine alert/maintenance alert\nThe check engine alert icon blinks when the\nengine malfunctions. Take the engine to a\nYamaha dealer and have it checked immedi-\nately. The check engine alert icon also\ncomes on when more than 100 hours have\nelapsed since the previous maintenance.\nNOTICE\nThe engine is not being controlled prop-\nerly. Take the engine to a Yamaha dealer\nand have it checked immediately.\nTIP:\n\u2022I n  c a s e  o f  a n  e n g i n e  m a l f u n c t i o n ,  t h e\ncheck engine alert icon keeps blinking until\nthe engine trouble is corrected.\n\u2022I n  c a s e  o f  a  m a i n t e n a n c e  a l e r t ,  t h e  c h e c k\nengine alert icon remains displayed until\nthe maintenance schedule is reset.\n\u2022I f  m o r e  t h a n  1 0 0  h o u r s  h a v e  e l a p s e d  s i n c e\nthe previous maintenance and a mainte-\nnance notification is displayed, take the\nengine to a Yamaha dealer and have it\nchecked immediately.\n\u2022F o r  d e t a i l s  o n  h o w  t o  r e s e t  t h e  m a i n t e -\nnance schedule, see \u201cManaging the main-\ntenance schedule (Maintenance)\u201d\n(page 21).\nMain screen\n00_6YC_owners_En.book  Page 15  Sunday, March 1, 2015  3:36 PM16\nY-COP display area\nWhen Y-COP is locked, the Y-COP icon\ncomes on.\n*I n s t a l l a t i o n  o f  Y - C O P  ( o p t i o n a l )  i s  r e q u i r e d\nfor this feature to be available.\nBasic display area\nThe tachometer, trim meter, and fuel gauge\nare displayed in the basic display area.\n\u25a0 Tachometer\nThe tachometer shows the engine revolu-\ntions per minute.\nCOMBO, ENGINE, TROLL\n\u25a0 Trim meter\nThe trim meter shows the trim angle of the\noutboard motor in 10 levels. When the out-\nboard motor is tilted up beyond the trim\nrange, all the segments turn to orange and\nblink.\nCOMBO, ENGINE, TROLL\n\u25a0 Fuel gauge\nThe fuel gauge indicates the remaining fuel\nin 10 levels. All the segments are displayed\nwhen the fuel tank is full. When the fuel level\nis low, the lowest segment blinks. When 2\nfuel tanks are connected, each segment is\ndivided into two, right and left, and remaining\nfuel level in each tank is displayed individual-\nly.\nMain screen\n00_6YC_owners_En.book  Page 16  Sunday, March 1, 2015  3:36 PM17\nBOAT\nSpecific selection area\nIt is possible to select the types of informa-\ntion displayed in the specific selection area.\nFor details on how to customize the specific\nselection area, see \u201cCustomizing the screen\n(Favorites)\u201d (page 23).\nAdjusting the trolling speed\nThe \u201cTROLL\u201d main screen allows to adjust\nthe trolling speed. The trolling speed can be\nadjusted randomly by increasing or decreas-\ning it in approximate steps of 50 r/min. In a\ntwin-engine application, the trolling speeds\nof both engines are synchronized.\nThis function is not available for outboard\nmotors not equipped with a trolling speed ad-\njustment function.\nIn order to switch to the trolling mode, all of\nthe following conditions must be satisfied.\n\u2022E n g i n e  r u n n i n g  ( s h i f t - i n ) .\n\u2022T h r o t t l e  f u l l y  c l o s e d .\n\u2022\u201c T R O L L \u201d  s e l e c t e d  a s  t h e  m a i n  s c r e e n .\n1. Select the \u201cTROLL\u201d main screen.\n2. Press the [SET] button to switch to the\ntrolling mode.\nTIP:\nWhile the trolling mode is active, \u201cTROLL\u201d is\nshown in the status bar in place of \u201cVIEW\u201d.\n3. Use the [ LI] (Up/Down) buttons of the\ndirectional keypad to adjust the trolling\nspeed.\nTIP:\n\u2022T o  i n c r e a s e  t h e  t r o l l i n g  s p e e d ,  p r e s s  t h e\n[L] (Up) button.\n\u2022T o  d e c r e a s e  t h e  t r o l l i n g  s p e e d ,  p r e s s  t h e\n[I] (Down) button.\n\u2022W h e n  w a r m i n g  u p  a  c o l d  e n g i n e ,  t h e  t r o l -\nling speed cannot be decreased below the\nengine high idle speed during warm up.\n\u2022T h e  t r o l l i n g  s p e e d  a d j u s t m e n t  r a n g e  d e -\npends on the outboard motor model.\nMain screen\n00_6YC_owners_En.book  Page 17  Sunday, March 1, 2015  3:36 PM18\n\u2022T h e  t r o l l i n g  s p e e d  c a n n o t  b e  a d j u s t e d\nwhen the gear shift is in the N position.\n4. Set the gear shift in the N position, and\nthen press the [CANCEL] button to exit\nthe trolling mode.\nTIP:\n\u2022W h e n  t h e  t r o l l i n g  m o d e  i s  c a n c e l e d ,\n\u201cTROLL\u201d switches to \u201cVIEW\u201d in the status\nbar.\n\u2022I f  t h e  t h r o t t l e  l e v e r  i s  i n  t h e  f u l l y  c l o s e d  p o -\nsition, pressing the [CANCEL] button will\ncancel the trolling mode, even when the\ngear shift is in the F or R position.\n\u2022T h e  t r o l l i n g  m o d e  i s  a l s o  c a n c e l e d  w h e n\nthe engine is turned off or when the engine\nspeed exceeds 3000 r/min.\nMain screen\n00_6YC_owners_En.book  Page 18  Sunday, March 1, 2015  3:36 PM19\nMENU screen\nVarious settings and reset operations can be\nmade in the \u201cMENU\u201d screen. This screen\nalso allows you to view information recorded\non the outboard motor.\nSwitching the screens\n1. Press the [MENU] button in the main\nscreen to display the \u201cMENU\u201d screen.\n2. Use the directional keypad buttons and\nthe [SET] button to select the desired\nmenu. The setting screen for the select-\ned menu is displayed.\nTo return to the ma in screen, press the\n[CANCEL] button while any item in the\nleftmost field is selected.\nTIP:\n\u2022P r e s s  t h e  [ C A N C E L ]  b u t t o n  t o  r e t u r n  t o  t h e\nprevious menu.\n\u2022I f  n o  a c t i o n  i s  p e r f o r m e d  f o r  3 0  s e c o n d s ,\nthe screen will automatically switch to the\nmain screen.\n\u2022P r e s s i n g  t h e  [ M E N U ]  b u t t o n  w i l l  a l s o\nswitch the display to the main screen.\n3. Use the directional keypad buttons and\nthe [SET] button to select from options\navailable for each item.\nPress the [CANCEL] button to return to\nthe \u201cMENU\u201d screen.\nSET\nMENU\nCANCEL\nSET\nMENU\nCANCEL\nSET\nMENU\nCANCEL\n[/g74]\n[/g74]\n[/g73][/g76]\nSET\nMENU\nCANCEL\nMENU screen\n00_6YC_owners_En.book  Page 19  Sunday, March 1, 2015  3:36 PM20\nTIP:\nPressing the [MENU] button will switch the\ndisplay to the main screen. However, the set-\ntings are not saved if the changes are not\nconfirmed using the [SET] button.\nFeatures\nThe \u201cMENU\u201d screen consists of the following menu items.\nSET\nMENU\nCANCEL\nLogs\nSettings\nAlarms\nTrip\nMaintenance\nBrightness\nDisplay\nClock\nUnits\nInitial\nCalibrate\nTrouble Codes\nFavorites\nColor\nTank Set\nReset\nTrim Level\nFuel Flow\nMENU screen\n00_6YC_owners_En.book  Page 20  Sunday, March 1, 2015  3:36 PM21\nMenu items\nThe display layout differs depending on the\nnumber of outboard motors. The sample\nscreen images here are for a twin-engine ap-\nplication.\n\u25a0 Resetting \u201cTrip\u201d information (Trip)\nIn the \u201cTrip\u201d menu, available fuel/fuel con-\nsumption, traveled distance, trip hours, and\ntotal operating hours can be reset.\n1. Use the directional keypad and the [SET]\nbutton to select \u201cT rip\u201d in the \u201cMENU\u201d\nscreen. The \u201cTRIP\u201d screen is displayed.\n2. Use the [ LI] (Up/Down) buttons on the\ndirectional keypad to select the item you\nwish to reset.\nSet using the [SET] button.\nTIP:\n\u2022R e s e t  A l l :  R e s e t s  a l l  t h e  c u r r e n t  s e t t i n g s  i n\n\u201cTrip\u201d.\n\u2022U S E D :  R e s e t s  f u e l  c o n s u m p t i o n .\n\u2022T R I P  D i s t a n c e :  R e s e t s  t r a v e l e d  d i s t a n c e .\n\u2022T R I P  H o u r s :  R e s e t s  t r i p  h o u r s .\n\u2022P r e s s i n g  t h e  [ C A N C E L ]  b u t t o n  w i l l  a l s o\ndisplay the \u201cTrip\u201d setting screen.\n\u25a0 Managing the maintenance schedule\n(Maintenance)\nIn the \u201cMaintenance\u201d menu, you can view or\nreset the current maintenance schedule.\n1. Use the directional keypad and the [SET]\nbutton to select \u201cMaintenance\u201d in the\n\u201cMENU\u201d screen.\n2. The \u201cMAINTENANCE\u201d screen is dis-\nplayed.\nMENU screen\n00_6YC_owners_En.book  Page 21  Sunday, March 1, 2015  3:36 PM22\nTIP:\n\u2022\u201c C u r r e n t \u201d  i n d i c a t e s  t h e  e l a p s e d  h o u r s\nsince the previous maintenance (reset).\n\u2022\u201c I n t e r v a l \u201d  i n d i c a t e s  t h e  r e c o m m e n d e d  i n -\nterval between maintenances.\n3. To reset the maintenance schedule, use\nthe [LI] (Up/Down) buttons on the di-\nrectional keypad to select the engine.\nSet using the [SET] button.\nTIP:\n\u2022R e s e t  A l l :  T h i s  o p t i o n  i s  a v a i l a b l e  f o r  a\ntwin-engine application only. It resets the\nmaintenance schedule of both engines.\n\u2022S T D :  D i s p l a y s  a n d  r e s e t s  t h e  c u r r e n t\nmaintenance schedule of each engine.\n\u2022I t  i s  n o t  p o s s i b l e  t o  c h a n g e  t h e  m a i n t e -\nnance interval (100 h).\n\u25a0 Changing backlight settings (Bright-\nness)\nIn the \u201cBrightness\u201d menu, the brightness lev-\nel of the backlight can be adjusted and\nchangeover time between Day and Night\nscreens can be configured for all meters syn-\nchronously.\n1. Use the directional keypad and the [SET]\nbutton to select \u201cB rightness\u201d in the\n\u201cMENU\u201d screen. The \u201cBRIGHTNESS\u201d\nscreen is displayed.\n2. Use the [ LI] (Up/Down) buttons on the\ndirectional keypad to select the item.\nUse the [HJ] (Left/Right) buttons on the\ndirectional keypad to change the set-\ntings.\nSet using the [SET] button.\nTIP:\n\u2022 Dimmer: Adjusts the brightness level of the\nbacklight. The brightness level can be ad-\njusted in 5 levels. The backlight turns off\nwhen the level is set to zero.\n\u2022S Y N C :  W h e n  \u201c O n \u201d  i s  selected, the settings\nare synchronously applied to all the con-\nnected meters.\n\u2022 Day & Night: When \u201cNight\u201d is selected, the\nbrightness of the screen is reduced to half\nof that of the Day screen. When \u201cAuto\u201d is\nselected, the screen switches between\nDay and Night screens at the changeover\ntime preset with \u201cAuto Timer Setting\u201d.\n\u2022A u t o  T i m e r  S e t t i n g :  A l l o w s  t o  c o n f i g u r e  t h e\nchangeover time from Day screen to Night\nscreen, and vice vers a. The changeover\ntime can be set between AM 0:00 and PM\n11:45 in 15-minute increments.\n\u2022E v e n  w h e n  \u201c A u t o \u201d  i s  s e l e c t e d ,  t h e  D a y\nscreen is displayed if time information can-\nnot be received from the GPS.\n\u2022P r e s s i n g  t h e  [ S E T ]  b u t t o n  w i l l  a l s o  s w i t c h\nthe display to the \u201cBRIGHTNESS\u201d screen.\nMENU screen\n00_6YC_owners_En.book  Page 22  Sunday, March 1, 2015  3:36 PM23\nThe main screen when \u201cNight\u201d is select-\ned.\n\u25a0 Customizing the screen (Favorites)\nIn the \u201cFavorites\u201d menu, it is possible to cus-\ntomize the main screens (\u201cENGINE\u201d,\n\u201cBOAT\u201d, \u201cCOMBO\u201d, and \u201cTROLL\u201d) by select-\ning the items to be displayed.\n1. Use the directional keypad and the [SET]\nbutton to select \u201cFavorites\u201d in the\n\u201cMENU\u201d screen. The \u201cFAVORITES\u201d\nscreen is displayed.\n2. Use the directional keypad to select a\nscreen from \u201cScreen Type\u201d.\nSet using the [SET] button.\nTIP:\n\u2022\u201c C o m b o \u201d  a n d  \u201c T r o l l \u201d  c a n  b e  c u s t o m i z e d  f o r\na single-engine application.\n\u2022\u201c E n g i n e \u201d ,  \u201c B o a t  S t a t s \u201d ,  \u201c C o m b o \u201d ,  a n d\n\u201cTroll\u201d can be customized for a twin-engine\napplication.\n\u2022U p  t o  4  t y p e s  ( A \u2013 D )  o f  \u201c S c r e e n  T y p e \u201d  c a n\nbe registered for each screen. When\n\u201cTroll\u201d is selected, screen types cannot be\nregistered.\n3. Use the [ LI] (Up/Down) buttons on the\ndirectional keypad to select a screen\nfrom \u201cScreen Type\u201d.\nSet using the [SET] button.\nMENU screen\n00_6YC_owners_En.book  Page 23  Sunday, March 1, 2015  3:36 PM24\nTIP:\n\u2022\u201c D i s p l a y \u201d  o r  \u201c N o  D i s p l a y \u201d  c a n  b e  s e l e c t e d\nfor a single-engine application.\n\u2022\u201c C o m b o 1 \u201d ,  \u201c C o m b o 2 \u201d ,  o r  \u201c N o  D i s p l a y \u201d\ncan be selected for a twin-engine applica-\ntion.\n\u2022W h e n  \u201c N o  D i s p l a y \u201d  i s  s e l e c t e d ,  t h e  s c r e e n\ntype you have selected in \u201cScreen Type\u201d is \nnot displayed.\n\u2022W h e n  \u201c T r o l l \u201d  i s  s e l e c t e d  f o r  \u201c S c r e e n  T y p e \u201d ,  \n\u201cScreen Display\u201d cannot be selected.\n4. Use the [ LI] (Up/Down) buttons on the\ndirectional keypad to select the item to be\ndisplayed.\nUse the [HJ] (Left/Right) buttons on the\ndirectional keypad to change the display\nposition of each item.\nSet using the [SET] button.\nTIP:\n\u2022T h e  d i s p l a y  s e t t i n g  s c r e e n  f o r  \u201c T r o l l \u201d  i s  t h e\nsame as those for \u201cC ombo\u201d (for single-\nengine application) and \u201cEngine\u201d (for twin-\nengine application).\nCombo, Troll (Single-engine application)\nCombo 1 (Twin-engine application)\nCombo 2 (Twin-engine application)\nBoat Stats\n1 3\n2\n2 3\n21\n3\n2\nMENU screen\n00_6YC_owners_En.book  Page 24  Sunday, March 1, 2015  3:36 PM25\nEngine, Troll (Twin-engine application)\n1. Specific selection area 1\n2. Specific selection area 2\n3. Specific selection area 3\n1\nMENU screen\n00_6YC_owners_En.book  Page 25  Sunday, March 1, 2015  3:36 PM26\nDisplay items for specific selection area 1\nBattery voltage value (Batt Voltage Value)\nShows the battery voltage in a digital value.\nTIP:\nWhen the battery voltage decrease s to less than 12 volts or in-\ncreases to 16 volts or more, the voltage value blinks.\nBattery voltage level (Batt Voltage Level)\nShows the battery voltage level of the engine in 5 levels.\nTIP:\nThe display shows \u201cL\u201d for low and \u201cH\u201d for high.\nCooling water temperature (Cooling Water Temp)\nShows the cooling water temperature in 5 levels.\nTIP:\nThe display shows \u201cC\u201d for cool and \u201cH\u201d for hot.\nCooling water pressure (PSI) (Water Pressure(PSI))\nShows the cooling water pressure in 5 levels.\n*I n s t a l l a t i o n  o f  t h e  w a t e r  p r e s s u r e sensor (optional) is required\nfor this feature to be available.\nNOTICE\n\u2022I f  t h e  c o o l i n g  w a t e r  p r e s s u r e  d r o p s  t o  1 0  p s i  ( 6 9  k P a )  o r\nless during cruising with th e throttle fully open, turn the\nengine off and check the cooling water inlet for clogs.\n\u2022C o n s u l t  y o u r  Y a m a h a  d e a l e r  i f  t h e  p r o b l e m  c a n n o t  b e  l o -\ncated and corrected.\nMENU screen\n00_6YC_owners_En.book  Page 26  Sunday, March 1, 2015  3:36 PM27\nCooling water pressure (Water Pressure)\nShows the cooling water pressure in 5 levels.\n*I n s t a l l a t i o n  o f  t h e  w a t e r  p r e s s u r e sensor (optional) is required\nfor this feature to be available.\nNOTICE\n\u2022I f  t h e  c o o l i n g  w a t e r  p r e s s u r e drops to the second segment\nor less during cruising with the throttle fully open, turn the\nengine off and check the cooling water inlet for clogs.\n\u2022C o n s u l t  y o u r  Y a m a h a  d e a l e r  i f  t h e  p r o b l e m  c a n n o t  b e  l o -\ncated and corrected.\nTIP:\nThe display shows \u201cL\u201d for low and \u201cH\u201d for high.\nEngine oil pressure (Oil Pressure)\nShows the engine oil pressure in 5 levels.\nTIP:\n\u2022T h e  d i s p l a y  s h o w s  \u201c L \u201d  f o r  l o w  a n d  \u201c H \u201d  f o r  h i g h .\n\u2022T h e  e n g i n e  o i l  p r e s s u r e  i n d i c a t o r  d o e s  n o t  i n d i c a t e  t h e  e n g i n e\noil level.\n\u2022F o r  F 1 3 0  a n d  s m a l l e r  e n g i n e s ,  t h e  e n g i n e  o i l  p r e s s u r e  d i s p l a y\nis not available. In this case, select items other than \u201cOil Pres-\nsure\u201d.\n\u2022D e p e n d i n g  o n  t h e  o p e r a t i n g  c o n d i t i o n s ,  s u c h  a s  w h e n  t h e  e n -\ngine is operated at extremely low speeds, the engine oil pres-\nsure may be shown as \u201cL\u201d (low) even when the engine is\noperating properly.\nNo display (Hide)\nNothing is displayed (blank area).\nMENU screen\n00_6YC_owners_En.book  Page 27  Sunday, March 1, 2015  3:36 PM28\nDisplay items for specific selection area 2\nTotal Fuel Flow (FLOW)\nShows the total amount of fuel flow per hour for all the engines.\nFuel Economy (ECON)\nShows the distance traveled per unit of fuel.\n*I n s t a l l a t i o n  o f  t h e  s p e e d  s e n s o r  (optional), multi sensor (option-\nal), or GPS is required for this feature to be available.\nTIP:\nFuel economy is affected by the type of boat being operated and\nthe operating conditions, and it is constantly changing.\nBoat Speed (SPEED)\nShows the speed-over-ground or speed-over-water of the boat.\n*S p e e d - o v e r - g r o u n d  i s  n o t  d i s p l a y e d  i f  a  G P S  i s  n o t  i n s t a l l e d .\n*I n s t a l l a t i o n  o f  t h e  s p e e d  s e n s o r  (optional) or multi sensor (op-\ntional) is required for this feature to be available.\nAvailable Fuel (AVAL)\nShows the total amount of remaining fuel in all the fuel tanks. If\nsensor calibration is not completed for all the fuel tank sensors,\nAvailable Fuel will not be displayed.\nFuel Level (FUEL)\nShows the fuel level in 10 levels. All the segments are displayed\nwhen the fuel tank is full. When the fuel level is low, the lowest\nsegment blinks. When 2 fuel tanks are connected, each segment\nis divided into two, right and left, and the remaining fuel level in\neach tank is displayed individually.\nNo display (Hide)\nNothing is displayed (blank area).\nMENU screen\n00_6YC_owners_En.book  Page 28  Sunday, March 1, 2015  3:36 PM29\nDisplay items for specific selection area 3\nSurface Water Temp (TEMP)\nShows the surface water temperature.\n*I n s t a l l a t i o n  o f  t h e  m u l t i  s e n s o r  ( o p t i o n a l )  i s  r e -\nquired for this feature to be available.\nDepth (DEPTH)\nShows the water depth.\n*I n s t a l l a t i o n  o f  t h e  m u l t i  s e n s o r  ( o p t i o n a l )  i s  r e -\nquired for this feature to be available.\nTrip Meter (h) (TRIP) (single-engine applica-\ntion)\nShows the operating hours of the engine.\nStbd Trip Meter (h) (S TRIP) (twin-engine ap-\nplication)\nShows the trip hours of the starboard engine.\nPort Trip Meter (h) (P  TRIP) (twin-engine ap-\nplication)\nShows the trip hours of the port engine.\nStbd Fuel Flow (S FLOW) (twin-engine appli-\ncation)\nShows the amount of fuel flow per hour for the\nstarboard engine.\nPort Fuel Flow (P FLOW) (twin-engine appli-\ncation)\nShows the amount of fuel flow per hour for the\nport engine.\nTotal Fuel Flow (FLOW)\nShows the total amount of fuel flow per hour for\nall the engines.\nAvailable Fuel (AVAL)\nShows the total amount of remaining fuel in all\nthe fuel tanks. If sensor calibration is not com-\npleted for all the fuel tank sensors, Available\nFuel will not be displayed.\nMENU screen\n00_6YC_owners_En.book  Page 29  Sunday, March 1, 2015  3:36 PM30\nTTL Fuel Consumption (USED)\nShows the total fuel consumption for all the en-\ngines.\nTIP:\nTTL Fuel Consumption can display up to\n9999 gal (9999 L). If the amount of consumed\nfuel exceeds this amount, \u201c9999\u201d will blink to no-\ntify the user.\nTrip Distance (TRIP)\nShows information from the speed sensor, multi sen-\nsor, and GPS converted to the distance traveled.\n*I n s t a l l a t i o n  o f  t h e  s peed sensor (optional),\nmulti sensor (optional), or GPS is required for\nthis feature to be available.\nTIP:\nThe distance traveled is  a f f e c t e d  b y  c u r r e n t s\nand other operating conditions and may differ\nfrom the actual distance traveled.\nFuel Economy (ECON)\nShows the distance traveled per unit of fuel.\n*I n s t a l l a t i o n  o f  t h e  s peed sensor (optional),\nmulti sensor (optional), or GPS is required for\nthis feature to be available.\nTIP:\nFuel economy is affected by the type of boat be-\ning operated and the operating conditions, and it\nis constantly changing.\nBoat Speed (SPEED)\nShows the speed-over-ground or speed-over-\nwater of the boat.\n*T h e  s p e e d - o v e r - g r o u n d  i s  o n l y  s h o w n  w h e n  a\nGPS is connected.\n*I n s t a l l a t i o n  o f  t h e  s p e e d  sensor (optional) or multi\nsensor (optional) is required for this feature to be\navailable.\nNo display (Hide)\nNothing is displayed (blank area).\nMENU screen\n00_6YC_owners_En.book  Page 30  Sunday, March 1, 2015  3:36 PM31\n\u25a0 Setting the background color (Color)\nIn the \u201cColor\u201d menu, it is possible to change\nthe colors for background and text of the\nmain screen.\n1. Use the directional keypad and the [SET]\nbutton to select \u201cColor\u201d in the \u201cMENU\u201d\nscreen. The \u201cCOLOR\u201d screen is dis-\nplayed.\n2. Use the [ LI] (Up/Down) buttons on the\ndirectional keypad to select the color.\nSet using the [SET] button.\nTIP:\n\u201cDefault\u201d or \u201cSpecial\u201d can be selected as the\nbackground color.\nDefault\nSpecial\n\u25a0 Adjusting the clock (Clock)\nIn the \u201cClock\u201d menu, it is possible to config-\nure a time offset.\nThis time offset will be applied for Daylight\nSavings Time and time zone difference.\n*T h e  c l o c k  i s  n o t  d i s p l a y e d  i f  a  G P S  i s  n o t  i n -\nstalled.\n1. Use the directional keypad and the [SET]\nbutton to select \u201cClock\u201d in the \u201cMENU\u201d\nscreen. The \u201cCLOCK\u201d screen is dis-\nplayed.\n2. Use the [ LI] (Up/Down) buttons on the\ndirectional keypad to select the amount\nof time to offset the clock.\nSet using the [SET] button.\nTIP:\nThe adjustable range is from -12h to +12h.\nMENU screen\n00_6YC_owners_En.book  Page 31  Sunday, March 1, 2015  3:36 PM32\n\u25a0 Setting the displayed units (Units)\nIn the \u201cUnits\u201d menu, it is possible to change\nthe displayed units.\n1. Use the directional keypad and the [SET]\nbutton to select \u201cUnits\u201d in the \u201cMENU\u201d\nscreen. The \u201cUNITS\u201d screen is displayed.\n2. Use the [ LI] (Up/Down) buttons on the\ndirectional keypad to select the item.\nUse the [HJ] (Left/Right) buttons on the\ndirectional keypad to select the unit.\nSet using the [SET] button.\nTIP:\n\u2022T h e  d e f a u l t  i s  s e t  t o  t h e  u n i t s  o f  \u201c m i l e \u201d\ngroup as shown in the table below (shaded\ncolumn).\n\u2022I f  t h e  u n i t  f o r  \u201c D i s t ance\u201d is changed, units\nfor other items will be changed accordingly\nto the units of the same group as follows.\n\u2022T h e  d i s p l a y  u n i t  c a n  b e  s e l e c t e d  f r o m  k m ,\nmile (land mile: 1.609 km), and nm (nauti-\ncal mile: 1.852 km).\n\u2022B o a t  S p e e d :  S e l e c t a b l e  f r o m  k m / h ,  m p h ,\nand knots.\n\u2022F u e l :  S e l e c t a b l e  f r o m  L  a n d  g a l .\n\u2022F u e l  F l o w :  S e l e c t a b l e  f r o m  L / h  a n d  g p h .\n\u2022E c o n o m y :  S e l e c t a b l e  f r o m  k m / L ,  m p g ,  a n d\nnm/L.\n\u2022T e m p e r a t u r e :  S e l e c t a b l e  f r o m  \u00b0 C  a n d  \u00b0 F .\n\u2022D e p t h :  S e l e c t a b l e  f r o m  m  a n d  f t .\nDistance km mile nm\nBoat Speed km/h mph knots\nFuel L gal L\nFuel Flow L/h gph L/h\nEconomy km/L mpg nm/L\nTemperature \u00b0C \u00b0F \u00b0C\nDepth m ft m\nMENU screen\n00_6YC_owners_En.book  Page 32  Sunday, March 1, 2015  3:36 PM33\n\u25a0 Setting the tanks (Tank Set)\nIn the \u201cTank Set\u201d menu, it is possible to con-\nfigure the settings for the fuel tank. Up to 2\ntanks can be configured.\n1. Use the directional keypad and the [SET]\nbutton to select \u201cTank Set\u201d in the \u201cMENU\u201d\nscreen. The \u201cTANK SETTINGS\u201d screen\nis displayed.\n2. Use the \u0002[HJ] (Left/Right) buttons on the\ndirectional keypad to select the tank\nnumber.\nSet using the [SET] button.\n3. Use the directional ke ypad to select the\ntank type from the \u201cType\u201d list.\nTIP:\n\u2022F U E L :  F u e l  t a n k\n\u2022U N U S E D :  T a n k  n o t  u s e d\n4. Use the directional ke ypad to select the\nsensor type from the \u201cSensor\u201d list.\nTIP:\n\u2022A B Y C :  U S A\n\u2022E U R :  E u r o p e\n\u2022Y A M A H A :  Y a m a h a\n\u2022A  f u e l  t a n k  s e n s o r  c a l i b r a t i o n  i s  r e q u i r e d  i f\nthe tank is configured as \u201cFUEL\u201d for \u201cTank\nSet\u201d.\n\u2022M u l t i p l e  P o i n t  C a l i b r a t i o n :  S e n s o r s  a r e  c a l -\nibrated at 5 points (0%, 25%, 50%, 75%,\n100%).\n\u2022S i n g l e  P o i n t  C a l i b r a t i o n :  S e n s o r s  a r e  c a l i -\nbrated at 1 point (0%).\n\u2022D e l e t e  C a l i b r a t i o n :  O l d  c a l i b r a t i o n  d a t a  w i l l\nbe deleted.\n\u2022N o n e :  F u e l  t a n k  s e n s o r  c a l i b r a t i o n  i s  n o t\ncompleted.\n\u2022N o  d i s p l a y :  T a n k  i s  c o n f i g u r e d  a s  \u201c U N -\nUSED\u201d.\n\u2022F o r  d e t a i l s  o n  h o w  t o  c a l i b r a t e  t h e  f u e l  t a n k\nsensor, see \u201cCalibrating the fuel tank sen-\nsors\u201d (page 7).\nMENU screen\n00_6YC_owners_En.book  Page 33  Sunday, March 1, 2015  3:36 PM34\n\u25a0 Initializing the meter (Reset)\nIn the \u201cReset\u201d menu, it is possible to reset the\nmeter to its default settings.\n1. Use the directional keypad and the [SET]\nbutton to select \u201cReset\u201d in the \u201cMENU\u201d\nscreen.\n2. Use the [ LI] (Up/Down) buttons on the\ndirectional keypad to select the item.\nSet using the [SET] button.\nTIP:\n\u2022T H I S  D E V I C E :  T h i s  m e t e r  w i l l  b e  i n i t i a l i z e d\nand restarted.\n\u2022E N G I N E  N U M . :  T h e  number of outboard\nmotors will be initialized. However, the\ntank settings (Tank Set) will not be initial-\nized.\n\u2022S Y S T E M  ( w h e n  m e c h a nical remote con-\ntrol is connected): Identification numbers\nstored in each engine will be initialized.\n\u2022C L  P L U S  G A T E W A Y  ( w h e n  D i g i t a l  E l e c -\ntronic Control is connected): Connected\nCommand Link Plus gateway will be initial-\nized.\nWhen mechanical remote control is con-\nnected\nWhen Digital Electronic Control is con-\nnected\n\u25a0 Adjusting the trim level to zero (Trim\nlevel)\nFully trim the outboard motor down, and then\nadjust the trim level to zero.\nFor details on how to adjust the trim level,\nsee \u201cAdjusting the trim level to zero\u201d\n(page 10).\nMENU screen\n00_6YC_owners_En.book  Page 34  Sunday, March 1, 2015  3:36 PM35\n\u25a0 Calibration of fuel consumption (Fuel\nFlow)\nIn the \u201cFuel Flow\u201d menu, it is possible to cal-\nibrate the difference between the fuel con-\nsumption display and the actual fuel\nconsumption.\nTIP:\n\u2022M e a s u r e m e n t  e r r o r s  m a y  o c c u r  i n  t h e  f u e l\nflow reading depending on the operating\nconditions or environmental conditions of\nthe engine.\n\u2022W h e n  c h a n g i n g  t h e  c o r r e c t i o n  v a l u e  f o r\nfuel flow, displayed values for total fuel\nflow (FLOW), total fuel consumption\n(USED), and fuel economy (ECON) will be\ncalibrated.\n1. Operate the outboard motor with the fuel\ntank filled with fuel.\n2. After operation, refill the fuel tank. Com-\npare the amount of fuel dispensed into\nthe fuel tank with the consumed fuel\namount shown on the display (USED).\n3. Use the directional keypad and the [SET]\nbutton to select \u201cFuel Flow\u201d in the\n\u201cMENU\u201d screen.\n4. Use the [ LI] (Up/Down) buttons on the\ndirectional keypad to  s e l e c t  t h e  d i f f e r -\nence in fuel amount between the actual\nvalue and the displayed value.\nSet using the [SET] button.\nTIP:\nThe displayed fuel flow value can be calibrat-\ned within the range of -7 to +7 percent.\n\u25a0 Trouble Codes\n\u2022I n  t h e  \u201c T r o u b l e  C o d e s \u201d  m e n u ,  i t  i s  p o s s i b l e\nto check the currently active trouble codes.\n\u2022I f  a n y  t r o u b l e  c o d e  i s  d e t e c t e d ,  c o n s u l t  a\nYamaha dealer.\nPlease mention the trouble code when you\ncontact a Yamaha dealer.\n1. Use the directional keypad and the [SET]\nbutton to select \u201cTrouble Codes\u201d in the\n\u201cMENU\u201d screen.\nMENU screen\n00_6YC_owners_En.book  Page 35  Sunday, March 1, 2015  3:36 PM36\nTIP:\nUp to 5 trouble codes are displayed for each\nengine.\nMENU screen\n00_6YC_owners_En.book  Page 36  Sunday, March 1, 2015  3:36 PM37\nAppendix\nRequirements for installation\nThe meter may not operate correctly under\nthe following conditions:\n\u2022W h e n  i n s t a l l e d  n e a r  a  s o u r c e  o f  n o i s e\n(generator, radio, antenna wire, etc.).\nKeep the meter away from noise sources.\n\u2022W h e n  t h e  a m b i e n t  t e m p e r a t u r e  i s  h i g h .\nAvoid exposure to high temperatures.\n\u2022W h e n  t h e  c o n n e c t e d  e x t e r n a l  d e v i c e  i s  n o t\ncompatible.\nUse a device that conforms to the require-\nments (i.e., NMEA0183-compatible GPS).\n\u2022I f  t h e  g a t e w a y  h u b  i s  n o t  c o n n e c t e d .\nUse a gateway hub to connect external de-\nvices such as NMEA2000 compatible\nGPS, fish finder, etc.\nProper care of the instrument\nIf the surface of the in strument is dirty, wipe\nit clean with a soft, lint-free cloth.\nNOTICE\n\u2022D o  n o t  u s e  h i g h - p r e s s u r e  w a s h e r s  o r\nsteam-jet cleaners. Water may enter the\ninstrument and cause the instrument to\nmalfunction.\n\u2022A v o i d  u s i n g  c h e m i c a l  c l e a n e r s  ( c o n -\ntaining alkalinity, acidity, or neutrality),\nwax containing an abrasive compound,\noil, solvents, or any other chemicals.\nOtherwise, the surface of the instru-\nment could be damaged.\nAppendix\n00_6YC_owners_En.book  Page 37  Sunday, March 1, 2015  3:36 PM38\nTemplate (actual size)\n153.1 (6.03)\n32.8 (1.29) 58 (2.28)\n4x/g823.2 (0.13)\n4x/g8251 (2.01)\n83 (3.27)\n27 (1.06)28 (1.1)\nmm (in)\nAppendix\n00_6YC_owners_En.book  Page 38  Sunday, March 1, 2015  3:36 PM00_6YC_owners_En.book  Page 39  Sunday, March 1, 2015  3:36 PM00_6YC_owners_En.book  Page 40  Sunday, March 1, 2015  3:36 PMPrinted in U.S.A.\nMarch 2015 - \u001a.\u0015 [ \u0014A%(\n00_6YC_owners_En.book  Page 39  Sunday, March 1, 2015  3:36 PM\n-*5\u000e\u0012\u0019\u0017\u0013\u0017\u000e\u0012\u0011\u000e\u0017\u0016",
      "Thank you for purchasing this Yamaha product.\n\u2022 For correct and safe use of this product, be sure to first read \nthis manual and the separate \u201cSafety Brochure\u201d .\n\u2022 After reading this manual, keep it together with the \nwarranty card in a safe place for future reference.\n Product, Accessory and Part Names\na Earphones\nb Controller\n* The microphone is built in.\nc Charging connector\nd Indicator\ne Function key\nf Eartips (SS, S, M, L) 1 set each\n* The M-size eartips are attached.\ng USB power cable\nh Startup Guide\ni User\u2019s Guide (Basic) (this manual)\nj Safety Brochure\n Charging\nWith the included USB power cable, connect the charging \nconnector on the earphones to the USB port on a \ncommercially available USB power adaptor (5 V DC/min. 0.5 A \noutput). \n\u2022 The indicator is lit in red during charging and goes off when \ncharging is completed.\n\u2022 Charging takes about two hours. When charged, the \nearphones can play back continuously for approximately \n14 hours.\n Making a Bluetooth\u00ae connection\nWhen using the earphones for the first time, the pairing \nprocess, which allows Bluetooth devices being connected to \ndetect each other, must be performed. Once paired, the \nBluetooth devices will be automatically connected the next \ntime they are turned on.\n\u25a0P a i r i n g\n1. Hold down the function key for at least five seconds to \nenter pairing standby.\n\u2022 The indicator flashes alternately in blue and red.\n2. Operate the connected Bluetooth device to display the \nBluetooth setting screen.\n3. Select \u201cYamaha EP-E30A\u201d from the displayed screen.\n\u2022 When pairing has completed, the indicator lights up in \nblue for 20 seconds, then flashes at 5-second intervals.\n\u2022 If pairing has not completed within two minutes, it will \nbe considered to have failed, and the indicator goes off. \n\u2022 To try pairing again, turn the earphones off, then on \nagain.\nFor details on use, refer to the website.\n\u25a0 Turning on/off\nHold down the function key on the controller for two seconds \nto turn the earphones on/off.\n\u2022 After the earphones turn on, the indicator flashes in red to \nindicate the remaining battery power.\nListening to music\nUsing the phone\nThe Bluetooth\u00ae word mark and logos are registered \ntrademarks owned by Bluetooth SIG, Inc. and any use of such \nmarks by Yamaha Corporation is under license. Other \ntrademarks and trade names are those of their respective \nowners. \n<&VSPQF\u0001NPEFM>\nWireless communication functions\nBluetooth\nRadio Frequency (Operational Frequency): 2402 MHz to 2480 \nMHz\nMaximum Output Power (E.I.R.P): 6.25 dBm\nNous vous remercions d'avoir choisi ce produit Yamaha.\n\u2022 Pour garantir une utilisation correcte et s\u00fbre de ce produit, \nveillez \u00e0 lire ce manuel ainsi que la \u00ab Brochure sur la \ns\u00e9curit\u00e9 \u00bb en annexe.\na\nL\n2\n1 3\nR\nb\nf\ng\nhi\nA B\nC\nT\ne\nc\nd\nj\nEarphones\nCasques Intra-auriculaires\n\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438 \u0432\u0441\u0442\u0430\u0432\u043d\u044b\u0435\n\u2a8c\u7e70\u430b\u7e70\u512c\n\u7e70\u9df9\u430b\u7e70\u5680\nUser Guide (Basic) \nMode d'emploi (\u00e9l\u00e9mentaire)\nBedienungsanleitung (Grundausgabe)\nAnv\u00e4ndarguide (Grundl\u00e4ggande)\nGuida dell'utente (base)\nManual de usuario (b\u00e1sica)\nGebruikershandleiding (basis)\nGuia do Usu\u00e1rio (b\u00e1sico)\nP\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f (\u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0435)\n\u69ab\u4893\u49d7\u2d9f\ue6ae\u34f9\u5117\ue6af\n\u2733\u69ab\u49d8\u2d9f\ue6ae\u34f9\u5117\u4cea\u26fc\ue6af\n\ucea7\ud4a4\ucf1f\uc880\ucf17\t\ubc2b\ucaf3\nAV19-0188\nYamaha Global Site\nhttps://www.yamaha.com/\nYamaha Downloads\nhttps://download.yamaha.com/\nManual Development Group\n\u00a9 2019 Yamaha Corporation\nPublished 10/2020\n2020 \u5e74 10 \u6708 \u53d1\u884c    AMOD-B0\nEnglish\nIntroduction\nThis product is not designed to be waterproof. Do not \nexpose this product to liquids such as sweat, rainwater or \nseawater.\nPreparation\nEN Basic operations\nPlay Briefly press the function key once. \nPause During playback, briefly press the \nfunction key once. \nRaise the volume Briefly press the + key once. \nLower the volume Briefly press the \u2013 key once. \nSkip to the next song Hold down the + key for two \nseconds. \nGo back to the \nbeginning of the song, \nor skip to the previous \nsong\nHold down the \u2013 key for two \nseconds. \nReceive a phone call When a phone call comes in, \nbriefly press the function key once. \nEnd a phone call During the phone call, briefly press \nthe function key once. \nRaise the conversation \nvolume\nBriefly press the + key once. \nLower the \nconversation volume\nBriefly press the \u2013 key once. \nIgnore a call When a phone call comes in, \nbriefly press the function key twice. \nSpecifications\nBluetooth version Version 5.0\nSupported profiles A 2 D P, AV R C P,  H F P, H SP\nSupported codecs SBC, AAC\nRF Output Power Class 2\nMaximum \ncommunication range\nApproximately 10 m \n(without interference)\nNumber of devices \nthat can be registered\nUp to 8 devices\nInternal rechargeable \nbattery\nLithium-ion battery\nCharging time Approximately 2 hours\nContinuous playback time Approximately 14 hours \nCharging temperature 5 \u2013 40 \u00b0C\nFran\u00e7ais\nIntroduction\nFR\n\u2022 Quand vous avez fini de consulter ce manuel, rangez-le en \nlieu s\u00fbr avec la carte de garantie pour toute r\u00e9f\u00e9rence \nult\u00e9rieure. Produit, accessoires et noms des \n\u00e9l\u00e9ments\na Casques intra-auriculaires\nb Commande\n* Comporte un microphone int\u00e9gr\u00e9.\nc Connecteur de charge\nd T\u00e9moin\ne Touche de fonction\nf Embouts (SS, S, M, L) 1 paire de chaque taille\n* Les embouts de taille M sont fix\u00e9s aux casques.\ng C\u00e2ble d'alimentation USB\nh Guide de d\u00e9marrage\ni Guide (\u00e9l\u00e9mentaire) de l'utilisateur (ce document)\nj Brochure sur la s\u00e9curit\u00e9\n Charge\nReliez avec le c\u00e2ble d'alimentation USB fourni le connecteur \nde charge des casques au port USB d'un adaptateur USB \ndisponible dans le commerce (5 V CC/ 0,5 A min.). \n\u2022 Le t\u00e9moin s'allume en rouge pendant la charge et s'\u00e9teint \nquand la charge est termin\u00e9e.\n\u2022 La charge prend environ deux heures. Une fois charg\u00e9s, les \ncasques ont une autonomie continue d'environ 14 heures.\n \u00c9tablir une connexion Bluetooth\u00ae\nLors de l'utilisation initiale des casques, il convient d'ex\u00e9cuter \nl'appairage, qui permet la d\u00e9tection mutuelle des dispositifs \nBluetooth connect\u00e9s. Une fois appair\u00e9s, les dispositifs \nBluetooth s'interconnectent automatiquement \u00e0 chaque mise \nsous tension.\n\u25a0 Appairage\n1. Maintenez la touche de fonction enfonc\u00e9e pendant au \nmoins cinq secondes pour activer l'attente \nd'appairage.\n\u2022 Le t\u00e9moin clignote alternativement en rouge et bleu.\n2. Affichez l'\u00e9cran des r\u00e9glages Bluetooth sur le dispositif \n\u00e0 connecter.\n3. S\u00e9lectionnez \u00ab Yamaha EP-E30A \u00bb sur cet \u00e9cran.\n\u2022 Quand l'appairage est termin\u00e9, le t\u00e9moin reste allum\u00e9 \nen bleu pendant 20 secondes puis clignote \u00e0 intervalles \nde 5 secondes.\n\u2022 Si l\u2019appairage n\u2019est pas termin\u00e9 dans un d\u00e9lai de deux \nminutes, la tentative est consid\u00e9r\u00e9e comme rat\u00e9e et le \nt\u00e9moin s\u2019\u00e9teint.\n\u2022 Mettez les casques hors tension puis \u00e0 nouveau sous \ntension, et essayez \u00e0 nouveau d\u2019appairer les dispositifs.\nPour des d\u00e9tails sur l\u2019utilisation, visitez notre site internet.\n\u25a0 Mise sous tension/hors tension\nMaintenez la touche de fonction de la commande enfonc\u00e9e \npendant plus de deux secondes pour mettre les casques sous \ntension/hors tension.\n\u2022 Apr\u00e8s la mise sous tension des casques, le t\u00e9moin clignote \nen rouge pour indiquer l'autonomie r\u00e9siduelle de la batterie.\n\u00c9coute de musique\nUtilisation du t\u00e9l\u00e9phone\nL'appellation et les logos Bluetooth\u00ae sont des marques \nd\u00e9pos\u00e9es de Bluetooth SIG, Inc. et toute utilisation de ces \nmarques par Yamaha Corporation fait l'objet d'une licence. Les \nautres marques et appellations commerciales sont la \npropri\u00e9t\u00e9 de leurs d\u00e9tenteurs respectifs. \n<.PE\u00c8MF\u0001QPVS\u0001M\b&VSPQF>\nFonctions de communication sans fil\nBluetooth\nFr\u00e9quence radio (fr\u00e9quence op\u00e9rationnelle): 2402 MHz \u00e0 2480 \nMHz\nPuissance de sortie maximum (E.I.R.P): 6,25 dBm\nVielen Dank, dass Sie sich f\u00fcr dieses Produkt von Yamaha \nentschieden haben.\n\u2022 Lesen Sie zuerst diese Anleitung und die gesonderte \n\u201eSicherheitsbrosch\u00fcre\u201c, um eine korrekte und sichere \nVerwendung dieses Produkts zu gew\u00e4hrleisten.\n\u2022 Bewahren Sie diese Anleitung nach dem Lesen zusammen \nmit der Garantiekarte f\u00fcr sp\u00e4tere Nachschlagzwecke \nsorgf\u00e4ltig auf.\n Produkt-, Zubeh\u00f6r- und \nTeilebezeichnungen\na Ohrh\u00f6rer\nb Steuereinheit\n* Das Mikrofon ist eingebaut.\nc Ladeanschluss\nd Anzeige\ne Funktionstaste\nf Ohrst\u00f6psel (SS, S, M, L) je 1 Satz\n* H\u00f6rer werkseitig mit St\u00f6pseln der Gr\u00f6\u00dfe M versehen.\ng USB-Stromversorgungskabel\nh Kurzanleitung\ni Grundlegende Bedienungsanleitung (vorliegend)\nj Sicherheitsbrosch\u00fcre\n Laden\nSchlie\u00dfen Sie das zugeh\u00f6rige USB-Stromversorgungskabel an \nden Ladeanschluss der Ohrh\u00f6rer und an den USB-Anschluss \neines handels\u00fcblichen USB-Netzteils (mit 5 VDC/min. 0,5 A \nAusgang) an. \n\u2022 Die Anzeige leuchtet w\u00e4hrend des Ladens rot und erlischt, \nwenn der Ladevorgang abgeschlossen ist.\n\u2022 Der Ladevorgang dauert ungef\u00e4 hr zwei Stunden. Nach dem \nLaden bieten die Ohrh\u00f6rer eine ununterbrochene \nWiedergabeleistung von ca. 14 Stunden.\n Bluetooth\u00ae-Verbindung herstellen\nWenn Sie die Ohrh\u00f6rer zum ersten Mal verwenden, muss ein \nPairing ausgef\u00fchrt werden, d.h. der Kopplungsvorgang, bei \ndem miteinander zu verbindende Bluetooth-Ger\u00e4te einander \nerkennen. Nach dem Pairing werden die Bluetooth-Ger\u00e4te \nbeim n\u00e4chsten Einschalten automatisch verbunden.\n\u25a0P a i r i n g\n1. Halten Sie die Funktionstaste mindestens f\u00fcnf \nSekunden lang gedr\u00fcckt, um auf Pairing-Standby \numzuschalten.\n\u2022 Die Anzeige blinkt abwechselnd blau und rot.\n2. Bedienen Sie das verbundene Bluetooth-Ger\u00e4t, um den \nBluetooth-Einstellungsbildschirm anzuzeigen.\n3. W\u00e4hlen Sie \u201e Yamaha EP-E30A\u201c im angezeigten \nBildschirm aus.\n\u2022 Nach Abschluss des Pairings leuchtet die Anzeige 20 \nSekunden lang blau und blinkt dann im 5-Sekunden-\nIntervall.\n\u2022 Ist das Pairing nicht innerhalb von zwei Minuten \nabgeschlossen, gilt es als fehlgeschlagen, und die \nAnzeige erlischt.\n\u2022 Um das Pairing erneut auszuf\u00fchren, das Ger\u00e4t aus- und \nwieder einschalten.\nN\u00e4heres zur Benutzung findet sich auf der Website.\n\u25a0E i n - / A u s s c h a l t e n\nHalten Sie die Funktionstaste der Steuereinheit zwei Sekunden \nlang gedr\u00fcckt, um die Ohrh\u00f6rer ein- oder auszuschalten.\n\u2022 Nach dem Einschalten der Ohrh\u00f6rer blinkt die Anzeige rot, \num die verbleibende Akkuleistung anzuzeigen.\nMusik h\u00f6ren\nTelefonieren\nBluetooth\u00ae-Begriff und -Logo sind eingetragene Warenzeichen \nvon Bluetooth SIG, Inc. deren Verwendung durch Yamaha \nCorporation unter Lizenz erfolgt. Alle anderen Warenzeichen \nbzw. eingetragenen Warenzeichen geh\u00f6ren den \nentsprechenden Firmen. \n<&VSPQB\u000e.PEFMM>\nDrahtlos-Kommunikationsfunktionen\nBluetooth\nEmpfangsfrequenz (Arbeitsfrequenz): 2.402\u20132.480 MHz\nMaximale Ausgangsleistung (E.I.R.P): 6,25 dBm\nTack f\u00f6r att du k\u00f6pt denna produkt fr\u00e5n Yamaha.\n\u2022 F\u00f6r korrekt och s\u00e4ker anv\u00e4ndning av denna produkt, ska du \nf\u00f6rst l\u00e4sa denna manual och \u201ds\u00e4kerhetsbroschyren\u201d .\n\u2022 N\u00e4r du har l\u00e4st denna manual, ska du f\u00f6rvara den \ntillsammans med garantikortet p\u00e5 en s\u00e4ker plats f\u00f6r \nframtida referens.\n Namn p\u00e5 produkt, tillbeh\u00f6r och delar\na \u00d6ronsn\u00e4ckor\nb Kontroller\n* Inbyggd mikrofon.\nc Kontaktdon f\u00f6r laddning\nd Indikator\ne Funktionsknapp\nf \u00d6ronproppar (SS, S, M, L) 1 upps\u00e4ttning vardera\n* M-storleken p\u00e5 \u00f6ronpropparna \u00e4r anslutna.\ng USB-str\u00f6mkabel\nh Startguide\ni Anv\u00e4ndarmanual (Grundl\u00e4ggande) ( i denna manual)\nj S\u00e4kerhetsbroschyr\n Ladda\nMed den medf\u00f6ljande USB-str\u00f6mkabeln ansluter du \nladdningskontakten p\u00e5 \u00f6ronsn\u00e4ckorna till USB-porten p\u00e5 en \nkommersiellt tillg\u00e4nglig USB-str\u00f6madapter (5 V DC / min. 0,5 A \nutg\u00e5ng) n\u00e4r den ska laddas. \n\u2022 Indikatorn t\u00e4nds r\u00f6tt under laddning och slocknar n\u00e4r \nladdningen \u00e4r klar.\n\u2022 Laddning tar cirka tv\u00e5 timmar. N\u00e4r de laddas kan \n\u00f6ronsn\u00e4ckorna spelas kontinuerligt i cirka 14 timmar.\n Uppr\u00e4tta en Bluetooth\u00ae-anslutning\nN\u00e4r du anv\u00e4nder \u00f6ronsn\u00e4ckorna f\u00f6r f\u00f6rsta g\u00e5ngen, ska \nprocessen f\u00f6r ihopparning, som g\u00f6r det m\u00f6jligt f\u00f6r Bluetooth-\nenheter som \u00e4r anslutna att uppt\u00e4cka varandra, utf\u00f6ras. N\u00e4r de \nhar parats ihop, ansluts Bluetooth-enheterna automatiskt \nn\u00e4sta g\u00e5ng de sl\u00e5s p\u00e5.\n\u25a0 Ihopparning\n1. H\u00e5ll funktionsknappen intryckt i minst fem sekunder \nf\u00f6r att ange ihopparning i standby.\n\u2022 Indikatorn blinkar v\u00e4xelvis bl\u00e5tt och r\u00f6tt.\n2. Anv\u00e4nd den anslutna Bluetooth-enheten f\u00f6r att visa \ninst\u00e4llningssk\u00e4rmen f\u00f6r Bluetooth.\n3. V\u00e4lj \u201dYamaha EP-E30A\u201d fr\u00e5n den visade sk\u00e4rmen.\n\u2022 N\u00e4r ihopparningen \u00e4r klar, t\u00e4nds indikatorn bl\u00e5 i 20 \nsekunder och blinkar sedan med 5 sekunders intervall.\n\u2022 Om ingen ihopparning har slutf\u00f6rts inom tv\u00e5 minuter, \nkommer det anses ha misslyckats och indikatorn \nslocknar.\n\u2022 F\u00f6r att f\u00f6rs\u00f6ka para ihop igen, ska produkten sl\u00e5s av och \nsedan p\u00e5 igen.\nF\u00f6r mer information, g\u00e5 till webbplatsen.\n\u25a0S l \u00e5  p \u00e5 / a v\nH\u00e5ll funktionsknappen intryckt p\u00e5 kontroller i tv\u00e5 sekunder f\u00f6r \natt sl\u00e5 p\u00e5/st\u00e4nga av \u00f6ronsn\u00e4ckorna.\n\u2022 N\u00e4r \u00f6ronsn\u00e4ckorna \u00e4r p\u00e5slagna, blinkar indikatorn r\u00f6d f\u00f6r \natt ange den \u00e5terst\u00e5ende batterikapaciteten.\nLyssna p\u00e5 musik\nAnv\u00e4nda telefonen\nBluetooth\u00ae-ordm\u00e4rket och logotyper \u00e4r registrerade \nvarum\u00e4rken som \u00e4gs av Bluetooth SIG, Inc., och all \nanv\u00e4ndning av s\u00e5dana varum\u00e4rken som tillh\u00f6r Yamaha \nCorporation \u00e4r under licens. Andra varum\u00e4rken och \nhandelsnamn tillh\u00f6r deras respektive \u00e4gare. \n<&VSPQB\u000eNPEFMM>\nTr\u00e5dl\u00f6sa kommunikationsfunktioner\nBluetooth\nRadiofrekvens (Operativ frekvens): 2402 MHz till 2480 MHz\nMaximal utg\u00e5ngseffekt (E.I.R.P): 6,25 dBm\nGrazie per aver acquistato questo prodotto Yamaha.\n\u2022 Per un utilizzo corretto e sicuro del prodotto, assicurarsi di \nleggere prima questo manuale e la \"Brochure sulla \nsicurezza\" .\n\u2022 Dopo aver letto il manuale, conservarlo insieme alla scheda \ndi garanzia in un luogo sicuro per future consultazioni.Nomi del prodotto, degli accessori e \ndelle parti\na Auricolari\nb Controller\n* Il microfono \u00e8 incorporato.\nc Connettore di ricarica\nd Indicatore\ne Tasto funzione\nf Cuscinetti (SS, S, M, L) 1 set ciascuno\n* I cuscinetti di dimensioni M sono gi\u00e0 fissati agli auricolari\nin vendita.\ng Cavo di alimentazione USB\nh Guida introduttiva\ni Guida dell'utente (base) (questo manuale)\nj Brochure sulla sicurezza\nRicarica\nCollegare il cavo di alimentazione USB incluso al connettore di \nricarica sugli auricolari e alla porta USB di un alimentatore \ndisponibile in commercio (5 V CC/min. 0,5 A in uscita). \n\u2022 L'indicatore \u00e8 acceso durante la ricarica e si spegne una \nvolta che la ricarica \u00e8 completata.\n\u2022 La ricarica dura circa due ore. Una volta ricaricati, gli \nauricolari possono riprodurre audio per circa 14 ore di \nseguito.\nEffettuare una connessione Bluetooth\u00ae\nQuando si utilizzano gli auricolari per la prima volta, \u00e8 \nnecessario eseguire la procedura di accoppiamento, che \nconsente ai dispositivi Bluetooth collegati di individuarsi a \nvicenda. Completato l'accoppiamento, i dispositivi Bluetooth \nsi connetteranno automaticamente alla successiva \naccensione.\n\u25a0 Accoppiamento\n1. Tenere premuto il tasto funzione per circa cinque \nsecondi per entrare in standby accoppiamento.\n\u2022 L'indicatore lampeggia alternandosi tra blu e rosso.\n2. Utilizzare il dispositivo Bluetooth connesso per \nvisualizzare la schermata delle impostazioni Bluetooth.\n3. Selezionare \"Yamaha EP-E30A\" nella schermata.\n\u2022 Completato l'accoppiamento, l'indicatore si illumina in \nblu per 20 secondi, quindi lampeggia ogni 5 secondi.\n\u2022 Se l'accoppiamento non viene completato entro due \nminuti, verr\u00e0 considerato come non riuscito e \nl'indicatore si spegner\u00e0.\n\u2022 Per ritentare l'accoppiamento, spegnere e riaccendere il \nprodotto.\nPer maggiori dettagli di utilizzo, visitare il sito web.\n\u25a0 Accensione/spegnimento\nTenere premuto il tasto funzione sul controller per due \nsecondi per accendere/spegnere gli auricolari.\n\u2022 Accesi gli auricolari, l'indicatore lampeggia in rosso a \nindicare l'alimentazione residua della batteria.\nAscolto di musica\nUtilizzo del telefono\nIl marchio e i loghi Bluetooth\u00ae sono marchi registrati di \nBluetooth SIG, Inc. e sono utilizzati da Yamaha Corporation su \nlicenza. Gli altri marchi e nomi commerciali appartengono ai \nrispettivi proprietari. \n<.PEFMMP\u0001QFS\u0001M\b&VSPQB>\nFunzioni di comunicazione wireless\nBluetooth\nFrequenza radio (frequenza operativa): da 2402 MHz a 2480 \nMHz\nPotenza massima in uscita (E.I.R.P): 6,25 dBm\nGracias por adquirir este producto Yamaha.\n\u2022 Para un uso correcto y seguro de este producto, lea primero \neste manual y el \u201cFolleto de seguridad\u201d (documentos \nseparados).\n\u2022 Despu\u00e9s de leer este manual, gu\u00e1rdelo junto con la tarjeta \nde garant\u00eda en un lugar seguro para futuras consultas. Nombres del producto, los accesorios y \nlas partes\na Auriculares intraurales\nb Controlador\n* El micr\u00f3fono est\u00e1 integrado en \u00e9l.\nc Conector de carga\nd Indicador\ne Tecla de funci\u00f3n\nf Olivas, un juego de cada tama\u00f1o: superpeque\u00f1as (SS), \npeque\u00f1as (S), medianas (M), grandes (L)\n* Las olivas de tama\u00f1o M vienen colocadas.\ng Cable de alimentaci\u00f3n USB\nh Gu\u00eda de inicio\ni Gu\u00eda de instrucciones (b\u00e1sica) (este manual)\nj Folleto de seguridad\n Carga\nCon el cable de alimentaci\u00f3n USB incluido, conecte el \nconector de carga de los auriculares al puerto USB de un \nadaptador de corriente USB de los que se venden en \nestablecimientos comerciales (salida de 5 V CC/m\u00edn. 0,5 A). \n\u2022 El indicador se ilumina en rojo durante la carga y se apaga \ncuando la carga se ha completado.\n\u2022 La carga dura aproximadamente dos horas. Una vez \ncargados, los auriculares pueden utilizarse de manera \ncontinuada durante aproximadamente 14 horas.\n Establecimiento de una conexi\u00f3n \nBluetooth\u00ae\nCuando utilice los auriculares por primera vez, deber\u00e1 realizar \nel proceso de emparejamiento, que permite que los \ndispositivos Bluetooth que se est\u00e1n conectando se detecten el \nuno al otro. Una vez emparejados, los dispositivos Bluetooth \nse conectar\u00e1n autom\u00e1ticamente la pr\u00f3xima vez que se \nenciendan.\n\u25a0 Emparejamiento\n1. Mantenga pulsada la tecla de funci\u00f3n al menos cinco \nsegundos para acceder al modo de espera de \nemparejamiento.\n\u2022 El indicador se iluminar\u00e1 alternativamente en azul y \nrojo.\n2. Realice las operaciones necesarias en el dispositivo \nBluetooth conectado para visualizar la pantalla de \najustes Bluetooth.\n3. Seleccione \u201cYamaha EP-E30A\u201d en dicha pantalla de \najustes.\n\u2022 Una vez completado el emparejamiento, el indicador se \niluminar\u00e1 en azul durante 20 segundos, y despu\u00e9s \nparpadear\u00e1 en intervalos de 5 segundos.\n\u2022 Si el emparejamiento no se ha completado en el \ntranscurso de dos minutos, se considerar\u00e1 que ha \nfallado y el indicador se apagar\u00e1.\n\u2022 Para intentar el emparejamiento otra vez, apague este \nproducto y vuelva a encenderlo de nuevo.\nPara m\u00e1s informaci\u00f3n sobre el uso, consulte el sitio web.\n\u25a0 Encendido/apagado\nMantenga pulsada la tecla de funci\u00f3n del controlador durante \ndos segundos para encender/apagar los auriculares.\n\u2022 Despu\u00e9s del encendido de los auriculares, el indicador \nparpadear\u00e1 en rojo para indicar la carga restante de la \nbater\u00eda.\nPara escuchar m\u00fasica\nPara uso con el tel\u00e9fono\nLa marca denominativa y los logotipos de Bluetooth\u00ae son marcas \ncomerciales registradas propiedad de Bluetooth SIG, Inc. y \ncualquier uso de dichas marcas por parte de Yamaha Corporation \nes bajo licencia. Otras marcas comerciales y nombres comerciales \nson propiedad de sus respectivos propietarios. \n<.PEFMP\u0001QBSB\u0001&VSPQB>\nFunciones de comunicaci\u00f3n inal\u00e1mbrica\nBluetooth\nFrecuencia de radio (frecuencia operativa): de 2402 MHz a \n2480 MHz\nPotencia de salida m\u00e1xima (E.I.R.P): 6,25 dBm\nBedankt voor het aanschaffen van dit Yamaha-product.\n\u2022 Lees voor correct en veilig gebruik van dit product eerst \ndeze handleiding en de afzonderlijke 'Veiligheidsbrochure' .\n\u2022 Bewaar de handleiding na het lezen samen met de \ngarantiekaart op een veilige plaats, om in de toekomst te \nkunnen raadplegen.\n Product, accessoire en namen \nonderdelen\na Oortelefoons\nb Controller\n* De microfoon is ingebouwd.\nc Oplaadaansluiting\nd Indicator\ne Functietoets\nf Oordoppen (SS, S, M, L) elk 1 set\n* De oordoppen in maat M zijn bevestigd.\ng USB-netsnoer\nh Starthandleiding\ni Gebruikershandleiding (basis) (deze handleiding)\nj Veiligheidsbrochure\n Opladen\nVerbind de oplaadaansluiting aan de oortelefoons met het \nmeegeleverde USB-netsnoer met de USB-poort van een in de \nhandel verkrijgbare USB-oplader (5 V DC/min. 0,5 A \nuitgangssignaal). \n\u2022 De indicator brandt rood tijdens het oplaadproces en gaat \nuit als het opladen is voltooid.\n\u2022 Opladen duurt ongeveer twee uur. Indien opgeladen \nkunnen de oortelefoons gedurende ongeveer 14 uur \ncontinu afspelen.\n Een Bluetooth\u00ae-verbinding tot stand \nbrengen\nAls de oortelefoons voor de eerste keer worden gebruikt, moet \nhet koppelingsproces worden uitgevoerd, waarmee Bluetooth-\napparaten elkaar voor verbinding kunnen detecteren. Eenmaal \ngekoppeld zullen de Bluetooth-apparaten automatisch worden \nverbonden als ze de volgende keer worden ingeschakeld.\n\u25a0K o p p e l e n\n1. Houd de functietoets minstens vijf seconden ingedrukt \nom de wachtstand koppelen te activeren.\n\u2022 De indicator knippert afwisselend blauw en rood.\n2. Bedien het verbonden Bluetooth-apparaat om het \nscherm Bluetooth-instellingen weer te geven.\n3. Selecteer 'Yamaha EP-E30A' op het weergegeven \nscherm.\n\u2022 Als het koppelen is voltooid, brandt de indicator \ngedurende 20 seconden blauw en gaat daarna \nknipperen in intervallen van 5 seconden.\n\u2022 Als het koppelen niet binnen twee minuten is voltooid, \nwordt dit beschouwd als te zijn mislukt en gaat de \nindicator uit.\n\u2022 Om opnieuw te koppelen schakelt u het product uit en \nweer in.\nKijk voor meer info op de website.\n\u25a0 In-/uitschakelen\nHoud de functietoets op de controller twee seconden \ningedrukt om de oortelefoons in of uit te schakelen.\n\u2022 Nadat de oortelefoons worden ingeschakeld, knippert de \nindicator rood om de resterende accucapaciteit weer te \ngeven.\nDe par sa conception, ce produit n\u2019est pas \u00e9tanche. \nN\u2019exposez pas ce produit \u00e0 des liquides tels que de la sueur, \nde l\u2019eau de pluie ou de l\u2019eau de mer.\nPr\u00e9paratifs\nOp\u00e9rations \u00e9l\u00e9mentaires\nLecture Appliquez une pression br\u00e8ve sur la \ntouche de fonction. \nPause Pendant la lecture, appliquez une \npression br\u00e8ve sur la touche de \nfonction. \nAugmenter le volume\nAppuyez bri\u00e8vement sur la touche +. \nAbaisser le volume Appuyez bri\u00e8vement sur la touche \u2013. \nPasser au morceau \nsuivant\nMaintenez la touche + enfonc\u00e9e \npendant deux secondes. \nRetourner au d\u00e9but \ndu morceau ou passer \nau morceau suivant\nMaintenez la touche \u2013 enfonc\u00e9e \npendant deux secondes. \nR\u00e9pondre \u00e0 un appel Quand vous recevez un appel, \nappliquez une pression br\u00e8ve sur la \ntouche de fonction. \nTerminer un appel Pendant un appel, appliquez une \npression br\u00e8ve sur la touche de \nfonction. \nAugmenter le volume \nde conversation\nAppuyez bri\u00e8vement sur la touche \n+. \nAbaisser le volume de \nconversation\nAppuyez bri\u00e8vement sur la touche \n\u2013. \nIgnorer un appel Quand vous recevez un appel, \nappliquez deux pressions br\u00e8ves \nsur la touche de fonction. \nCaract\u00e9ristiques techniques\nVersion Bluetooth Version 5.0\nProfils pris en charge A2DP, AVRCP, HFP, HSP\nCodecs pris en charge SBC, AAC\nPuissance de sortie RF Classe 2\nPort\u00e9e maximum \nde communication\nEnviron 10 m \n(sans interf\u00e9rences)\nNombres de dispositifs \npouvant \u00eatre enregistr\u00e9s\n8 dispositifs maximum\nBatterie interne rechargeable Batterie au lithium-ion\nDur\u00e9e de charge Environ 2 heures\nAutonomie de lecture \ncontinue\nEnviron 14 heures \nPlage de temp\u00e9rature de \ncharge\n5 \u00e0 40 \u00b0C\nDeutsch\nEinleitung\nDieses Ger\u00e4t ist nicht wasserdicht. Setzen Sie dieses Ger\u00e4t \nkeinen Fl\u00fcssigkeiten wie Schwei\u00df, Regenwasser oder \nMeerwasser aus.\nVorbereitung\nGrundlegende Bedienung\nWiedergabe Funktionstaste einmal kurz \ndr\u00fccken \nPause W\u00e4hrend der Wiedergabe \nFunktionstaste einmal kurz \ndr\u00fccken \nLautst\u00e4rke anheben +-Taste einmal kurz dr\u00fccken \nLautst\u00e4rke senken \u2212-Taste einmal kurz dr\u00fccken \nVorlauf zum n\u00e4chsten \nTitel\n+-Taste zwei Sekunden lang \ngedr\u00fcckt halten \nR\u00fccklauf zum Anfang \ndes Titels oder zum \nvorigen Titel\n\u2212-Taste zwei Sekunden lang \ngedr\u00fcckt halten \nAnruf empfangen Bei eingehendem Anruf \nFunktionstaste einmal dr\u00fccken \nAnruf beenden W\u00e4hrend des Anrufs \nFunktionstaste einmal kurz \ndr\u00fccken \nGespr\u00e4chslautst\u00e4rke \nanheben\n+-Taste einmal kurz dr\u00fccken \nGespr\u00e4chslautst\u00e4rke \nsenken \n\u2212-Taste einmal kurz dr\u00fccken \nAnruf ignorieren Bei eingehendem Anruf \nFunktionstaste zweimal kurz \ndr\u00fccken \nTechnische Daten\nBluetooth-Version 5.0\nUnterst\u00fctzte Profile A 2 DP,  AVR C P, H FP,  HS P\nUnterst\u00fctzte Codecs SBC, AAC\nRF-Ausgangsleistung Klasse 2\nMax. Reichweite ca. 10 m (ohne St\u00f6rungen)\nAnzahl registrierbarer Ger\u00e4te max. 8\nInterner Akku Lithium-Ionen-Batterie\nDE\nLadezeit ca. 2 Stunden\nKontinuierliche Wiedergabezeit ca. 14 Stunden \nLadetemperatur 5\u201340 \u00b0C\nSvenska\nIntroduktion\nDenna produkt \u00e4r inte designad f\u00f6r att vara vattent\u00e4t. Uts\u00e4tt \ninte produkten f\u00f6r v\u00e4tskor som svett, regnvatten eller \nhavsvatten.\nF\u00f6rberedande\nGrundl\u00e4ggande funktioner\nSpela upp Tryck kort en g\u00e5ng p\u00e5 \nfunktionsknappen. \nPausa Under uppspelning, tryck kort en \ng\u00e5ng p\u00e5 funktionsknappen. \nH\u00f6ja volymen Tryck kort p\u00e5 + knappen en g\u00e5ng. \nS\u00e4nka volymen Tryck kort p\u00e5 - knappen en g\u00e5ng. \nHoppa till n\u00e4sta l\u00e5t H\u00e5ll + knappen intryckt i tv\u00e5 \nsekunder. \nG\u00e5 tillbaka till b\u00f6rjan \nav l\u00e5ten eller hoppa till \nf\u00f6reg\u00e5ende l\u00e5t\nH\u00e5ll - knappen intryckt i tv\u00e5 \nsekunder. \nTa ett telefonsamtal N\u00e4r ett telefonsamtal f\u00e5s, tryck kort \np\u00e5 funktionsknappen en g\u00e5ng. \nAvsluta ett \ntelefonsamtal\nUnder ett telefonsamtal, tryck kort \np\u00e5 funktionsknappen en g\u00e5ng. \nH\u00f6j ljudvolymen Tryck kort p\u00e5 + knappen en g\u00e5ng. \nS\u00e4nk ljudvolymen Tryck kort p\u00e5 - knappen en g\u00e5ng. \nAvvisa ett samtal N\u00e4r ett telefonsamtal f\u00e5s, tryck kort \np\u00e5 funktionsknappen tv\u00e5 g\u00e5nger. \nSpecifikationer\nBluetooth-version Version 5.0\nProfiler som st\u00f6ds A2DP, AVRCP, HFP, HSP\nKodek som st\u00f6ds SBC, AAC\nRF-utg\u00e5ngseffekt Klass 2\nMax \nkommunikationsr\u00e4ckvidd\nUngef\u00e4r 10 m \n(utan st\u00f6rningar)\nAntal enheter som kan \nregistreras\nUpp till 8 enheter\nSV\nInternt uppladdningsbart \nbatteri\nLitium jon batteri\nLaddningstid Cirka 2 timmar\nKontinuerlig \nuppspelningstid\nCirka 14 timmar \nLaddningstemperatur 5 \u2013 40\u00b0C\nItaliano\nIntroduzione\nIl prodotto non \u00e8 progettato per essere impermeabile. Non \nesporre il prodotto a liquidi come sudore, acqua piovana o \nacqua di mare.\nPreparazione\nOperazioni di base\nRiproduci Premere brevemente una volta il \ntasto. \nPausa Premere brevemente una volta il \ntasto durante la riproduzione. \nAumentare il volume Premere brevemente una volta il \ntasto +. \nAbbassare il volume Premere brevemente una volta il \ntasto \u2212. \nSaltare al brano \nsuccessivo\nTenere premuto il tasto + per due \nsecondi. \nTornare all'inizio del \nbrano o saltare al \nbrano precedente\nTenere premuto il tasto \u2212 per due \nsecondi. \nRicevere chiamate All'arrivo di una chiamata, \npremere brevemente una volta il \ntasto funzione. \nTerminare chiamate Durante la conversazione, premere \nuna volta il tasto funzione. \nAumentare il volume \ndella conversazione\nPremere brevemente una volta il \ntasto +. \nAbbassare il volume \ndella conversazione\nPremere brevemente una volta il \ntasto \u2212. \nIgnorare chiamate All'arrivo di una chiamata, \npremere brevemente due volte il \ntasto funzione. \nIT\nSpecifiche\nVersione Bluetooth Versione 5.0\nProfili supportati A2DP , AVRCP , HFP , HSP\nCodec supportati SBC, AAC\nPotenza RF in uscita Classe 2\nMassima portata di \ncomunicazione\nCirca 10 m \n(senza interferenze)\nNumero di dispositivi \nregistrabili\nFino a 8 dispositivi\nBatteria interna ricaricabile Batteria agli ioni di litio\nTempo di ricarica Circa 2 ore\nTempo di riproduzione \ncontinuata\nCirca 14 ore \nTemperatura di ricarica 5 \u2013 40 \u00b0C\nEspa\u00f1ol\nIntroducci\u00f3n\nEste producto no est\u00e1 dise\u00f1ado para ser resistente al agua. \nNo exponga este producto a l\u00edquidos como el sudor, la lluvia \no el agua de mar.\nPreparaci\u00f3n\nOperaciones b\u00e1sicas\nReproducir Pulse brevemente la tecla de \nfunci\u00f3n una vez. \nPoner en pausa Durante la reproducci\u00f3n, pulse \nbrevemente la tecla de funci\u00f3n una \nvez. \nSubir el volumen Pulse brevemente la tecla + una vez. \nBajar el volumen Pulse brevemente la tecla \u2013 una vez. \nSaltar a la canci\u00f3n \nsiguiente\nMantenga pulsada la tecla + durante \ndos segundos. \nES\nVolver al principio de \nla canci\u00f3n o saltar a \nla canci\u00f3n anterior\nMantenga pulsada la tecla \u2013 durante \ndos segundos. \nRecibir una llamada \ntelef\u00f3nica\nCuando entre una llamada \ntelef\u00f3nica, pulse brevemente la \ntecla de funci\u00f3n una vez. \nTerminar una llamada \ntelef\u00f3nica\nDurante la llamada telef\u00f3nica, \npulse brevemente la tecla de \nfunci\u00f3n una vez. \nSubir el volumen de la \nconversaci\u00f3n\nPulse brevemente la tecla + una \nvez. \nBajar el volumen de la \nconversaci\u00f3n\nPulse brevemente la tecla \u2013 una \nvez. \nIgnorar una llamada Cuando entre una llamada \ntelef\u00f3nica, pulse brevemente la \ntecla de funci\u00f3n dos veces. \nEspecificaciones\nVersi\u00f3n de Bluetooth Versi\u00f3n 5.0\nPerfiles compatibles A2DP , AVRCP , HFP , HSP\nC\u00f3decs compatibles SBC, AAC\nPotencia de salida de RF Clase 2\nAlcance de comunicaci\u00f3n \nm\u00e1ximo\nAproximadamente 10 m \n(sin interferencias)\nN\u00famero de dispositivos \nque pueden registrarse\nHasta 8 dispositivos\nBater\u00eda interna recargable Bater\u00eda de iones de litio\nTiempo de carga Aproximadamente 2 horas\nTiempo de reproducci\u00f3n \ncontinua\nAproximadamente 14 horas \nTemperatura de carga 5 \u2013 40 \u00b0C\nNederlands\nIntroductie\nDit product is niet ontworpen om waterdicht te zijn. Stel dit \nproduct niet bloot aan vloeistoffen als transpiratievocht, \nregenwater of zeewater.\nVOORBEREIDING\nNLThank you for purchasing this Yamaha product.\n\u2022 For correct and safe use of this product, be sure to first read \nthis manual and the separate \u201cSafety Brochure\u201d .\n\u2022 After reading this manual, keep it together with the \nwarranty card in a safe place for future reference.\n Product, Accessory and Part Names\na Earphones\nb Controller\n* The microphone is built in.\nc Charging connector\nd Indicator\ne Function key\nf Eartips (SS, S, M, L) 1 set each\n* The M-size eartips are attached.\ng USB power cable\nh Startup Guide\ni User\u2019s Guide (Basic) (this manual)\nj Safety Brochure\n Charging\nWith the included USB power cable, connect the charging \nconnector on the earphones to the USB port on a \ncommercially available USB power adaptor (5 V DC/min. 0.5 A \noutput). \n\u2022 The indicator is lit in red during charging and goes off when \ncharging is completed.\n\u2022 Charging takes about two hours. When charged, the \nearphones can play back continuously for approximately \n14 hours.\n Making a Bluetooth\u00ae connection\nWhen using the earphones for the first time, the pairing \nprocess, which allows Bluetooth devices being connected to \ndetect each other, must be performed. Once paired, the \nBluetooth devices will be automatically connected the next \ntime they are turned on.\n\u25a0P a i r i n g\n1. Hold down the function key for at least five seconds to \nenter pairing standby.\n\u2022 The indicator flashes alternately in blue and red.\n2. Operate the connected Bluetooth device to display the \nBluetooth setting screen.\n3. Select \u201cYamaha EP-E30A\u201d from the displayed screen.\n\u2022 When pairing has completed, the indicator lights up in \nblue for 20 seconds, then flashes at 5-second intervals.\n\u2022 If pairing has not completed within two minutes, it will \nbe considered to have failed, and the indicator goes off. \n\u2022 To try pairing again, turn the earphones off, then on \nagain.\nFor details on use, refer to the website.\n\u25a0 Turning on/off\nHold down the function key on the controller for two seconds \nto turn the earphones on/off.\n\u2022 After the earphones turn on, the indicator flashes in red to \nindicate the remaining battery power.\nListening to music\nUsing the phone\nThe Bluetooth\u00ae word mark and logos are registered \ntrademarks owned by Bluetooth SIG, Inc. and any use of such \nmarks by Yamaha Corporation is under license. Other \ntrademarks and trade names are those of their respective \nowners. \n<&VSPQF\u0001NPEFM>\nWireless communication functions\nBluetooth\nRadio Frequency (Operational Frequency): 2402 MHz to 2480 \nMHz\nMaximum Output Power (E.I.R.P): 6.25 dBm\nNous vous remercions d'avoir choisi ce produit Yamaha.\n\u2022 Pour garantir une utilisation correcte et s\u00fbre de ce produit, \nveillez \u00e0 lire ce manuel ainsi que la \u00ab Brochure sur la \ns\u00e9curit\u00e9 \u00bb en annexe.\na\nL\n2\n1 3\nR\nb\nf\ng\nhi\nA B\nC\nT\ne\nc\nd\nj\nEarphones\nCasques Intra-auriculaires\n\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438 \u0432\u0441\u0442\u0430\u0432\u043d\u044b\u0435\n\u2a8c\u7e70\u430b\u7e70\u512c\n\u7e70\u9df9\u430b\u7e70\u5680\nUser Guide (Basic) \nMode d'emploi (\u00e9l\u00e9mentaire)\nBedienungsanleitung (Grundausgabe)\nAnv\u00e4ndarguide (Grundl\u00e4ggande)\nGuida dell'utente (base)\nManual de usuario (b\u00e1sica)\nGebruikershandleiding (basis)\nGuia do Usu\u00e1rio (b\u00e1sico)\nP\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f (\u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0435)\n\u69ab\u4893\u49d7\u2d9f\ue6ae\u34f9\u5117\ue6af\n\u2733\u69ab\u49d8\u2d9f\ue6ae\u34f9\u5117\u4cea\u26fc\ue6af\n\ucea7\ud4a4\ucf1f\uc880\ucf17\t\ubc2b\ucaf3\nAV19-0188\nYamaha Global Site\nhttps://www.yamaha.com/\nYamaha Downloads\nhttps://download.yamaha.com/\nManual Development Group\n\u00a9 2019 Yamaha Corporation\nPublished 10/2020\n2020 \u5e74 10 \u6708 \u53d1\u884c    AMOD-B0\nEnglish\nIntroduction\nThis product is not designed to be waterproof. Do not \nexpose this product to liquids such as sweat, rainwater or \nseawater.\nPreparation\nEN Basic operations\nPlay Briefly press the function key once. \nPause During playback, briefly press the \nfunction key once. \nRaise the volume Briefly press the + key once. \nLower the volume Briefly press the \u2013 key once. \nSkip to the next song Hold down the + key for two \nseconds. \nGo back to the \nbeginning of the song, \nor skip to the previous \nsong\nHold down the \u2013 key for two \nseconds. \nReceive a phone call When a phone call comes in, \nbriefly press the function key once. \nEnd a phone call During the phone call, briefly press \nthe function key once. \nRaise the conversation \nvolume\nBriefly press the + key once. \nLower the \nconversation volume\nBriefly press the \u2013 key once. \nIgnore a call When a phone call comes in, \nbriefly press the function key twice. \nSpecifications\nBluetooth version Version 5.0\nSupported profiles A 2 D P, AV R C P,  H F P, H SP\nSupported codecs SBC, AAC\nRF Output Power Class 2\nMaximum \ncommunication range\nApproximately 10 m \n(without interference)\nNumber of devices \nthat can be registered\nUp to 8 devices\nInternal rechargeable \nbattery\nLithium-ion battery\nCharging time Approximately 2 hours\nContinuous playback time Approximately 14 hours \nCharging temperature 5 \u2013 40 \u00b0C\nFran\u00e7ais\nIntroduction\nFR\n\u2022 Quand vous avez fini de consulter ce manuel, rangez-le en \nlieu s\u00fbr avec la carte de garantie pour toute r\u00e9f\u00e9rence \nult\u00e9rieure. Produit, accessoires et noms des \n\u00e9l\u00e9ments\na Casques intra-auriculaires\nb Commande\n* Comporte un microphone int\u00e9gr\u00e9.\nc Connecteur de charge\nd T\u00e9moin\ne Touche de fonction\nf Embouts (SS, S, M, L) 1 paire de chaque taille\n* Les embouts de taille M sont fix\u00e9s aux casques.\ng C\u00e2ble d'alimentation USB\nh Guide de d\u00e9marrage\ni Guide (\u00e9l\u00e9mentaire) de l'utilisateur (ce document)\nj Brochure sur la s\u00e9curit\u00e9\n Charge\nReliez avec le c\u00e2ble d'alimentation USB fourni le connecteur \nde charge des casques au port USB d'un adaptateur USB \ndisponible dans le commerce (5 V CC/ 0,5 A min.). \n\u2022 Le t\u00e9moin s'allume en rouge pendant la charge et s'\u00e9teint \nquand la charge est termin\u00e9e.\n\u2022 La charge prend environ deux heures. Une fois charg\u00e9s, les \ncasques ont une autonomie continue d'environ 14 heures.\n \u00c9tablir une connexion Bluetooth\u00ae\nLors de l'utilisation initiale des casques, il convient d'ex\u00e9cuter \nl'appairage, qui permet la d\u00e9tection mutuelle des dispositifs \nBluetooth connect\u00e9s. Une fois appair\u00e9s, les dispositifs \nBluetooth s'interconnectent automatiquement \u00e0 chaque mise \nsous tension.\n\u25a0 Appairage\n1. Maintenez la touche de fonction enfonc\u00e9e pendant au \nmoins cinq secondes pour activer l'attente \nd'appairage.\n\u2022 Le t\u00e9moin clignote alternativement en rouge et bleu.\n2. Affichez l'\u00e9cran des r\u00e9glages Bluetooth sur le dispositif \n\u00e0 connecter.\n3. S\u00e9lectionnez \u00ab Yamaha EP-E30A \u00bb sur cet \u00e9cran.\n\u2022 Quand l'appairage est termin\u00e9, le t\u00e9moin reste allum\u00e9 \nen bleu pendant 20 secondes puis clignote \u00e0 intervalles \nde 5 secondes.\n\u2022 Si l\u2019appairage n\u2019est pas termin\u00e9 dans un d\u00e9lai de deux \nminutes, la tentative est consid\u00e9r\u00e9e comme rat\u00e9e et le \nt\u00e9moin s\u2019\u00e9teint.\n\u2022 Mettez les casques hors tension puis \u00e0 nouveau sous \ntension, et essayez \u00e0 nouveau d\u2019appairer les dispositifs.\nPour des d\u00e9tails sur l\u2019utilisation, visitez notre site internet.\n\u25a0 Mise sous tension/hors tension\nMaintenez la touche de fonction de la commande enfonc\u00e9e \npendant plus de deux secondes pour mettre les casques sous \ntension/hors tension.\n\u2022 Apr\u00e8s la mise sous tension des casques, le t\u00e9moin clignote \nen rouge pour indiquer l'autonomie r\u00e9siduelle de la batterie.\n\u00c9coute de musique\nUtilisation du t\u00e9l\u00e9phone\nL'appellation et les logos Bluetooth\u00ae sont des marques \nd\u00e9pos\u00e9es de Bluetooth SIG, Inc. et toute utilisation de ces \nmarques par Yamaha Corporation fait l'objet d'une licence. Les \nautres marques et appellations commerciales sont la \npropri\u00e9t\u00e9 de leurs d\u00e9tenteurs respectifs. \n<.PE\u00c8MF\u0001QPVS\u0001M\b&VSPQF>\nFonctions de communication sans fil\nBluetooth\nFr\u00e9quence radio (fr\u00e9quence op\u00e9rationnelle): 2402 MHz \u00e0 2480 \nMHz\nPuissance de sortie maximum (E.I.R.P): 6,25 dBm\nVielen Dank, dass Sie sich f\u00fcr dieses Produkt von Yamaha \nentschieden haben.\n\u2022 Lesen Sie zuerst diese Anleitung und die gesonderte \n\u201eSicherheitsbrosch\u00fcre\u201c, um eine korrekte und sichere \nVerwendung dieses Produkts zu gew\u00e4hrleisten.\n\u2022 Bewahren Sie diese Anleitung nach dem Lesen zusammen \nmit der Garantiekarte f\u00fcr sp\u00e4tere Nachschlagzwecke \nsorgf\u00e4ltig auf.\n Produkt-, Zubeh\u00f6r- und \nTeilebezeichnungen\na Ohrh\u00f6rer\nb Steuereinheit\n* Das Mikrofon ist eingebaut.\nc Ladeanschluss\nd Anzeige\ne Funktionstaste\nf Ohrst\u00f6psel (SS, S, M, L) je 1 Satz\n* H\u00f6rer werkseitig mit St\u00f6pseln der Gr\u00f6\u00dfe M versehen.\ng USB-Stromversorgungskabel\nh Kurzanleitung\ni Grundlegende Bedienungsanleitung (vorliegend)\nj Sicherheitsbrosch\u00fcre\n Laden\nSchlie\u00dfen Sie das zugeh\u00f6rige USB-Stromversorgungskabel an \nden Ladeanschluss der Ohrh\u00f6rer und an den USB-Anschluss \neines handels\u00fcblichen USB-Netzteils (mit 5 VDC/min. 0,5 A \nAusgang) an. \n\u2022 Die Anzeige leuchtet w\u00e4hrend des Ladens rot und erlischt, \nwenn der Ladevorgang abgeschlossen ist.\n\u2022 Der Ladevorgang dauert ungef\u00e4 hr zwei Stunden. Nach dem \nLaden bieten die Ohrh\u00f6rer eine ununterbrochene \nWiedergabeleistung von ca. 14 Stunden.\n Bluetooth\u00ae-Verbindung herstellen\nWenn Sie die Ohrh\u00f6rer zum ersten Mal verwenden, muss ein \nPairing ausgef\u00fchrt werden, d.h. der Kopplungsvorgang, bei \ndem miteinander zu verbindende Bluetooth-Ger\u00e4te einander \nerkennen. Nach dem Pairing werden die Bluetooth-Ger\u00e4te \nbeim n\u00e4chsten Einschalten automatisch verbunden.\n\u25a0P a i r i n g\n1. Halten Sie die Funktionstaste mindestens f\u00fcnf \nSekunden lang gedr\u00fcckt, um auf Pairing-Standby \numzuschalten.\n\u2022 Die Anzeige blinkt abwechselnd blau und rot.\n2. Bedienen Sie das verbundene Bluetooth-Ger\u00e4t, um den \nBluetooth-Einstellungsbildschirm anzuzeigen.\n3. W\u00e4hlen Sie \u201e Yamaha EP-E30A\u201c im angezeigten \nBildschirm aus.\n\u2022 Nach Abschluss des Pairings leuchtet die Anzeige 20 \nSekunden lang blau und blinkt dann im 5-Sekunden-\nIntervall.\n\u2022 Ist das Pairing nicht innerhalb von zwei Minuten \nabgeschlossen, gilt es als fehlgeschlagen, und die \nAnzeige erlischt.\n\u2022 Um das Pairing erneut auszuf\u00fchren, das Ger\u00e4t aus- und \nwieder einschalten.\nN\u00e4heres zur Benutzung findet sich auf der Website.\n\u25a0E i n - / A u s s c h a l t e n\nHalten Sie die Funktionstaste der Steuereinheit zwei Sekunden \nlang gedr\u00fcckt, um die Ohrh\u00f6rer ein- oder auszuschalten.\n\u2022 Nach dem Einschalten der Ohrh\u00f6rer blinkt die Anzeige rot, \num die verbleibende Akkuleistung anzuzeigen.\nMusik h\u00f6ren\nTelefonieren\nBluetooth\u00ae-Begriff und -Logo sind eingetragene Warenzeichen \nvon Bluetooth SIG, Inc. deren Verwendung durch Yamaha \nCorporation unter Lizenz erfolgt. Alle anderen Warenzeichen \nbzw. eingetragenen Warenzeichen geh\u00f6ren den \nentsprechenden Firmen. \n<&VSPQB\u000e.PEFMM>\nDrahtlos-Kommunikationsfunktionen\nBluetooth\nEmpfangsfrequenz (Arbeitsfrequenz): 2.402\u20132.480 MHz\nMaximale Ausgangsleistung (E.I.R.P): 6,25 dBm\nTack f\u00f6r att du k\u00f6pt denna produkt fr\u00e5n Yamaha.\n\u2022 F\u00f6r korrekt och s\u00e4ker anv\u00e4ndning av denna produkt, ska du \nf\u00f6rst l\u00e4sa denna manual och \u201ds\u00e4kerhetsbroschyren\u201d .\n\u2022 N\u00e4r du har l\u00e4st denna manual, ska du f\u00f6rvara den \ntillsammans med garantikortet p\u00e5 en s\u00e4ker plats f\u00f6r \nframtida referens.\n Namn p\u00e5 produkt, tillbeh\u00f6r och delar\na \u00d6ronsn\u00e4ckor\nb Kontroller\n* Inbyggd mikrofon.\nc Kontaktdon f\u00f6r laddning\nd Indikator\ne Funktionsknapp\nf \u00d6ronproppar (SS, S, M, L) 1 upps\u00e4ttning vardera\n* M-storleken p\u00e5 \u00f6ronpropparna \u00e4r anslutna.\ng USB-str\u00f6mkabel\nh Startguide\ni Anv\u00e4ndarmanual (Grundl\u00e4ggande) ( i denna manual)\nj S\u00e4kerhetsbroschyr\n Ladda\nMed den medf\u00f6ljande USB-str\u00f6mkabeln ansluter du \nladdningskontakten p\u00e5 \u00f6ronsn\u00e4ckorna till USB-porten p\u00e5 en \nkommersiellt tillg\u00e4nglig USB-str\u00f6madapter (5 V DC / min. 0,5 A \nutg\u00e5ng) n\u00e4r den ska laddas. \n\u2022 Indikatorn t\u00e4nds r\u00f6tt under laddning och slocknar n\u00e4r \nladdningen \u00e4r klar.\n\u2022 Laddning tar cirka tv\u00e5 timmar. N\u00e4r de laddas kan \n\u00f6ronsn\u00e4ckorna spelas kontinuerligt i cirka 14 timmar.\n Uppr\u00e4tta en Bluetooth\u00ae-anslutning\nN\u00e4r du anv\u00e4nder \u00f6ronsn\u00e4ckorna f\u00f6r f\u00f6rsta g\u00e5ngen, ska \nprocessen f\u00f6r ihopparning, som g\u00f6r det m\u00f6jligt f\u00f6r Bluetooth-\nenheter som \u00e4r anslutna att uppt\u00e4cka varandra, utf\u00f6ras. N\u00e4r de \nhar parats ihop, ansluts Bluetooth-enheterna automatiskt \nn\u00e4sta g\u00e5ng de sl\u00e5s p\u00e5.\n\u25a0 Ihopparning\n1. H\u00e5ll funktionsknappen intryckt i minst fem sekunder \nf\u00f6r att ange ihopparning i standby.\n\u2022 Indikatorn blinkar v\u00e4xelvis bl\u00e5tt och r\u00f6tt.\n2. Anv\u00e4nd den anslutna Bluetooth-enheten f\u00f6r att visa \ninst\u00e4llningssk\u00e4rmen f\u00f6r Bluetooth.\n3. V\u00e4lj \u201dYamaha EP-E30A\u201d fr\u00e5n den visade sk\u00e4rmen.\n\u2022 N\u00e4r ihopparningen \u00e4r klar, t\u00e4nds indikatorn bl\u00e5 i 20 \nsekunder och blinkar sedan med 5 sekunders intervall.\n\u2022 Om ingen ihopparning har slutf\u00f6rts inom tv\u00e5 minuter, \nkommer det anses ha misslyckats och indikatorn \nslocknar.\n\u2022 F\u00f6r att f\u00f6rs\u00f6ka para ihop igen, ska produkten sl\u00e5s av och \nsedan p\u00e5 igen.\nF\u00f6r mer information, g\u00e5 till webbplatsen.\n\u25a0S l \u00e5  p \u00e5 / a v\nH\u00e5ll funktionsknappen intryckt p\u00e5 kontroller i tv\u00e5 sekunder f\u00f6r \natt sl\u00e5 p\u00e5/st\u00e4nga av \u00f6ronsn\u00e4ckorna.\n\u2022 N\u00e4r \u00f6ronsn\u00e4ckorna \u00e4r p\u00e5slagna, blinkar indikatorn r\u00f6d f\u00f6r \natt ange den \u00e5terst\u00e5ende batterikapaciteten.\nLyssna p\u00e5 musik\nAnv\u00e4nda telefonen\nBluetooth\u00ae-ordm\u00e4rket och logotyper \u00e4r registrerade \nvarum\u00e4rken som \u00e4gs av Bluetooth SIG, Inc., och all \nanv\u00e4ndning av s\u00e5dana varum\u00e4rken som tillh\u00f6r Yamaha \nCorporation \u00e4r under licens. Andra varum\u00e4rken och \nhandelsnamn tillh\u00f6r deras respektive \u00e4gare. \n<&VSPQB\u000eNPEFMM>\nTr\u00e5dl\u00f6sa kommunikationsfunktioner\nBluetooth\nRadiofrekvens (Operativ frekvens): 2402 MHz till 2480 MHz\nMaximal utg\u00e5ngseffekt (E.I.R.P): 6,25 dBm\nGrazie per aver acquistato questo prodotto Yamaha.\n\u2022 Per un utilizzo corretto e sicuro del prodotto, assicurarsi di \nleggere prima questo manuale e la \"Brochure sulla \nsicurezza\" .\n\u2022 Dopo aver letto il manuale, conservarlo insieme alla scheda \ndi garanzia in un luogo sicuro per future consultazioni.Nomi del prodotto, degli accessori e \ndelle parti\na Auricolari\nb Controller\n* Il microfono \u00e8 incorporato.\nc Connettore di ricarica\nd Indicatore\ne Tasto funzione\nf Cuscinetti (SS, S, M, L) 1 set ciascuno\n* I cuscinetti di dimensioni M sono gi\u00e0 fissati agli auricolari\nin vendita.\ng Cavo di alimentazione USB\nh Guida introduttiva\ni Guida dell'utente (base) (questo manuale)\nj Brochure sulla sicurezza\nRicarica\nCollegare il cavo di alimentazione USB incluso al connettore di \nricarica sugli auricolari e alla porta USB di un alimentatore \ndisponibile in commercio (5 V CC/min. 0,5 A in uscita). \n\u2022 L'indicatore \u00e8 acceso durante la ricarica e si spegne una \nvolta che la ricarica \u00e8 completata.\n\u2022 La ricarica dura circa due ore. Una volta ricaricati, gli \nauricolari possono riprodurre audio per circa 14 ore di \nseguito.\nEffettuare una connessione Bluetooth\u00ae\nQuando si utilizzano gli auricolari per la prima volta, \u00e8 \nnecessario eseguire la procedura di accoppiamento, che \nconsente ai dispositivi Bluetooth collegati di individuarsi a \nvicenda. Completato l'accoppiamento, i dispositivi Bluetooth \nsi connetteranno automaticamente alla successiva \naccensione.\n\u25a0 Accoppiamento\n1. Tenere premuto il tasto funzione per circa cinque \nsecondi per entrare in standby accoppiamento.\n\u2022 L'indicatore lampeggia alternandosi tra blu e rosso.\n2. Utilizzare il dispositivo Bluetooth connesso per \nvisualizzare la schermata delle impostazioni Bluetooth.\n3. Selezionare \"Yamaha EP-E30A\" nella schermata.\n\u2022 Completato l'accoppiamento, l'indicatore si illumina in \nblu per 20 secondi, quindi lampeggia ogni 5 secondi.\n\u2022 Se l'accoppiamento non viene completato entro due \nminuti, verr\u00e0 considerato come non riuscito e \nl'indicatore si spegner\u00e0.\n\u2022 Per ritentare l'accoppiamento, spegnere e riaccendere il \nprodotto.\nPer maggiori dettagli di utilizzo, visitare il sito web.\n\u25a0 Accensione/spegnimento\nTenere premuto il tasto funzione sul controller per due \nsecondi per accendere/spegnere gli auricolari.\n\u2022 Accesi gli auricolari, l'indicatore lampeggia in rosso a \nindicare l'alimentazione residua della batteria.\nAscolto di musica\nUtilizzo del telefono\nIl marchio e i loghi Bluetooth\u00ae sono marchi registrati di \nBluetooth SIG, Inc. e sono utilizzati da Yamaha Corporation su \nlicenza. Gli altri marchi e nomi commerciali appartengono ai \nrispettivi proprietari. \n<.PEFMMP\u0001QFS\u0001M\b&VSPQB>\nFunzioni di comunicazione wireless\nBluetooth\nFrequenza radio (frequenza operativa): da 2402 MHz a 2480 \nMHz\nPotenza massima in uscita (E.I.R.P): 6,25 dBm\nGracias por adquirir este producto Yamaha.\n\u2022 Para un uso correcto y seguro de este producto, lea primero \neste manual y el \u201cFolleto de seguridad\u201d (documentos \nseparados).\n\u2022 Despu\u00e9s de leer este manual, gu\u00e1rdelo junto con la tarjeta \nde garant\u00eda en un lugar seguro para futuras consultas. Nombres del producto, los accesorios y \nlas partes\na Auriculares intraurales\nb Controlador\n* El micr\u00f3fono est\u00e1 integrado en \u00e9l.\nc Conector de carga\nd Indicador\ne Tecla de funci\u00f3n\nf Olivas, un juego de cada tama\u00f1o: superpeque\u00f1as (SS), \npeque\u00f1as (S), medianas (M), grandes (L)\n* Las olivas de tama\u00f1o M vienen colocadas.\ng Cable de alimentaci\u00f3n USB\nh Gu\u00eda de inicio\ni Gu\u00eda de instrucciones (b\u00e1sica) (este manual)\nj Folleto de seguridad\n Carga\nCon el cable de alimentaci\u00f3n USB incluido, conecte el \nconector de carga de los auriculares al puerto USB de un \nadaptador de corriente USB de los que se venden en \nestablecimientos comerciales (salida de 5 V CC/m\u00edn. 0,5 A). \n\u2022 El indicador se ilumina en rojo durante la carga y se apaga \ncuando la carga se ha completado.\n\u2022 La carga dura aproximadamente dos horas. Una vez \ncargados, los auriculares pueden utilizarse de manera \ncontinuada durante aproximadamente 14 horas.\n Establecimiento de una conexi\u00f3n \nBluetooth\u00ae\nCuando utilice los auriculares por primera vez, deber\u00e1 realizar \nel proceso de emparejamiento, que permite que los \ndispositivos Bluetooth que se est\u00e1n conectando se detecten el \nuno al otro. Una vez emparejados, los dispositivos Bluetooth \nse conectar\u00e1n autom\u00e1ticamente la pr\u00f3xima vez que se \nenciendan.\n\u25a0 Emparejamiento\n1. Mantenga pulsada la tecla de funci\u00f3n al menos cinco \nsegundos para acceder al modo de espera de \nemparejamiento.\n\u2022 El indicador se iluminar\u00e1 alternativamente en azul y \nrojo.\n2. Realice las operaciones necesarias en el dispositivo \nBluetooth conectado para visualizar la pantalla de \najustes Bluetooth.\n3. Seleccione \u201cYamaha EP-E30A\u201d en dicha pantalla de \najustes.\n\u2022 Una vez completado el emparejamiento, el indicador se \niluminar\u00e1 en azul durante 20 segundos, y despu\u00e9s \nparpadear\u00e1 en intervalos de 5 segundos.\n\u2022 Si el emparejamiento no se ha completado en el \ntranscurso de dos minutos, se considerar\u00e1 que ha \nfallado y el indicador se apagar\u00e1.\n\u2022 Para intentar el emparejamiento otra vez, apague este \nproducto y vuelva a encenderlo de nuevo.\nPara m\u00e1s informaci\u00f3n sobre el uso, consulte el sitio web.\n\u25a0 Encendido/apagado\nMantenga pulsada la tecla de funci\u00f3n del controlador durante \ndos segundos para encender/apagar los auriculares.\n\u2022 Despu\u00e9s del encendido de los auriculares, el indicador \nparpadear\u00e1 en rojo para indicar la carga restante de la \nbater\u00eda.\nPara escuchar m\u00fasica\nPara uso con el tel\u00e9fono\nLa marca denominativa y los logotipos de Bluetooth\u00ae son marcas \ncomerciales registradas propiedad de Bluetooth SIG, Inc. y \ncualquier uso de dichas marcas por parte de Yamaha Corporation \nes bajo licencia. Otras marcas comerciales y nombres comerciales \nson propiedad de sus respectivos propietarios. \n<.PEFMP\u0001QBSB\u0001&VSPQB>\nFunciones de comunicaci\u00f3n inal\u00e1mbrica\nBluetooth\nFrecuencia de radio (frecuencia operativa): de 2402 MHz a \n2480 MHz\nPotencia de salida m\u00e1xima (E.I.R.P): 6,25 dBm\nBedankt voor het aanschaffen van dit Yamaha-product.\n\u2022 Lees voor correct en veilig gebruik van dit product eerst \ndeze handleiding en de afzonderlijke 'Veiligheidsbrochure' .\n\u2022 Bewaar de handleiding na het lezen samen met de \ngarantiekaart op een veilige plaats, om in de toekomst te \nkunnen raadplegen.\n Product, accessoire en namen \nonderdelen\na Oortelefoons\nb Controller\n* De microfoon is ingebouwd.\nc Oplaadaansluiting\nd Indicator\ne Functietoets\nf Oordoppen (SS, S, M, L) elk 1 set\n* De oordoppen in maat M zijn bevestigd.\ng USB-netsnoer\nh Starthandleiding\ni Gebruikershandleiding (basis) (deze handleiding)\nj Veiligheidsbrochure\n Opladen\nVerbind de oplaadaansluiting aan de oortelefoons met het \nmeegeleverde USB-netsnoer met de USB-poort van een in de \nhandel verkrijgbare USB-oplader (5 V DC/min. 0,5 A \nuitgangssignaal). \n\u2022 De indicator brandt rood tijdens het oplaadproces en gaat \nuit als het opladen is voltooid.\n\u2022 Opladen duurt ongeveer twee uur. Indien opgeladen \nkunnen de oortelefoons gedurende ongeveer 14 uur \ncontinu afspelen.\n Een Bluetooth\u00ae-verbinding tot stand \nbrengen\nAls de oortelefoons voor de eerste keer worden gebruikt, moet \nhet koppelingsproces worden uitgevoerd, waarmee Bluetooth-\napparaten elkaar voor verbinding kunnen detecteren. Eenmaal \ngekoppeld zullen de Bluetooth-apparaten automatisch worden \nverbonden als ze de volgende keer worden ingeschakeld.\n\u25a0K o p p e l e n\n1. Houd de functietoets minstens vijf seconden ingedrukt \nom de wachtstand koppelen te activeren.\n\u2022 De indicator knippert afwisselend blauw en rood.\n2. Bedien het verbonden Bluetooth-apparaat om het \nscherm Bluetooth-instellingen weer te geven.\n3. Selecteer 'Yamaha EP-E30A' op het weergegeven \nscherm.\n\u2022 Als het koppelen is voltooid, brandt de indicator \ngedurende 20 seconden blauw en gaat daarna \nknipperen in intervallen van 5 seconden.\n\u2022 Als het koppelen niet binnen twee minuten is voltooid, \nwordt dit beschouwd als te zijn mislukt en gaat de \nindicator uit.\n\u2022 Om opnieuw te koppelen schakelt u het product uit en \nweer in.\nKijk voor meer info op de website.\n\u25a0 In-/uitschakelen\nHoud de functietoets op de controller twee seconden \ningedrukt om de oortelefoons in of uit te schakelen.\n\u2022 Nadat de oortelefoons worden ingeschakeld, knippert de \nindicator rood om de resterende accucapaciteit weer te \ngeven.\nDe par sa conception, ce produit n\u2019est pas \u00e9tanche. \nN\u2019exposez pas ce produit \u00e0 des liquides tels que de la sueur, \nde l\u2019eau de pluie ou de l\u2019eau de mer.\nPr\u00e9paratifs\nOp\u00e9rations \u00e9l\u00e9mentaires\nLecture Appliquez une pression br\u00e8ve sur la \ntouche de fonction. \nPause Pendant la lecture, appliquez une \npression br\u00e8ve sur la touche de \nfonction. \nAugmenter le volume\nAppuyez bri\u00e8vement sur la touche +. \nAbaisser le volume Appuyez bri\u00e8vement sur la touche \u2013. \nPasser au morceau \nsuivant\nMaintenez la touche + enfonc\u00e9e \npendant deux secondes. \nRetourner au d\u00e9but \ndu morceau ou passer \nau morceau suivant\nMaintenez la touche \u2013 enfonc\u00e9e \npendant deux secondes. \nR\u00e9pondre \u00e0 un appel Quand vous recevez un appel, \nappliquez une pression br\u00e8ve sur la \ntouche de fonction. \nTerminer un appel Pendant un appel, appliquez une \npression br\u00e8ve sur la touche de \nfonction. \nAugmenter le volume \nde conversation\nAppuyez bri\u00e8vement sur la touche \n+. \nAbaisser le volume de \nconversation\nAppuyez bri\u00e8vement sur la touche \n\u2013. \nIgnorer un appel Quand vous recevez un appel, \nappliquez deux pressions br\u00e8ves \nsur la touche de fonction. \nCaract\u00e9ristiques techniques\nVersion Bluetooth Version 5.0\nProfils pris en charge A2DP, AVRCP, HFP, HSP\nCodecs pris en charge SBC, AAC\nPuissance de sortie RF Classe 2\nPort\u00e9e maximum \nde communication\nEnviron 10 m \n(sans interf\u00e9rences)\nNombres de dispositifs \npouvant \u00eatre enregistr\u00e9s\n8 dispositifs maximum\nBatterie interne rechargeable Batterie au lithium-ion\nDur\u00e9e de charge Environ 2 heures\nAutonomie de lecture \ncontinue\nEnviron 14 heures \nPlage de temp\u00e9rature de \ncharge\n5 \u00e0 40 \u00b0C\nDeutsch\nEinleitung\nDieses Ger\u00e4t ist nicht wasserdicht. Setzen Sie dieses Ger\u00e4t \nkeinen Fl\u00fcssigkeiten wie Schwei\u00df, Regenwasser oder \nMeerwasser aus.\nVorbereitung\nGrundlegende Bedienung\nWiedergabe Funktionstaste einmal kurz \ndr\u00fccken \nPause W\u00e4hrend der Wiedergabe \nFunktionstaste einmal kurz \ndr\u00fccken \nLautst\u00e4rke anheben +-Taste einmal kurz dr\u00fccken \nLautst\u00e4rke senken \u2212-Taste einmal kurz dr\u00fccken \nVorlauf zum n\u00e4chsten \nTitel\n+-Taste zwei Sekunden lang \ngedr\u00fcckt halten \nR\u00fccklauf zum Anfang \ndes Titels oder zum \nvorigen Titel\n\u2212-Taste zwei Sekunden lang \ngedr\u00fcckt halten \nAnruf empfangen Bei eingehendem Anruf \nFunktionstaste einmal dr\u00fccken \nAnruf beenden W\u00e4hrend des Anrufs \nFunktionstaste einmal kurz \ndr\u00fccken \nGespr\u00e4chslautst\u00e4rke \nanheben\n+-Taste einmal kurz dr\u00fccken \nGespr\u00e4chslautst\u00e4rke \nsenken \n\u2212-Taste einmal kurz dr\u00fccken \nAnruf ignorieren Bei eingehendem Anruf \nFunktionstaste zweimal kurz \ndr\u00fccken \nTechnische Daten\nBluetooth-Version 5.0\nUnterst\u00fctzte Profile A 2 DP,  AVR C P, H FP,  HS P\nUnterst\u00fctzte Codecs SBC, AAC\nRF-Ausgangsleistung Klasse 2\nMax. Reichweite ca. 10 m (ohne St\u00f6rungen)\nAnzahl registrierbarer Ger\u00e4te max. 8\nInterner Akku Lithium-Ionen-Batterie\nDE\nLadezeit ca. 2 Stunden\nKontinuierliche Wiedergabezeit ca. 14 Stunden \nLadetemperatur 5\u201340 \u00b0C\nSvenska\nIntroduktion\nDenna produkt \u00e4r inte designad f\u00f6r att vara vattent\u00e4t. Uts\u00e4tt \ninte produkten f\u00f6r v\u00e4tskor som svett, regnvatten eller \nhavsvatten.\nF\u00f6rberedande\nGrundl\u00e4ggande funktioner\nSpela upp Tryck kort en g\u00e5ng p\u00e5 \nfunktionsknappen. \nPausa Under uppspelning, tryck kort en \ng\u00e5ng p\u00e5 funktionsknappen. \nH\u00f6ja volymen Tryck kort p\u00e5 + knappen en g\u00e5ng. \nS\u00e4nka volymen Tryck kort p\u00e5 - knappen en g\u00e5ng. \nHoppa till n\u00e4sta l\u00e5t H\u00e5ll + knappen intryckt i tv\u00e5 \nsekunder. \nG\u00e5 tillbaka till b\u00f6rjan \nav l\u00e5ten eller hoppa till \nf\u00f6reg\u00e5ende l\u00e5t\nH\u00e5ll - knappen intryckt i tv\u00e5 \nsekunder. \nTa ett telefonsamtal N\u00e4r ett telefonsamtal f\u00e5s, tryck kort \np\u00e5 funktionsknappen en g\u00e5ng. \nAvsluta ett \ntelefonsamtal\nUnder ett telefonsamtal, tryck kort \np\u00e5 funktionsknappen en g\u00e5ng. \nH\u00f6j ljudvolymen Tryck kort p\u00e5 + knappen en g\u00e5ng. \nS\u00e4nk ljudvolymen Tryck kort p\u00e5 - knappen en g\u00e5ng. \nAvvisa ett samtal N\u00e4r ett telefonsamtal f\u00e5s, tryck kort \np\u00e5 funktionsknappen tv\u00e5 g\u00e5nger. \nSpecifikationer\nBluetooth-version Version 5.0\nProfiler som st\u00f6ds A2DP, AVRCP, HFP, HSP\nKodek som st\u00f6ds SBC, AAC\nRF-utg\u00e5ngseffekt Klass 2\nMax \nkommunikationsr\u00e4ckvidd\nUngef\u00e4r 10 m \n(utan st\u00f6rningar)\nAntal enheter som kan \nregistreras\nUpp till 8 enheter\nSV\nInternt uppladdningsbart \nbatteri\nLitium jon batteri\nLaddningstid Cirka 2 timmar\nKontinuerlig \nuppspelningstid\nCirka 14 timmar \nLaddningstemperatur 5 \u2013 40\u00b0C\nItaliano\nIntroduzione\nIl prodotto non \u00e8 progettato per essere impermeabile. Non \nesporre il prodotto a liquidi come sudore, acqua piovana o \nacqua di mare.\nPreparazione\nOperazioni di base\nRiproduci Premere brevemente una volta il \ntasto. \nPausa Premere brevemente una volta il \ntasto durante la riproduzione. \nAumentare il volume Premere brevemente una volta il \ntasto +. \nAbbassare il volume Premere brevemente una volta il \ntasto \u2212. \nSaltare al brano \nsuccessivo\nTenere premuto il tasto + per due \nsecondi. \nTornare all'inizio del \nbrano o saltare al \nbrano precedente\nTenere premuto il tasto \u2212 per due \nsecondi. \nRicevere chiamate All'arrivo di una chiamata, \npremere brevemente una volta il \ntasto funzione. \nTerminare chiamate Durante la conversazione, premere \nuna volta il tasto funzione. \nAumentare il volume \ndella conversazione\nPremere brevemente una volta il \ntasto +. \nAbbassare il volume \ndella conversazione\nPremere brevemente una volta il \ntasto \u2212. \nIgnorare chiamate All'arrivo di una chiamata, \npremere brevemente due volte il \ntasto funzione. \nIT\nSpecifiche\nVersione Bluetooth Versione 5.0\nProfili supportati A2DP , AVRCP , HFP , HSP\nCodec supportati SBC, AAC\nPotenza RF in uscita Classe 2\nMassima portata di \ncomunicazione\nCirca 10 m \n(senza interferenze)\nNumero di dispositivi \nregistrabili\nFino a 8 dispositivi\nBatteria interna ricaricabile Batteria agli ioni di litio\nTempo di ricarica Circa 2 ore\nTempo di riproduzione \ncontinuata\nCirca 14 ore \nTemperatura di ricarica 5 \u2013 40 \u00b0C\nEspa\u00f1ol\nIntroducci\u00f3n\nEste producto no est\u00e1 dise\u00f1ado para ser resistente al agua. \nNo exponga este producto a l\u00edquidos como el sudor, la lluvia \no el agua de mar.\nPreparaci\u00f3n\nOperaciones b\u00e1sicas\nReproducir Pulse brevemente la tecla de \nfunci\u00f3n una vez. \nPoner en pausa Durante la reproducci\u00f3n, pulse \nbrevemente la tecla de funci\u00f3n una \nvez. \nSubir el volumen Pulse brevemente la tecla + una vez. \nBajar el volumen Pulse brevemente la tecla \u2013 una vez. \nSaltar a la canci\u00f3n \nsiguiente\nMantenga pulsada la tecla + durante \ndos segundos. \nES\nVolver al principio de \nla canci\u00f3n o saltar a \nla canci\u00f3n anterior\nMantenga pulsada la tecla \u2013 durante \ndos segundos. \nRecibir una llamada \ntelef\u00f3nica\nCuando entre una llamada \ntelef\u00f3nica, pulse brevemente la \ntecla de funci\u00f3n una vez. \nTerminar una llamada \ntelef\u00f3nica\nDurante la llamada telef\u00f3nica, \npulse brevemente la tecla de \nfunci\u00f3n una vez. \nSubir el volumen de la \nconversaci\u00f3n\nPulse brevemente la tecla + una \nvez. \nBajar el volumen de la \nconversaci\u00f3n\nPulse brevemente la tecla \u2013 una \nvez. \nIgnorar una llamada Cuando entre una llamada \ntelef\u00f3nica, pulse brevemente la \ntecla de funci\u00f3n dos veces. \nEspecificaciones\nVersi\u00f3n de Bluetooth Versi\u00f3n 5.0\nPerfiles compatibles A2DP , AVRCP , HFP , HSP\nC\u00f3decs compatibles SBC, AAC\nPotencia de salida de RF Clase 2\nAlcance de comunicaci\u00f3n \nm\u00e1ximo\nAproximadamente 10 m \n(sin interferencias)\nN\u00famero de dispositivos \nque pueden registrarse\nHasta 8 dispositivos\nBater\u00eda interna recargable Bater\u00eda de iones de litio\nTiempo de carga Aproximadamente 2 horas\nTiempo de reproducci\u00f3n \ncontinua\nAproximadamente 14 horas \nTemperatura de carga 5 \u2013 40 \u00b0C\nNederlands\nIntroductie\nDit product is niet ontworpen om waterdicht te zijn. Stel dit \nproduct niet bloot aan vloeistoffen als transpiratievocht, \nregenwater of zeewater.\nVOORBEREIDING\nNLThank you for purchasing this Yamaha product.\n\u2022 For correct and safe use of this product, be sure to first read \nthis manual and the separate \u201cSafety Brochure\u201d .\n\u2022 After reading this manual, keep it together with the \nwarranty card in a safe place for future reference.\n Product, Accessory and Part Names\na Earphones\nb Controller\n* The microphone is built in.\nc Charging connector\nd Indicator\ne Function key\nf Eartips (SS, S, M, L) 1 set each\n* The M-size eartips are attached.\ng USB power cable\nh Startup Guide\ni User\u2019s Guide (Basic) (this manual)\nj Safety Brochure\n Charging\nWith the included USB power cable, connect the charging \nconnector on the earphones to the USB port on a \ncommercially available USB power adaptor (5 V DC/min. 0.5 A \noutput). \n\u2022 The indicator is lit in red during charging and goes off when \ncharging is completed.\n\u2022 Charging takes about two hours. When charged, the \nearphones can play back continuously for approximately \n14 hours.\n Making a Bluetooth\u00ae connection\nWhen using the earphones for the first time, the pairing \nprocess, which allows Bluetooth devices being connected to \ndetect each other, must be performed. Once paired, the \nBluetooth devices will be automatically connected the next \ntime they are turned on.\n\u25a0P a i r i n g\n1. Hold down the function key for at least five seconds to \nenter pairing standby.\n\u2022 The indicator flashes alternately in blue and red.\n2. Operate the connected Bluetooth device to display the \nBluetooth setting screen.\n3. Select \u201cYamaha EP-E30A\u201d from the displayed screen.\n\u2022 When pairing has completed, the indicator lights up in \nblue for 20 seconds, then flashes at 5-second intervals.\n\u2022 If pairing has not completed within two minutes, it will \nbe considered to have failed, and the indicator goes off. \n\u2022 To try pairing again, turn the earphones off, then on \nagain.\nFor details on use, refer to the website.\n\u25a0 Turning on/off\nHold down the function key on the controller for two seconds \nto turn the earphones on/off.\n\u2022 After the earphones turn on, the indicator flashes in red to \nindicate the remaining battery power.\nListening to music\nUsing the phone\nThe Bluetooth\u00ae word mark and logos are registered \ntrademarks owned by Bluetooth SIG, Inc. and any use of such \nmarks by Yamaha Corporation is under license. Other \ntrademarks and trade names are those of their respective \nowners. \n<&VSPQF\u0001NPEFM>\nWireless communication functions\nBluetooth\nRadio Frequency (Operational Frequency): 2402 MHz to 2480 \nMHz\nMaximum Output Power (E.I.R.P): 6.25 dBm\nNous vous remercions d'avoir choisi ce produit Yamaha.\n\u2022 Pour garantir une utilisation correcte et s\u00fbre de ce produit, \nveillez \u00e0 lire ce manuel ainsi que la \u00ab Brochure sur la \ns\u00e9curit\u00e9 \u00bb en annexe.\na\nL\n2\n1 3\nR\nb\nf\ng\nhi\nA B\nC\nT\ne\nc\nd\nj\nEarphones\nCasques Intra-auriculaires\n\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438 \u0432\u0441\u0442\u0430\u0432\u043d\u044b\u0435\n\u2a8c\u7e70\u430b\u7e70\u512c\n\u7e70\u9df9\u430b\u7e70\u5680\nUser Guide (Basic) \nMode d'emploi (\u00e9l\u00e9mentaire)\nBedienungsanleitung (Grundausgabe)\nAnv\u00e4ndarguide (Grundl\u00e4ggande)\nGuida dell'utente (base)\nManual de usuario (b\u00e1sica)\nGebruikershandleiding (basis)\nGuia do Usu\u00e1rio (b\u00e1sico)\nP\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f (\u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0435)\n\u69ab\u4893\u49d7\u2d9f\ue6ae\u34f9\u5117\ue6af\n\u2733\u69ab\u49d8\u2d9f\ue6ae\u34f9\u5117\u4cea\u26fc\ue6af\n\ucea7\ud4a4\ucf1f\uc880\ucf17\t\ubc2b\ucaf3\nAV19-0188\nYamaha Global Site\nhttps://www.yamaha.com/\nYamaha Downloads\nhttps://download.yamaha.com/\nManual Development Group\n\u00a9 2019 Yamaha Corporation\nPublished 10/2020\n2020 \u5e74 10 \u6708 \u53d1\u884c    AMOD-B0\nEnglish\nIntroduction\nThis product is not designed to be waterproof. Do not \nexpose this product to liquids such as sweat, rainwater or \nseawater.\nPreparation\nEN Basic operations\nPlay Briefly press the function key once. \nPause During playback, briefly press the \nfunction key once. \nRaise the volume Briefly press the + key once. \nLower the volume Briefly press the \u2013 key once. \nSkip to the next song Hold down the + key for two \nseconds. \nGo back to the \nbeginning of the song, \nor skip to the previous \nsong\nHold down the \u2013 key for two \nseconds. \nReceive a phone call When a phone call comes in, \nbriefly press the function key once. \nEnd a phone call During the phone call, briefly press \nthe function key once. \nRaise the conversation \nvolume\nBriefly press the + key once. \nLower the \nconversation volume\nBriefly press the \u2013 key once. \nIgnore a call When a phone call comes in, \nbriefly press the function key twice. \nSpecifications\nBluetooth version Version 5.0\nSupported profiles A 2 D P, AV R C P,  H F P, H SP\nSupported codecs SBC, AAC\nRF Output Power Class 2\nMaximum \ncommunication range\nApproximately 10 m \n(without interference)\nNumber of devices \nthat can be registered\nUp to 8 devices\nInternal rechargeable \nbattery\nLithium-ion battery\nCharging time Approximately 2 hours\nContinuous playback time Approximately 14 hours \nCharging temperature 5 \u2013 40 \u00b0C\nFran\u00e7ais\nIntroduction\nFR\n\u2022 Quand vous avez fini de consulter ce manuel, rangez-le en \nlieu s\u00fbr avec la carte de garantie pour toute r\u00e9f\u00e9rence \nult\u00e9rieure. Produit, accessoires et noms des \n\u00e9l\u00e9ments\na Casques intra-auriculaires\nb Commande\n* Comporte un microphone int\u00e9gr\u00e9.\nc Connecteur de charge\nd T\u00e9moin\ne Touche de fonction\nf Embouts (SS, S, M, L) 1 paire de chaque taille\n* Les embouts de taille M sont fix\u00e9s aux casques.\ng C\u00e2ble d'alimentation USB\nh Guide de d\u00e9marrage\ni Guide (\u00e9l\u00e9mentaire) de l'utilisateur (ce document)\nj Brochure sur la s\u00e9curit\u00e9\n Charge\nReliez avec le c\u00e2ble d'alimentation USB fourni le connecteur \nde charge des casques au port USB d'un adaptateur USB \ndisponible dans le commerce (5 V CC/ 0,5 A min.). \n\u2022 Le t\u00e9moin s'allume en rouge pendant la charge et s'\u00e9teint \nquand la charge est termin\u00e9e.\n\u2022 La charge prend environ deux heures. Une fois charg\u00e9s, les \ncasques ont une autonomie continue d'environ 14 heures.\n \u00c9tablir une connexion Bluetooth\u00ae\nLors de l'utilisation initiale des casques, il convient d'ex\u00e9cuter \nl'appairage, qui permet la d\u00e9tection mutuelle des dispositifs \nBluetooth connect\u00e9s. Une fois appair\u00e9s, les dispositifs \nBluetooth s'interconnectent automatiquement \u00e0 chaque mise \nsous tension.\n\u25a0 Appairage\n1. Maintenez la touche de fonction enfonc\u00e9e pendant au \nmoins cinq secondes pour activer l'attente \nd'appairage.\n\u2022 Le t\u00e9moin clignote alternativement en rouge et bleu.\n2. Affichez l'\u00e9cran des r\u00e9glages Bluetooth sur le dispositif \n\u00e0 connecter.\n3. S\u00e9lectionnez \u00ab Yamaha EP-E30A \u00bb sur cet \u00e9cran.\n\u2022 Quand l'appairage est termin\u00e9, le t\u00e9moin reste allum\u00e9 \nen bleu pendant 20 secondes puis clignote \u00e0 intervalles \nde 5 secondes.\n\u2022 Si l\u2019appairage n\u2019est pas termin\u00e9 dans un d\u00e9lai de deux \nminutes, la tentative est consid\u00e9r\u00e9e comme rat\u00e9e et le \nt\u00e9moin s\u2019\u00e9teint.\n\u2022 Mettez les casques hors tension puis \u00e0 nouveau sous \ntension, et essayez \u00e0 nouveau d\u2019appairer les dispositifs.\nPour des d\u00e9tails sur l\u2019utilisation, visitez notre site internet.\n\u25a0 Mise sous tension/hors tension\nMaintenez la touche de fonction de la commande enfonc\u00e9e \npendant plus de deux secondes pour mettre les casques sous \ntension/hors tension.\n\u2022 Apr\u00e8s la mise sous tension des casques, le t\u00e9moin clignote \nen rouge pour indiquer l'autonomie r\u00e9siduelle de la batterie.\n\u00c9coute de musique\nUtilisation du t\u00e9l\u00e9phone\nL'appellation et les logos Bluetooth\u00ae sont des marques \nd\u00e9pos\u00e9es de Bluetooth SIG, Inc. et toute utilisation de ces \nmarques par Yamaha Corporation fait l'objet d'une licence. Les \nautres marques et appellations commerciales sont la \npropri\u00e9t\u00e9 de leurs d\u00e9tenteurs respectifs. \n<.PE\u00c8MF\u0001QPVS\u0001M\b&VSPQF>\nFonctions de communication sans fil\nBluetooth\nFr\u00e9quence radio (fr\u00e9quence op\u00e9rationnelle): 2402 MHz \u00e0 2480 \nMHz\nPuissance de sortie maximum (E.I.R.P): 6,25 dBm\nVielen Dank, dass Sie sich f\u00fcr dieses Produkt von Yamaha \nentschieden haben.\n\u2022 Lesen Sie zuerst diese Anleitung und die gesonderte \n\u201eSicherheitsbrosch\u00fcre\u201c, um eine korrekte und sichere \nVerwendung dieses Produkts zu gew\u00e4hrleisten.\n\u2022 Bewahren Sie diese Anleitung nach dem Lesen zusammen \nmit der Garantiekarte f\u00fcr sp\u00e4tere Nachschlagzwecke \nsorgf\u00e4ltig auf.\n Produkt-, Zubeh\u00f6r- und \nTeilebezeichnungen\na Ohrh\u00f6rer\nb Steuereinheit\n* Das Mikrofon ist eingebaut.\nc Ladeanschluss\nd Anzeige\ne Funktionstaste\nf Ohrst\u00f6psel (SS, S, M, L) je 1 Satz\n* H\u00f6rer werkseitig mit St\u00f6pseln der Gr\u00f6\u00dfe M versehen.\ng USB-Stromversorgungskabel\nh Kurzanleitung\ni Grundlegende Bedienungsanleitung (vorliegend)\nj Sicherheitsbrosch\u00fcre\n Laden\nSchlie\u00dfen Sie das zugeh\u00f6rige USB-Stromversorgungskabel an \nden Ladeanschluss der Ohrh\u00f6rer und an den USB-Anschluss \neines handels\u00fcblichen USB-Netzteils (mit 5 VDC/min. 0,5 A \nAusgang) an. \n\u2022 Die Anzeige leuchtet w\u00e4hrend des Ladens rot und erlischt, \nwenn der Ladevorgang abgeschlossen ist.\n\u2022 Der Ladevorgang dauert ungef\u00e4 hr zwei Stunden. Nach dem \nLaden bieten die Ohrh\u00f6rer eine ununterbrochene \nWiedergabeleistung von ca. 14 Stunden.\n Bluetooth\u00ae-Verbindung herstellen\nWenn Sie die Ohrh\u00f6rer zum ersten Mal verwenden, muss ein \nPairing ausgef\u00fchrt werden, d.h. der Kopplungsvorgang, bei \ndem miteinander zu verbindende Bluetooth-Ger\u00e4te einander \nerkennen. Nach dem Pairing werden die Bluetooth-Ger\u00e4te \nbeim n\u00e4chsten Einschalten automatisch verbunden.\n\u25a0P a i r i n g\n1. Halten Sie die Funktionstaste mindestens f\u00fcnf \nSekunden lang gedr\u00fcckt, um auf Pairing-Standby \numzuschalten.\n\u2022 Die Anzeige blinkt abwechselnd blau und rot.\n2. Bedienen Sie das verbundene Bluetooth-Ger\u00e4t, um den \nBluetooth-Einstellungsbildschirm anzuzeigen.\n3. W\u00e4hlen Sie \u201e Yamaha EP-E30A\u201c im angezeigten \nBildschirm aus.\n\u2022 Nach Abschluss des Pairings leuchtet die Anzeige 20 \nSekunden lang blau und blinkt dann im 5-Sekunden-\nIntervall.\n\u2022 Ist das Pairing nicht innerhalb von zwei Minuten \nabgeschlossen, gilt es als fehlgeschlagen, und die \nAnzeige erlischt.\n\u2022 Um das Pairing erneut auszuf\u00fchren, das Ger\u00e4t aus- und \nwieder einschalten.\nN\u00e4heres zur Benutzung findet sich auf der Website.\n\u25a0E i n - / A u s s c h a l t e n\nHalten Sie die Funktionstaste der Steuereinheit zwei Sekunden \nlang gedr\u00fcckt, um die Ohrh\u00f6rer ein- oder auszuschalten.\n\u2022 Nach dem Einschalten der Ohrh\u00f6rer blinkt die Anzeige rot, \num die verbleibende Akkuleistung anzuzeigen.\nMusik h\u00f6ren\nTelefonieren\nBluetooth\u00ae-Begriff und -Logo sind eingetragene Warenzeichen \nvon Bluetooth SIG, Inc. deren Verwendung durch Yamaha \nCorporation unter Lizenz erfolgt. Alle anderen Warenzeichen \nbzw. eingetragenen Warenzeichen geh\u00f6ren den \nentsprechenden Firmen. \n<&VSPQB\u000e.PEFMM>\nDrahtlos-Kommunikationsfunktionen\nBluetooth\nEmpfangsfrequenz (Arbeitsfrequenz): 2.402\u20132.480 MHz\nMaximale Ausgangsleistung (E.I.R.P): 6,25 dBm\nTack f\u00f6r att du k\u00f6pt denna produkt fr\u00e5n Yamaha.\n\u2022 F\u00f6r korrekt och s\u00e4ker anv\u00e4ndning av denna produkt, ska du \nf\u00f6rst l\u00e4sa denna manual och \u201ds\u00e4kerhetsbroschyren\u201d .\n\u2022 N\u00e4r du har l\u00e4st denna manual, ska du f\u00f6rvara den \ntillsammans med garantikortet p\u00e5 en s\u00e4ker plats f\u00f6r \nframtida referens.\n Namn p\u00e5 produkt, tillbeh\u00f6r och delar\na \u00d6ronsn\u00e4ckor\nb Kontroller\n* Inbyggd mikrofon.\nc Kontaktdon f\u00f6r laddning\nd Indikator\ne Funktionsknapp\nf \u00d6ronproppar (SS, S, M, L) 1 upps\u00e4ttning vardera\n* M-storleken p\u00e5 \u00f6ronpropparna \u00e4r anslutna.\ng USB-str\u00f6mkabel\nh Startguide\ni Anv\u00e4ndarmanual (Grundl\u00e4ggande) ( i denna manual)\nj S\u00e4kerhetsbroschyr\n Ladda\nMed den medf\u00f6ljande USB-str\u00f6mkabeln ansluter du \nladdningskontakten p\u00e5 \u00f6ronsn\u00e4ckorna till USB-porten p\u00e5 en \nkommersiellt tillg\u00e4nglig USB-str\u00f6madapter (5 V DC / min. 0,5 A \nutg\u00e5ng) n\u00e4r den ska laddas. \n\u2022 Indikatorn t\u00e4nds r\u00f6tt under laddning och slocknar n\u00e4r \nladdningen \u00e4r klar.\n\u2022 Laddning tar cirka tv\u00e5 timmar. N\u00e4r de laddas kan \n\u00f6ronsn\u00e4ckorna spelas kontinuerligt i cirka 14 timmar.\n Uppr\u00e4tta en Bluetooth\u00ae-anslutning\nN\u00e4r du anv\u00e4nder \u00f6ronsn\u00e4ckorna f\u00f6r f\u00f6rsta g\u00e5ngen, ska \nprocessen f\u00f6r ihopparning, som g\u00f6r det m\u00f6jligt f\u00f6r Bluetooth-\nenheter som \u00e4r anslutna att uppt\u00e4cka varandra, utf\u00f6ras. N\u00e4r de \nhar parats ihop, ansluts Bluetooth-enheterna automatiskt \nn\u00e4sta g\u00e5ng de sl\u00e5s p\u00e5.\n\u25a0 Ihopparning\n1. H\u00e5ll funktionsknappen intryckt i minst fem sekunder \nf\u00f6r att ange ihopparning i standby.\n\u2022 Indikatorn blinkar v\u00e4xelvis bl\u00e5tt och r\u00f6tt.\n2. Anv\u00e4nd den anslutna Bluetooth-enheten f\u00f6r att visa \ninst\u00e4llningssk\u00e4rmen f\u00f6r Bluetooth.\n3. V\u00e4lj \u201dYamaha EP-E30A\u201d fr\u00e5n den visade sk\u00e4rmen.\n\u2022 N\u00e4r ihopparningen \u00e4r klar, t\u00e4nds indikatorn bl\u00e5 i 20 \nsekunder och blinkar sedan med 5 sekunders intervall.\n\u2022 Om ingen ihopparning har slutf\u00f6rts inom tv\u00e5 minuter, \nkommer det anses ha misslyckats och indikatorn \nslocknar.\n\u2022 F\u00f6r att f\u00f6rs\u00f6ka para ihop igen, ska produkten sl\u00e5s av och \nsedan p\u00e5 igen.\nF\u00f6r mer information, g\u00e5 till webbplatsen.\n\u25a0S l \u00e5  p \u00e5 / a v\nH\u00e5ll funktionsknappen intryckt p\u00e5 kontroller i tv\u00e5 sekunder f\u00f6r \natt sl\u00e5 p\u00e5/st\u00e4nga av \u00f6ronsn\u00e4ckorna.\n\u2022 N\u00e4r \u00f6ronsn\u00e4ckorna \u00e4r p\u00e5slagna, blinkar indikatorn r\u00f6d f\u00f6r \natt ange den \u00e5terst\u00e5ende batterikapaciteten.\nLyssna p\u00e5 musik\nAnv\u00e4nda telefonen\nBluetooth\u00ae-ordm\u00e4rket och logotyper \u00e4r registrerade \nvarum\u00e4rken som \u00e4gs av Bluetooth SIG, Inc., och all \nanv\u00e4ndning av s\u00e5dana varum\u00e4rken som tillh\u00f6r Yamaha \nCorporation \u00e4r under licens. Andra varum\u00e4rken och \nhandelsnamn tillh\u00f6r deras respektive \u00e4gare. \n<&VSPQB\u000eNPEFMM>\nTr\u00e5dl\u00f6sa kommunikationsfunktioner\nBluetooth\nRadiofrekvens (Operativ frekvens): 2402 MHz till 2480 MHz\nMaximal utg\u00e5ngseffekt (E.I.R.P): 6,25 dBm\nGrazie per aver acquistato questo prodotto Yamaha.\n\u2022 Per un utilizzo corretto e sicuro del prodotto, assicurarsi di \nleggere prima questo manuale e la \"Brochure sulla \nsicurezza\" .\n\u2022 Dopo aver letto il manuale, conservarlo insieme alla scheda \ndi garanzia in un luogo sicuro per future consultazioni.Nomi del prodotto, degli accessori e \ndelle parti\na Auricolari\nb Controller\n* Il microfono \u00e8 incorporato.\nc Connettore di ricarica\nd Indicatore\ne Tasto funzione\nf Cuscinetti (SS, S, M, L) 1 set ciascuno\n* I cuscinetti di dimensioni M sono gi\u00e0 fissati agli auricolari\nin vendita.\ng Cavo di alimentazione USB\nh Guida introduttiva\ni Guida dell'utente (base) (questo manuale)\nj Brochure sulla sicurezza\nRicarica\nCollegare il cavo di alimentazione USB incluso al connettore di \nricarica sugli auricolari e alla porta USB di un alimentatore \ndisponibile in commercio (5 V CC/min. 0,5 A in uscita). \n\u2022 L'indicatore \u00e8 acceso durante la ricarica e si spegne una \nvolta che la ricarica \u00e8 completata.\n\u2022 La ricarica dura circa due ore. Una volta ricaricati, gli \nauricolari possono riprodurre audio per circa 14 ore di \nseguito.\nEffettuare una connessione Bluetooth\u00ae\nQuando si utilizzano gli auricolari per la prima volta, \u00e8 \nnecessario eseguire la procedura di accoppiamento, che \nconsente ai dispositivi Bluetooth collegati di individuarsi a \nvicenda. Completato l'accoppiamento, i dispositivi Bluetooth \nsi connetteranno automaticamente alla successiva \naccensione.\n\u25a0 Accoppiamento\n1. Tenere premuto il tasto funzione per circa cinque \nsecondi per entrare in standby accoppiamento.\n\u2022 L'indicatore lampeggia alternandosi tra blu e rosso.\n2. Utilizzare il dispositivo Bluetooth connesso per \nvisualizzare la schermata delle impostazioni Bluetooth.\n3. Selezionare \"Yamaha EP-E30A\" nella schermata.\n\u2022 Completato l'accoppiamento, l'indicatore si illumina in \nblu per 20 secondi, quindi lampeggia ogni 5 secondi.\n\u2022 Se l'accoppiamento non viene completato entro due \nminuti, verr\u00e0 considerato come non riuscito e \nl'indicatore si spegner\u00e0.\n\u2022 Per ritentare l'accoppiamento, spegnere e riaccendere il \nprodotto.\nPer maggiori dettagli di utilizzo, visitare il sito web.\n\u25a0 Accensione/spegnimento\nTenere premuto il tasto funzione sul controller per due \nsecondi per accendere/spegnere gli auricolari.\n\u2022 Accesi gli auricolari, l'indicatore lampeggia in rosso a \nindicare l'alimentazione residua della batteria.\nAscolto di musica\nUtilizzo del telefono\nIl marchio e i loghi Bluetooth\u00ae sono marchi registrati di \nBluetooth SIG, Inc. e sono utilizzati da Yamaha Corporation su \nlicenza. Gli altri marchi e nomi commerciali appartengono ai \nrispettivi proprietari. \n<.PEFMMP\u0001QFS\u0001M\b&VSPQB>\nFunzioni di comunicazione wireless\nBluetooth\nFrequenza radio (frequenza operativa): da 2402 MHz a 2480 \nMHz\nPotenza massima in uscita (E.I.R.P): 6,25 dBm\nGracias por adquirir este producto Yamaha.\n\u2022 Para un uso correcto y seguro de este producto, lea primero \neste manual y el \u201cFolleto de seguridad\u201d (documentos \nseparados).\n\u2022 Despu\u00e9s de leer este manual, gu\u00e1rdelo junto con la tarjeta \nde garant\u00eda en un lugar seguro para futuras consultas. Nombres del producto, los accesorios y \nlas partes\na Auriculares intraurales\nb Controlador\n* El micr\u00f3fono est\u00e1 integrado en \u00e9l.\nc Conector de carga\nd Indicador\ne Tecla de funci\u00f3n\nf Olivas, un juego de cada tama\u00f1o: superpeque\u00f1as (SS), \npeque\u00f1as (S), medianas (M), grandes (L)\n* Las olivas de tama\u00f1o M vienen colocadas.\ng Cable de alimentaci\u00f3n USB\nh Gu\u00eda de inicio\ni Gu\u00eda de instrucciones (b\u00e1sica) (este manual)\nj Folleto de seguridad\n Carga\nCon el cable de alimentaci\u00f3n USB incluido, conecte el \nconector de carga de los auriculares al puerto USB de un \nadaptador de corriente USB de los que se venden en \nestablecimientos comerciales (salida de 5 V CC/m\u00edn. 0,5 A). \n\u2022 El indicador se ilumina en rojo durante la carga y se apaga \ncuando la carga se ha completado.\n\u2022 La carga dura aproximadamente dos horas. Una vez \ncargados, los auriculares pueden utilizarse de manera \ncontinuada durante aproximadamente 14 horas.\n Establecimiento de una conexi\u00f3n \nBluetooth\u00ae\nCuando utilice los auriculares por primera vez, deber\u00e1 realizar \nel proceso de emparejamiento, que permite que los \ndispositivos Bluetooth que se est\u00e1n conectando se detecten el \nuno al otro. Una vez emparejados, los dispositivos Bluetooth \nse conectar\u00e1n autom\u00e1ticamente la pr\u00f3xima vez que se \nenciendan.\n\u25a0 Emparejamiento\n1. Mantenga pulsada la tecla de funci\u00f3n al menos cinco \nsegundos para acceder al modo de espera de \nemparejamiento.\n\u2022 El indicador se iluminar\u00e1 alternativamente en azul y \nrojo.\n2. Realice las operaciones necesarias en el dispositivo \nBluetooth conectado para visualizar la pantalla de \najustes Bluetooth.\n3. Seleccione \u201cYamaha EP-E30A\u201d en dicha pantalla de \najustes.\n\u2022 Una vez completado el emparejamiento, el indicador se \niluminar\u00e1 en azul durante 20 segundos, y despu\u00e9s \nparpadear\u00e1 en intervalos de 5 segundos.\n\u2022 Si el emparejamiento no se ha completado en el \ntranscurso de dos minutos, se considerar\u00e1 que ha \nfallado y el indicador se apagar\u00e1.\n\u2022 Para intentar el emparejamiento otra vez, apague este \nproducto y vuelva a encenderlo de nuevo.\nPara m\u00e1s informaci\u00f3n sobre el uso, consulte el sitio web.\n\u25a0 Encendido/apagado\nMantenga pulsada la tecla de funci\u00f3n del controlador durante \ndos segundos para encender/apagar los auriculares.\n\u2022 Despu\u00e9s del encendido de los auriculares, el indicador \nparpadear\u00e1 en rojo para indicar la carga restante de la \nbater\u00eda.\nPara escuchar m\u00fasica\nPara uso con el tel\u00e9fono\nLa marca denominativa y los logotipos de Bluetooth\u00ae son marcas \ncomerciales registradas propiedad de Bluetooth SIG, Inc. y \ncualquier uso de dichas marcas por parte de Yamaha Corporation \nes bajo licencia. Otras marcas comerciales y nombres comerciales \nson propiedad de sus respectivos propietarios. \n<.PEFMP\u0001QBSB\u0001&VSPQB>\nFunciones de comunicaci\u00f3n inal\u00e1mbrica\nBluetooth\nFrecuencia de radio (frecuencia operativa): de 2402 MHz a \n2480 MHz\nPotencia de salida m\u00e1xima (E.I.R.P): 6,25 dBm\nBedankt voor het aanschaffen van dit Yamaha-product.\n\u2022 Lees voor correct en veilig gebruik van dit product eerst \ndeze handleiding en de afzonderlijke 'Veiligheidsbrochure' .\n\u2022 Bewaar de handleiding na het lezen samen met de \ngarantiekaart op een veilige plaats, om in de toekomst te \nkunnen raadplegen.\n Product, accessoire en namen \nonderdelen\na Oortelefoons\nb Controller\n* De microfoon is ingebouwd.\nc Oplaadaansluiting\nd Indicator\ne Functietoets\nf Oordoppen (SS, S, M, L) elk 1 set\n* De oordoppen in maat M zijn bevestigd.\ng USB-netsnoer\nh Starthandleiding\ni Gebruikershandleiding (basis) (deze handleiding)\nj Veiligheidsbrochure\n Opladen\nVerbind de oplaadaansluiting aan de oortelefoons met het \nmeegeleverde USB-netsnoer met de USB-poort van een in de \nhandel verkrijgbare USB-oplader (5 V DC/min. 0,5 A \nuitgangssignaal). \n\u2022 De indicator brandt rood tijdens het oplaadproces en gaat \nuit als het opladen is voltooid.\n\u2022 Opladen duurt ongeveer twee uur. Indien opgeladen \nkunnen de oortelefoons gedurende ongeveer 14 uur \ncontinu afspelen.\n Een Bluetooth\u00ae-verbinding tot stand \nbrengen\nAls de oortelefoons voor de eerste keer worden gebruikt, moet \nhet koppelingsproces worden uitgevoerd, waarmee Bluetooth-\napparaten elkaar voor verbinding kunnen detecteren. Eenmaal \ngekoppeld zullen de Bluetooth-apparaten automatisch worden \nverbonden als ze de volgende keer worden ingeschakeld.\n\u25a0K o p p e l e n\n1. Houd de functietoets minstens vijf seconden ingedrukt \nom de wachtstand koppelen te activeren.\n\u2022 De indicator knippert afwisselend blauw en rood.\n2. Bedien het verbonden Bluetooth-apparaat om het \nscherm Bluetooth-instellingen weer te geven.\n3. Selecteer 'Yamaha EP-E30A' op het weergegeven \nscherm.\n\u2022 Als het koppelen is voltooid, brandt de indicator \ngedurende 20 seconden blauw en gaat daarna \nknipperen in intervallen van 5 seconden.\n\u2022 Als het koppelen niet binnen twee minuten is voltooid, \nwordt dit beschouwd als te zijn mislukt en gaat de \nindicator uit.\n\u2022 Om opnieuw te koppelen schakelt u het product uit en \nweer in.\nKijk voor meer info op de website.\n\u25a0 In-/uitschakelen\nHoud de functietoets op de controller twee seconden \ningedrukt om de oortelefoons in of uit te schakelen.\n\u2022 Nadat de oortelefoons worden ingeschakeld, knippert de \nindicator rood om de resterende accucapaciteit weer te \ngeven.\nDe par sa conception, ce produit n\u2019est pas \u00e9tanche. \nN\u2019exposez pas ce produit \u00e0 des liquides tels que de la sueur, \nde l\u2019eau de pluie ou de l\u2019eau de mer.\nPr\u00e9paratifs\nOp\u00e9rations \u00e9l\u00e9mentaires\nLecture Appliquez une pression br\u00e8ve sur la \ntouche de fonction. \nPause Pendant la lecture, appliquez une \npression br\u00e8ve sur la touche de \nfonction. \nAugmenter le volume\nAppuyez bri\u00e8vement sur la touche +. \nAbaisser le volume Appuyez bri\u00e8vement sur la touche \u2013. \nPasser au morceau \nsuivant\nMaintenez la touche + enfonc\u00e9e \npendant deux secondes. \nRetourner au d\u00e9but \ndu morceau ou passer \nau morceau suivant\nMaintenez la touche \u2013 enfonc\u00e9e \npendant deux secondes. \nR\u00e9pondre \u00e0 un appel Quand vous recevez un appel, \nappliquez une pression br\u00e8ve sur la \ntouche de fonction. \nTerminer un appel Pendant un appel, appliquez une \npression br\u00e8ve sur la touche de \nfonction. \nAugmenter le volume \nde conversation\nAppuyez bri\u00e8vement sur la touche \n+. \nAbaisser le volume de \nconversation\nAppuyez bri\u00e8vement sur la touche \n\u2013. \nIgnorer un appel Quand vous recevez un appel, \nappliquez deux pressions br\u00e8ves \nsur la touche de fonction. \nCaract\u00e9ristiques techniques\nVersion Bluetooth Version 5.0\nProfils pris en charge A2DP, AVRCP, HFP, HSP\nCodecs pris en charge SBC, AAC\nPuissance de sortie RF Classe 2\nPort\u00e9e maximum \nde communication\nEnviron 10 m \n(sans interf\u00e9rences)\nNombres de dispositifs \npouvant \u00eatre enregistr\u00e9s\n8 dispositifs maximum\nBatterie interne rechargeable Batterie au lithium-ion\nDur\u00e9e de charge Environ 2 heures\nAutonomie de lecture \ncontinue\nEnviron 14 heures \nPlage de temp\u00e9rature de \ncharge\n5 \u00e0 40 \u00b0C\nDeutsch\nEinleitung\nDieses Ger\u00e4t ist nicht wasserdicht. Setzen Sie dieses Ger\u00e4t \nkeinen Fl\u00fcssigkeiten wie Schwei\u00df, Regenwasser oder \nMeerwasser aus.\nVorbereitung\nGrundlegende Bedienung\nWiedergabe Funktionstaste einmal kurz \ndr\u00fccken \nPause W\u00e4hrend der Wiedergabe \nFunktionstaste einmal kurz \ndr\u00fccken \nLautst\u00e4rke anheben +-Taste einmal kurz dr\u00fccken \nLautst\u00e4rke senken \u2212-Taste einmal kurz dr\u00fccken \nVorlauf zum n\u00e4chsten \nTitel\n+-Taste zwei Sekunden lang \ngedr\u00fcckt halten \nR\u00fccklauf zum Anfang \ndes Titels oder zum \nvorigen Titel\n\u2212-Taste zwei Sekunden lang \ngedr\u00fcckt halten \nAnruf empfangen Bei eingehendem Anruf \nFunktionstaste einmal dr\u00fccken \nAnruf beenden W\u00e4hrend des Anrufs \nFunktionstaste einmal kurz \ndr\u00fccken \nGespr\u00e4chslautst\u00e4rke \nanheben\n+-Taste einmal kurz dr\u00fccken \nGespr\u00e4chslautst\u00e4rke \nsenken \n\u2212-Taste einmal kurz dr\u00fccken \nAnruf ignorieren Bei eingehendem Anruf \nFunktionstaste zweimal kurz \ndr\u00fccken \nTechnische Daten\nBluetooth-Version 5.0\nUnterst\u00fctzte Profile A 2 DP,  AVR C P, H FP,  HS P\nUnterst\u00fctzte Codecs SBC, AAC\nRF-Ausgangsleistung Klasse 2\nMax. Reichweite ca. 10 m (ohne St\u00f6rungen)\nAnzahl registrierbarer Ger\u00e4te max. 8\nInterner Akku Lithium-Ionen-Batterie\nDE\nLadezeit ca. 2 Stunden\nKontinuierliche Wiedergabezeit ca. 14 Stunden \nLadetemperatur 5\u201340 \u00b0C\nSvenska\nIntroduktion\nDenna produkt \u00e4r inte designad f\u00f6r att vara vattent\u00e4t. Uts\u00e4tt \ninte produkten f\u00f6r v\u00e4tskor som svett, regnvatten eller \nhavsvatten.\nF\u00f6rberedande\nGrundl\u00e4ggande funktioner\nSpela upp Tryck kort en g\u00e5ng p\u00e5 \nfunktionsknappen. \nPausa Under uppspelning, tryck kort en \ng\u00e5ng p\u00e5 funktionsknappen. \nH\u00f6ja volymen Tryck kort p\u00e5 + knappen en g\u00e5ng. \nS\u00e4nka volymen Tryck kort p\u00e5 - knappen en g\u00e5ng. \nHoppa till n\u00e4sta l\u00e5t H\u00e5ll + knappen intryckt i tv\u00e5 \nsekunder. \nG\u00e5 tillbaka till b\u00f6rjan \nav l\u00e5ten eller hoppa till \nf\u00f6reg\u00e5ende l\u00e5t\nH\u00e5ll - knappen intryckt i tv\u00e5 \nsekunder. \nTa ett telefonsamtal N\u00e4r ett telefonsamtal f\u00e5s, tryck kort \np\u00e5 funktionsknappen en g\u00e5ng. \nAvsluta ett \ntelefonsamtal\nUnder ett telefonsamtal, tryck kort \np\u00e5 funktionsknappen en g\u00e5ng. \nH\u00f6j ljudvolymen Tryck kort p\u00e5 + knappen en g\u00e5ng. \nS\u00e4nk ljudvolymen Tryck kort p\u00e5 - knappen en g\u00e5ng. \nAvvisa ett samtal N\u00e4r ett telefonsamtal f\u00e5s, tryck kort \np\u00e5 funktionsknappen tv\u00e5 g\u00e5nger. \nSpecifikationer\nBluetooth-version Version 5.0\nProfiler som st\u00f6ds A2DP, AVRCP, HFP, HSP\nKodek som st\u00f6ds SBC, AAC\nRF-utg\u00e5ngseffekt Klass 2\nMax \nkommunikationsr\u00e4ckvidd\nUngef\u00e4r 10 m \n(utan st\u00f6rningar)\nAntal enheter som kan \nregistreras\nUpp till 8 enheter\nSV\nInternt uppladdningsbart \nbatteri\nLitium jon batteri\nLaddningstid Cirka 2 timmar\nKontinuerlig \nuppspelningstid\nCirka 14 timmar \nLaddningstemperatur 5 \u2013 40\u00b0C\nItaliano\nIntroduzione\nIl prodotto non \u00e8 progettato per essere impermeabile. Non \nesporre il prodotto a liquidi come sudore, acqua piovana o \nacqua di mare.\nPreparazione\nOperazioni di base\nRiproduci Premere brevemente una volta il \ntasto. \nPausa Premere brevemente una volta il \ntasto durante la riproduzione. \nAumentare il volume Premere brevemente una volta il \ntasto +. \nAbbassare il volume Premere brevemente una volta il \ntasto \u2212. \nSaltare al brano \nsuccessivo\nTenere premuto il tasto + per due \nsecondi. \nTornare all'inizio del \nbrano o saltare al \nbrano precedente\nTenere premuto il tasto \u2212 per due \nsecondi. \nRicevere chiamate All'arrivo di una chiamata, \npremere brevemente una volta il \ntasto funzione. \nTerminare chiamate Durante la conversazione, premere \nuna volta il tasto funzione. \nAumentare il volume \ndella conversazione\nPremere brevemente una volta il \ntasto +. \nAbbassare il volume \ndella conversazione\nPremere brevemente una volta il \ntasto \u2212. \nIgnorare chiamate All'arrivo di una chiamata, \npremere brevemente due volte il \ntasto funzione. \nIT\nSpecifiche\nVersione Bluetooth Versione 5.0\nProfili supportati A2DP , AVRCP , HFP , HSP\nCodec supportati SBC, AAC\nPotenza RF in uscita Classe 2\nMassima portata di \ncomunicazione\nCirca 10 m \n(senza interferenze)\nNumero di dispositivi \nregistrabili\nFino a 8 dispositivi\nBatteria interna ricaricabile Batteria agli ioni di litio\nTempo di ricarica Circa 2 ore\nTempo di riproduzione \ncontinuata\nCirca 14 ore \nTemperatura di ricarica 5 \u2013 40 \u00b0C\nEspa\u00f1ol\nIntroducci\u00f3n\nEste producto no est\u00e1 dise\u00f1ado para ser resistente al agua. \nNo exponga este producto a l\u00edquidos como el sudor, la lluvia \no el agua de mar.\nPreparaci\u00f3n\nOperaciones b\u00e1sicas\nReproducir Pulse brevemente la tecla de \nfunci\u00f3n una vez. \nPoner en pausa Durante la reproducci\u00f3n, pulse \nbrevemente la tecla de funci\u00f3n una \nvez. \nSubir el volumen Pulse brevemente la tecla + una vez. \nBajar el volumen Pulse brevemente la tecla \u2013 una vez. \nSaltar a la canci\u00f3n \nsiguiente\nMantenga pulsada la tecla + durante \ndos segundos. \nES\nVolver al principio de \nla canci\u00f3n o saltar a \nla canci\u00f3n anterior\nMantenga pulsada la tecla \u2013 durante \ndos segundos. \nRecibir una llamada \ntelef\u00f3nica\nCuando entre una llamada \ntelef\u00f3nica, pulse brevemente la \ntecla de funci\u00f3n una vez. \nTerminar una llamada \ntelef\u00f3nica\nDurante la llamada telef\u00f3nica, \npulse brevemente la tecla de \nfunci\u00f3n una vez. \nSubir el volumen de la \nconversaci\u00f3n\nPulse brevemente la tecla + una \nvez. \nBajar el volumen de la \nconversaci\u00f3n\nPulse brevemente la tecla \u2013 una \nvez. \nIgnorar una llamada Cuando entre una llamada \ntelef\u00f3nica, pulse brevemente la \ntecla de funci\u00f3n dos veces. \nEspecificaciones\nVersi\u00f3n de Bluetooth Versi\u00f3n 5.0\nPerfiles compatibles A2DP , AVRCP , HFP , HSP\nC\u00f3decs compatibles SBC, AAC\nPotencia de salida de RF Clase 2\nAlcance de comunicaci\u00f3n \nm\u00e1ximo\nAproximadamente 10 m \n(sin interferencias)\nN\u00famero de dispositivos \nque pueden registrarse\nHasta 8 dispositivos\nBater\u00eda interna recargable Bater\u00eda de iones de litio\nTiempo de carga Aproximadamente 2 horas\nTiempo de reproducci\u00f3n \ncontinua\nAproximadamente 14 horas \nTemperatura de carga 5 \u2013 40 \u00b0C\nNederlands\nIntroductie\nDit product is niet ontworpen om waterdicht te zijn. Stel dit \nproduct niet bloot aan vloeistoffen als transpiratievocht, \nregenwater of zeewater.\nVOORBEREIDING\nNL",
      "ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.1  Unpacking 3  Adjusting the computer stand\n2  Installing the stand base\nComputer Keyboard* Mouse* Power cord Publications\n* Selected models only.\n1. Some models may contain items that are not listed.\n2. The descriptions in this guide might be different from what you see on your \ncomputer, depending on the computer model and configuration.\n3. Product design and specifications may be changed without notice.\n4  Connecting the keyboard and mouse\n5  Connecting the power cable\n6  Turning on the computer\nNOTE: If your computer is equipped with a wireless keyboard or mouse, \nfollow the installation instructions for those devices.\nFront view of the computer\nATTENTION: Be sure not to block any air vents on the computer.  \nBlocked air vents may cause thermal problems.\nATTENTION: If your computer is equipped with an infrared (IR) remote \ncontrol, do not put the USB IR receiver on the same plane as the screen.\nATTENTION: Do not touch the vents unless they have completely cooled \ndown.\n 3D camera\n Camera LED indicator\n 2D camera\n Built-in microphone\nLeft and right view of the computer\n Headset hanger\n HDMI-in switch button\n Power button\n USB 3.0 connector\n USB 3.0 connector\n Memory card reader\n Combo audio jack \n1\n Mic connector\n USB 3.0 connector\n USB 3.0 connector/USB power \noff charging connector\n Optical drive eject button\nRear view of the computer\n HDMI-in connector\n USB 2.0 connector\n USB 2.0 connector\n HDMI-out connector\n Ethernet connector\n Power connector\n Security cable slot\n Cable clip\n Vents\nNOTES:\n\u2022\t The\tcombo\taudio\tjack\tdoes\tnot\tsupport\tconventional\tmicrophones.\n\u2022\t Different\tindustry\tstandards\tmay\tlead\tto\tthe\trecording\tfunction\tnot\tbeing\t\nsupported if third-party headphones or headsets are connected.\n\u2022\t The\theadset\thanger\tcan\tonly\tsupport\tloads\tup\tto\t1kg\tin\tweight.\t\nConnecting to the Internet\nYou need a contract with an Internet Service Provider (ISP) and some hardware \nin order to connect to the Internet. ISPs and the services they offer vary by \ncountry. Contact your ISP for services available in your country. Your computer \nsupports wireless (selected models only) or wired networks that connect your \ncomputer to other devices.\nWired network connection\nFor wired networks, connect one end of an Ethernet cable (purchased \nseparately) to the Ethernet connector on your computer, and then connect the \nother end to the network router or broadband modem. Consult your ISP for \ndetailed setup instructions.\nNOTE: Broadband modem and router installation procedures vary \ndepending on the manufacturer. Follow the manufacturer\u2019s instructions.\nWireless network connection\nFor wireless networks, you can use the built-in Wi-Fi antenna to access your \nwireless home network.\nTo connect your computer to the wireless network:\nClick or tap the network icon on the taskbar, then select the name of your \nrouter. Click or tap Connect, then follow the steps on the screen.\nTest the wireless network by opening your Web browser and accessing any \nWeb site.\nor\n1\n Combo audio jack: Used to connect headsets.\nService and Support information\nWindows help system\nThe following information describes the technical support that is available for \nyour product, during the warranty period or throughout the life of your product. \nRefer to the Lenovo Limited Warranty for a full explanation of Lenovo warranty \nterms. See \u201cLenovo Limited Warranty notice\u201d later in this document for details \non accessing the full warranty. If you purchased Lenovo services, refer to the \nfollowing terms and conditions for detailed information:\n\u2013 For Lenovo Warranty Service Upgrades or Extensions,  \ngo to: http://support.lenovo.com/lwsu\n\u2013 For Lenovo Accidental Damage Protection Services,  \ngo to: http://support.lenovo.com/ladps\n\u2013 Online technical support is available during the lifetime of a product at: \nhttp://www.lenovo.com/support\nThe Windows help system provides you with detailed information about using \nthe Windows operating system.\nTo access the Windows help system, do the following:\nOpen the Start menu and then click or tap Get Started to get more details.\nYou can also enter Get Started or what you\u2019re looking for in the search box \non the taskbar. You\u2019ll get suggestions and answers to your questions about \nWindows and the best search results available from your PC and the Internet.\nTo perform system recovery after a serious error in Windows 10, click or tap \nthe Start button and select Settings \u2192 Update & security \u2192 Recovery. \nThen, follow the instructions on the screen for system recovery.\nATTENTION: The Windows operating system is provided by Microsoft \nCorporation. Please use it in accordance with the END USER LICENSE \nAGREEMENT (EULA) between you and Microsoft. For any question related \nto the operating system, please contact Microsoft directly.ideacentre \nAIO Y910\nQuick Start Guide\nAccessing other manuals\nCarrying the computer\nThe Important Product Information Guide has been preloaded onto your \ncomputer. To access this guide, do the following:\nFor models come with Lenovo Companion:\n1. Open the Start menu and then click or tap Lenovo Companion.\n2. Click or tap Support \u2192 User Guide. \nFor models without Lenovo Companion:\nOpen the Start menu and then click or tap Manual to access this guide.\nFor models without preinstalled operating systems:\nGo to the Lenovo Support Web site to access your manuals. See \u201cDownloading \nmanuals\u201d for additional information.\nDownloading manuals\nThe latest electronic publications for your computer are available from the \nLenovo Support Web site. To download the publications,  \ngo to: www.lenovo.com/UserManuals.\nReplacing or upgrading hardware\nSafety information for replacing CRUs\nDo not open your computer or attempt any repairs before reading the \u201cImportant \nsafety information\u201d in the Safety, Warranty, Environment, Recycling Information \nGuide and Important Product Information Guide that was included with your \ncomputer.\nIf you no longer have this copy of the Safety, Warranty, Environment, Recycling \nInformation Guide and Important Product Information Guide, you can obtain \none online from the website at http://www.lenovo.com/UserManuals.\nPre-disassembly instructions\nBefore proceeding with the disassembly procedure, make sure that you do the \nfollowing:\n1. Turn off the power to the system and all peripherals.\n2. Unplug all power and signal cables from the computer.\n3. Place the system on a flat, stable surface.\nCRUs for your computer include: \n\u2022 keyboard \u2022 adapter\n\u2022 mouse \u2022 hard disk drive\n\u2022 optical drive \u2022 memory module\n\u2022 solid state drive*\nThis part contains instructions for replacing the following parts:\n\u2022 Hard disk drive (Follow steps: \n1  3 )\n\u2022 Solid state drive (Follow steps: 1  4 )\n\u2022 Optical drive (Follow steps: 1  3  5 )\n\u2022 Memory module (Follow steps: 2  6 )\nATTENTION: Turn off the computer and wait 5 minutes to let it cool down \nbefore removing any part of the computer.\nATTENTION: The parts marked with  can get hot during operation.  \nDo not touch!\nReplacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware Replacing or upgrading hardware \n1  Removing the left rear cover 3  Removing the hard disk drive 5  Removing the optical drive 7  Removing the graphic card\n2  Removing the right rear cover 4  Removing the solid state drive 6  Removing the memory moduleThe following information refers to ideacentre AIO Y910 machine type:\nF0CJ [AIO Y910-27ISH/Energy Star]\nLenovo, the Lenovo logo and ideacentre are trademarks of Lenovo in the United \nStates, other countries, or both. \nWindows is a trademark of the Microsoft group of companies. \nOther company, product, or service names may be trademarks or service marks of \nothers.\n\u00a9 Copyright Lenovo 2016. LIMITED AND RESTRICTED RIGHTS NOTICE: If data or \nsoftware is delivered pursuant to a General Services Administration \u201cGSA\u201d contract, \nuse, reproduction, or disclosure is subject to restrictions set forth in Contract \nNo. GS-35F-05925. \nVersion 1.0 2016.07\nSP40L06497\nThe graphic card is optional CRU for experienced users. Instructions for \nreplacing this part can be found in the Hardware Maintenance Manual from \nthe Lenovo Support Web Site. To obtain a copy of the Hardware Maintenance \nManual, go to: www.lenovo.com/UserManuals.\nTo remove the graphic card: follow steps \u201c\n2 , 7 \u201d.",
      "1 \nThank you for choosing the Huawei E3276 4G LTE \nMobile Internet Key. With your Mobile Internet Key, \nyou can enjoy a full high speed Internet experience on \nthe go. \n \nThis guide shows you how to set-up and connect to the internet with your Huawei \nMobile Internet Key.  For detailed information visit www.telusmobility.com/support \nor see the help menu of the Connection Manager on your computer. \n 2 \nGetting to Know Your Mobile Internet Key  \n \n1\n3\n2\n6\n45\n \n \n \n \n 3 \n1  USB Connector  \nIt connects the Internet Key to a PC, \nfolds for safe transport, and rotates \nto help you get the best signal.  \n \n \n2  USB Connector Release Button \n \n \n3 The LED light indicates if the internet \nkey has found a network and if you have \nsuccessfully connected to a mobile high \nspeed internet network.  \n \n\uf06c Blue, blinking: 4G network available. \n\uf06c Cyan, blinking: 4G LTE available. \n\uf06c Blue, solid: Connected to a 4G network. \n\uf06c Cyan, solid: Connected to a 4G LTE network. \n \n4 SIM card slot  \n \n5 microSD Card Slot  \n \n6  External Antenna Ports \nThe Internet Key can support 2 external \nantennas for better signal reception. \n \n 4 \nGetting Your Internet Key Ready \n \nInstall your TELUS SIM Card  \n \n1. Slide the cover off by sliding it towards the usb connector.  \n2. Insert the TELUS SIM card into the SIM card slot.  \n \nNote:  \nEnsure the beveled edge of the SIM card is aligned with the SIM card slot as seen \nin the diagram.  \n3. Slide the front cover back onto the Internet Key until it clicks into place.  \n 5 \nInserting the microSD Card  \n \nInsert the microSD card into the Internet Key, as shown in the following figure.  \n \n \n \nNote:  \n\uf06c The microSD card is an optional accessory that can be purchased from a \nTELUS store. \n\uf06c Do not remove the microSD or SIM card while the Internet Key is connected to \nyour computer. Otherwise, the cards or Internet Key may be damaged.  \n 6 \nGetting your Computer Ready \n \nThe steps to install the Connection Manager depend on your computer\u2019s operating \nsystem (OS). Follow the instructions below for your computer type. It is \nrecommended you uninstall any existing programs used with a mobile internet key \nprior to completing the next steps. \n \nStep 1: Connect the Mobile Internet Key to your computer.  \n \nPlug the connector of the Internet Key directly into a USB port of your computer.  \nDo not use a USB hub.  \n \n 7 \nStep 2: Installing the Mobile Internet Key Connection Manager  \n \nWindows PC (Windows XP&7) \n1. The computer will automatically detect the new hardware and open the \ninstallation wizard.  \n2. Follow the on-screen instructions of the installation wizard. \n3. Installation may take several minutes.  A progress bar will be displayed.  \n4. After the program is installed, a shortcut icon for Connection Manager  appears \non the desktop. \n5. Remove the Internet Key and reboot your computer. \n \nNote:  \nIf the installation wizard does not automatically open, it can be launched manually.  \nNavigate to HUAWEI > Connection Manager under \"My Computer\" and look for \nthe Setup file. Double-click Setup.exe to open the setup wizard.  \n \nWindows PC (Windows 8) \n1. Charm bar detects the new hardware. Choose Run AutoRun.exe. 8 \n2. Follow the on-screen instructions of the installation wizard. \n3. Installation may take several minutes.  A progress bar will be displayed.  \n4. After the program is installed, a shortcut icon for Connection Manager appears \non the desktop. \n5. Remove the Internet Key and reboot your computer. \n \nMAC \n1. The OS will automatically detect the new hardware and place the HUAWEI \nicon on the screen. Double click Connection Manager icon to launch the \ninstallation. \n2. Follow the on-screen instructions of the installation wizard. If presented with \nAuthenticate dialog box, enter the Name/Password used to log your MAC then \nclick OK. \n3. Installation may take several minutes.  A progress bar will be displayed.  \n4. After the program is installed, a shortcut icon for Connection Mana ger \nappears on the desktop. \n5. Remove the Internet Key and reboot your computer. \n \n 9 \nStep 3: Connecting to the Internet \n \nCongratulations, your Mobile Internet Key and computer are now ready for high \nspeed internet access! Follow the steps below to connect to the internet . \n1. Double-click the \n  on the desktop to launch the Connection Manager. \n \nThe first time you connect you will be prompted to setup data volume limits.  If you \ndo not know what your data plan allows, please contact your service provider.  \n \n2. Wait until you see TELUS beside the signal strength bars. \n3. Click Connect. \n \nNote:  \nIt is recommended you disconnect when you are not using the computer.  \n \nDisconnecting the Internet Key from your computer (Windows XP) \n1. Click Disconnect in the Connection Manager. \n2. Close the Connection Manager. 10 \n3. Double-click \n in the notification area. Then the Unplug or Eject \nHardware dialog box appears. \n4. Select the corresponding hardware and then click Stop.  \n5. When the message \"It is safe to remove the device\" appears, remove the \nInternet Key.  \n \nDisconnecting the Internet Key from your computer (Windows 7) \n1. Click Disconnect in the Connection Manager. \n2. Close the Connection Manager. \n3. Click \n  in the notification area.  \n4. Select the corresponding hardware and then click it.  \n5. Remove the Internet Key. \n \nDisconnecting the Internet Key from your computer (Windows 8) \n1. Click Disconnect in the Connection Manager. \n2. Close the Connection Manager. 11 \n3. Click \n  in the Safely Remove Hardware and Eject Media in System tray.  \n4. Choose Eject the CD Drive to remove Internet Key. \n \nDisconnecting the Internet Key from your computer (MAC) \n1. Click Disconnect in the Connection Manager. \n2. Close the Connection Manager. \n3. Choose the Connection Manager icon in the desktop.  \n4.  Press control and click the left mouse simultaneously. \n5. Select eject and then remove the Internet Key. \n \nUninstalling the Connection Manager (Windows XP) \n1. Choose Start > Control Panel > Add or Remove Programs.  \n2. Find the Connection Manager and click Remove to uninstall the \nConnection Manager. \n \nUninstalling the Connection Manager (Windows 7) \n1. Choose Start > Control Panel > Uninstall a program.  12 \n2. Find the Connection Manager and double-click it to uninstall the \nConnection Manager. \n \nUninstalling the Connection Manager (Windows 8) \n1. Go to the location of the software and choose the uninst.exe. \n2. Follow the on-screen instructions of the uninstallation wizard. \n3. Uninstallation may take several minutes.  A progress bar will be displayed.  \n4. After the program is uninstalled, a shortcut icon for Connection Manager \ndisappears on the desktop. \n \nUninstalling the Connection Manager (MAC) \n1. Choose Finder > Go > Application.  \n2. Choose the Connection Manager. \n3. Press control and click the left mouse simultaneously. \n4. Select Move to Trash to uninstall the Connection Manager. \nNote:  \nExit the Connection Manager Application before uninstalling it. \nVersion: V100R001_01 Part Number: 31010NWB ",
      "Caution \n1. Safety Instructions .........................................................................................1\n2. Packing List ....................................................................................................3\n3. Appearance ...................................................................................................4\n4. Installing ........................................................................................................5\n5. LED Status Light Indicator ..............................................................................9\n6. Loop Recording ..............................................................................................9\n7. Setting Up the N3 Pro on DDPAI App ............................................................10\n8. 24H Parking Monitoring Mode .....................................................................12\n9. GPS Function ...............................................................................................14\n10. DDPAI App and Dash Cam Firmware Upgrading ..........................................15\n11. File Browsing on PC ....................................................................................15\n12.FAQ ............................................................................................................16\n13. Product Specifications................................................................................18\nAppendix: Installing the Hardwire Kit ..............................................................19\nFCC Statement .................................................................................................21Thank you for purchasing the DDPAI N3 Pro ultra-high-definition digital Dash Cam, which offers high-quality \ncaptured images and a high-definition video experience.\nCaution\nBefore installing the N3 Pro Dash Cam, perform power-on a check to confirm the N3 Pro Dash Cam can be \nconnected to the DDPAI app normally.\nIf there is anything wrong with the connection, or any questions or suggestions, please contact us without \nhesitation.feedback@DDPAI.com\nScan to Watch the Unboxing Video Installation Video\nScan to Download DDPAI App Contact Us\n1\n1. Safety Instructions\nPlease read all of the the instructions thoroughly before attempting the installation or operation of this Dash Cam. \nIf any damage is caused by failure to follow the manual instructions, the warranty shall be void. This manual should \nbe kept properly for future reference.\nThank you for purchasing the DDPAI N3 Pro ultra-high-definition digital Dash Cam (N3 Pro Dash Cam), which offers \nhigh-quality captured images and a high-definition video experience.\nThe N3 Pro Dash Cam is equipped with a brand new 1/2.7-inch CMOS image sensor, 5-megapixel Ultra HD image, \nD\u00b2 save emergency dual storage, 24H parking monitoring, GPS, and other functions. With the built-in Wi-Fi, it \nconnects to the DDPAI App, which enables users to watch and download the captured videos and images in real-\ntime.\n1.1 Safety Cautions\n\u25cf Only suitable for motor vehicles.\n\u25cf Only suitable for DC12V ~ 24V vehicles.\n\u25cf Do not place the the N3 Pro Dash Cam in a humid environment, otherwise possible external or internal damage \nwill be caused.\n\u25cf Do not expose the N3 Pro Dash Cam to rain, moisture or water, otherwise possible external or internal damage \nwill be caused.\n\u25cf Do not attempt to disassemble or repair without permission. If there are any problems with the N3 Pro Dash \nCam or its accessories, please contact the seller DDPAI Customer Service for help.\n\u25cf Do not expose the N3 Pro Dash Cam to overheating, dust environment, or use any chemicals or cleaning solvents \nto clean the device.\n\u25cf Clean the lens with a soft, damp cloth, and clean carefully to avoid scratches that could affect the quality of the \nvideo.\n\u25cf Avoid dropping, puncturing or disassembling.\n\u25cf Do not use the N3 Pro Dash Cam in the environment below -4\u00b0F(-20\u00b0) or above 158\u00b0F (70\u00b0).\n\u25cf Better to take off the camera from the vehicle when not in use for a long period of time to prevent from damages \ncaused by exposure to overheated environment. \n\u25cf TF Memory Card: check whether there is any damages or cracks on the surface before inserting an external \nmemory card. Do not use memory cards that are obviously physically distorted or covered cracks, for there is a risk \nof short circuits.\n\u25cf Use the specified USB car charger included in the package, otherwise the Dash Cam may repeatedly restart due \nto the mismatch of power supply.\n\u25cf Check the DDPAI App runs normally before installation and confirm that the App can be connected to the Dash \nCam, please contact us if there is anything wrong: feedback@DDPAI.com.\n\u25cf If your App fails to connect to the Dash Cam, check whether the relevant permission of running this App is given. \nIn order to function normally, all the permissions required by the App should be granted. For specific purposes, \nplease see the App User Agreement. If the failure still exists, please contact us: feedback@DDPAI.com.\n1.2 Warnings \nFailure to comply with safety warnings and regulations may result in serious injury or death.\n\u25cf Keep gadgets out of reach of children.\n\u25cf Be sure to use Specified accessories and designated parts. The use of other non-designated parts may damage \nthe equipment and lead to fire, electric shock or equipment failure.2\n\u25cf If the N3 Pro Dash Cam is damaged or malfunctioning, please stop using immediately, then consult the seller . \nContinued use may cause fires, electric shocks, or vehicle malfunctions. Do not install if it obstructs driving, to \navoid accidents.\n\u25cf Do not install in areas that may block drivers\u2019 sight, including but not limited to rearview mirrors or airbag \noutlets:\n1. Do not place the N3 Pro Dash Cam on the dashboard or in front of airbags when it is not secured.\n2. Do not install the N3 Pro Dash Cam on the windshield when it is not secured, as it may fall and distract the \ndriver, causing a safety hazard.\n\u25cf Do not install or perform any operations while driving. If necessary, pull over properly to ensure the safety before \nusing.\n\u25cf Please note that the N3 Pro Dash Cam may infringe on privacy and other rights, depending on how the user uses \nit. We do not assume any legal responsibility for the above scenarios.\n\u25cf Due to the use of a wide-angle lens, some images may be distorted and differ from the actual image. This \nphenomenon is not a malfunction but a characteristic of a wide-angle lens.\n\u25cf some countries or states in the United States, drivers are prohibited from installing anything on the windshield or \nin specific areas of the windshield. Owners/operators should install or use this equipment in compliance with local \nlaws.\n1.3 User Agreement\nBy purchasing the N3 Pro, you agree to all terms and conditions of the preventive measures, precautions, warnings, \nand warranty mentioned in this manual.\nIf you do not agree to any of the terms mentioned in this manual for any reason, please do not hesitate to contact \nus: feedback@DDPAI.com.3\n2. Packing List\n2.1 Kit for the N3 Pro\n2.2 Kit for the Rear Camera \nIf the components are found to be damaged, missing, or waterlogged after unpacking, please do not hesitate to \ncontact us: feedback@DDPAI.com.\nN3 Pro Dash Cam Car chargerPower cable Mount\nVehicle Installation Tool User manualAdhesive sticker Electrostatic sticker\nAdhesive stickerRear camera Rear camera power cable4\n3. Appearance\n1. Parking monitoring indicator 2. lens\n3. USB Type-C port 4. Indicator light\n5. MIC 6. Reset button\n7. Mounting bracket 8. Speaker\n9. Lock button 10. Micro SD card slot\n11. Rear camera 12. Adhesive sticker\n13. Power cable 14. Interface to rear camera\n1\n5 6\n9\n2\n3\n4\n7\n8\n11\n12\n13\n10\n145\n4. Installing\nThe N3 Pro Dash Cam is Only suitable for motor vehicles.\nThe N3 Pro Dash Cam is factory configured and ready for use after installation.\n4.1 Installing Procedure\n4.1.1 Installing the TF Memory Card\n\u25cf The N3 Pro supports a maximum of 512GB TF memory card and requires a Class 10 high-speed card. Different \nbrands of TF memory cards may affect the storage of performance, some TF memory cards have the risk of missing \nrecords. It is recommended to use the following brands of TF memory cards: DDPAI, Kingston, Samsung, Toshiba, \nand Lexar . \n\u25cf Insert or remove the TF card Only when the Dash Cam is not connected to the power supply. Inserting or \nremoving the TF card when the Dash Cam is running may damage the files within.\n\u25cf Please confirm the orientation of the TF card before installation, if the memory card is forcibly inserted in the \nwrong direction, the memory card, card slot or the card data may be damaged.\n1. Open the TF card slot cover on the left side of N3 Pro.\n2. Insert the TF card into the N3 Pro slot (with the text side facing upwards and the connecting finger side \nfacing downwards), and then push the card into the slot.\n3. Close the card slot cover .\n4.1.2 Power-On Checking\n\u25cf Do not perform any operation to set up this product while driving. \n\u25cf Downloading the app will incur your mobile data usage, if you don\u2019t wish so, connect your smartphone to the \nInternet. \nBefore installing the N3 Pro Dash Cam, perform power-on check to confirm the N3 Pro Dash Cam is connected to \nthe DDPAI App normally.\nPlease note that the Dash Cam will automatically format the \nTF card at the first time using the TF memory card.6\n1. Scan to install the DDPAI App on smartphone.\nNote: Please grant the DDPAI App relative application permissions to install, otherwise it may lead the \nconnection failure. \nFor more details, please see Download and Install the DDPAI App.\n2. Connect the N3 Pro power cable, start the vehicle and turn on the N3 Pro.\na. Take out the N3 Pro host, holder, vehicle charger, Type-C power cable.\nb. Connect the USB connector of the power cable to the N3 Pro and insert the other end into the power port \nof the N3 Pro.\nc. Insert the vehicle charger into the cigarette lighter socket and start the vehicle. \nThen the N3 Pro will automatically power on with the welcome greeting \"Hello, DDPAI\". When the indicator \nlight turns blue from purple, it indicates that the Dash Cam power on normally, otherwise, please check the \nfault according to the status of the indicator. For a detailed description of the status of the indicator light, \nplease see Read the Indicator Light.\ncar cigarette lighter7\n4.1.3 Installing the N3 Pro onto Your Windshield\n\u25cf After performing the power-on checking, please confirm the N3 Pro is connected to the DDPAI app normally.\n\u25cf Do not install this product or perform any other operation to set up this product while driving.\n1. Select an appropriate place to locate the N3 Pro and clean this area:\nYou are recommended in most cases to install the N3 Pro on your front windshield (Next to the rearview mirror on \nthe driver\u2019s side), as shown in the figure below.\nClean the location area on the windshield with a clean and soft cloth from dust, plastic, and other stuff, to ensure \nthat the N3 Pro can be firmly attached to the front windshield.\nUse the vehicle wire trim tool to run the power cable along the front windshield and the roof, and hide the cable on \nthe A-pillar, so as to not block the driver\u2019s sight and cause danger .\n2. Locate the installation position based on the guidelines:\nAfter connecting the N3 Pro to your phone, you will see the horizontal and vertical installation guidelines on the \nscreen to assist with installation, according which the installation position of the N3 Pro is located:\n\u25cf Vertically: Moving the N3 Pro side to side with the help of the vertical guideline on the middle of the \nscreen, keep adjusting until to the middle of the vehicle\u2019s front end align to this vertical guideline.\n\u25cf Horizontally: Rotating the camera up and down with the help of the horizontal guideline on the middle of \nthe screen, keep adjusting until the ground-level horizon outside vanishes from the scope of this horizontal \nguideline.\n3. Fix the N3 Pro in place.\nAfter locating the installation position, attach the electrostatic sticker and remove the red backing film from the 3M \nadhesive sticker on the back of the N3 Pro. Then attach the N3 Pro to the predetermined location, ensuring that \nthe 3M adhesive sticker on the back is firmly attached to the windshield. The camera is facing towards the front of \nthe vehicle.\nThe optimal installation position8\n4.2 (Optional) Installing the Rear Camera \nNote: This section is Only applicable to users who have purchased the rear camera. If you have not purchased one, \nplease skip this section.\n1. Clean the rear windshield (the approximate installation position of the rear camera Is shown in the figure below).\n2. Take out the rear camera and the power cable and connect them.\n3. Insert the rear power cable fully into the N3 Pro headphone jack.\nCaution: Please make sure that the power cable is fully plugged into the jack, if not, the rear camera indicator will \nnot on, and the rear camera will not work.\n4. Route the rear power cable from the N3 Pro to the rear camera, cable routing is shown in the figure above.\n5. Remove the red backing film of the adhesive, and adjust the field of view of the rear camera, then fix the rear \ncamera to the inner wall of the rear windshield.\n9\n5. LED Status Light Indicator\nIndicator Light Status of the Dash Cam\nOff Powered off or Power Cable is Not Connected\nSteady Purple Powering on or Upgrading\nFlashing Purple Rapidly Tf Storage Card Inserted Failure or Storage Abnormality\nBlinking Blue Normal Operating\nBlinking Red Parking Monitoring\nOff (The Rear Camera) Powered off or Power Cable is Not Connected\nSteady Green (The Rear Camera) Normal Operating\n6. Loop Recording\nThe Device automatically starts recording after powering on without any additional settings required.\nPrevious recordings will be overwritten in a loop when storage is full. Emergency events will be locked and not be \noverwritten.Previous recordings will be overwritten in a loop when storage is full. Emergency events will be locked \nand not be overwritten.\nNote: Please download and save emergency events in time. If there are more than 10 emergency events, they will \nbe overwritten in a loop.\nRecording10\n7. Setting Up the N3 Pro on DDPAI App\nThe Dash Cam can be connected to the DDPAI App on smartphone via its built-in Wi-Fi without incurring user's \ndata usage, forming a small Dash Cam-smartphone WLAN. Through this Wi-Fi network, you can preview, playback, \ndownload and share driving recordings and photos in real time on the DDPAI App on smartphone.\n7.1 DDPAI App Downloading and installing the DDPAI App\n7.1.1 Download methods:\n1. Scan the following QR code on the smartphone.\n2. Download the DDPAI App from Google Play or App Store. \n7.1.2 When the download is complete, click on the App to install.\nNote: If the authorizations prompt during installation, please grant full permissions, otherwise it may affect the \nconnection between DDPAI App with the N3 Pro.\n7.2 Connecting N3 Pro to the DDPAI App\n7.2.1 Guide to adding connecting N3 Pro:\na. On the Device main page of the DDPAI App, + Connect new devices and enter the selection page. Then click on \n\"Screen-less Dash Cam\".\n11\nb. On the wizard page, click \"Next\" and \"Connect Wi-Fi\" , and the detected Dash Cam Wi-Fi will be displayed (e.g., \nDDPAI_N3 Pro_XXXX). Click \"Connect\".\nc. Connect your smartphone to the Dash Cam's built-in Wi-Fi \"DDPAI_N3 Pro_XXXX\", with the default password: \n1234567890.\n\u2022For iPhone: find settings, select \"WLAN\", and select the \"DDPAI_N3 Pro_XXXX\".\n\u2022 Android: find Wi-Fi list, connect to the \"DDPAI_N3 Pro_XXXX\".\niPhone\nAndroid\n12\n7.2.2 Once connected successfully, you will be taken directly to the real-time preview \ninterface(Real-time/Playback) of the DDPAI App for the Dash Cam.\nWhen connecting the Dash Cam for the first time, the following prompts will appear:\n\u25cf Prompt to modify Wi-Fi password: Please modify the password according to the actual situation, and keep the \npassword safe after modification.\nNote: If you forget the password, press and hold the rightmost button on the Dash Cam for 6-8 seconds to restore \nthe default password and reset the Dash Cam to factory settings.\n\u25cf Prompt to switch to other networks: Choose \"Keep connection\".\nNote: Choosing \"Switch WLAN\" will disconnect the wireless connection with the Dash Cam.\n\u25cf Prompt to enable high compression video encoding: It is recommended to choose \"Enable\". After enabling it, the \nvideo quality of the recorded footage will be improved and clearer than the default configuration.\n7.3 How to Use the DDPAI App\nAfter installing the DDPAI App, please scan the QR code below to obtain instructions on how to use the App.\nNote: As the App version will be regularly optimized and updated, the operation methods shown in the example \nmay be slightly different from the actual ones. If you have any problems, please send us feedback anytime, and \nwe will update the online documentation promptly.\n8. 24H Parking Monitoring Mode\nThis function is Only offered to users who have purchased the hardwire kit. If you have not purchased it, this \nfunction is not supported.\nAfter installing the DDPAI intelligent hardwire kit to the fuse box, the Parking Mode setting will appear in the DDPAI \nApp's camera settings as shown in the figure below:\n13\n8.1 After vehicle is turned off enters\nOptions Description\nTime-Lapse Recording It will save a considerable amount of device storage space, but the viewing \nspeed will be faster . Time-Lapse Recording is selected by default.\nNormal Recording The device will record continuously and overwrite the oldest files when the \nstorage is full.\nSleep Mode The device will go into sleep mode and will be awakened when an \nemergency event is detected.\n8.2 Setting Parking Monitoring Duration\nAfter the vehicle engine is turned off and reaches this duration, the device will go into sleep mode to protect \nthe vehicle\u2019s battery. The options include 12H, 24H, and 48H, with 24H selected by default.\nNote: When setting this timer, please consider the monitoring duration in advance to avoid the risk of battery \ndepletion and to prevent the device from affecting the vehicle\u2019s startup.\n8.3 Vehicle Battery Protection\nSet the minimum safe voltage: when the battery voltage is detected to be lower than the set safe voltage, the \nrecorder will automatically shut down to avoid continuous battery depletion and protect the vehicle\u2019s battery. \nThe default setting is Middle (12V).\n14\nYou can also view the real-time battery voltage detection on this page.\n9. GPS Function\nThe N3 Pro Dash Cam supports GPS function, and it is turned on by default. \nWhen the N3 Pro Dash Cam is in an open outdoor area, it can receive GPS signals. After the GPS signal is \nsuccessfully located, the GPS information will be recorded in your recorded video. You can view various information \nsuch as time, speed, altitude, direction, and driving track in the DDPAI App.\nNote: GPS signal reception is affected in densely populated residential areas, underground parking lots, tunnels, \nand cloudy weather and etc.\nThe Dash Cam's GPS function is turned on by default, and you can also manually turn it on:\n1. On the main page, click on the SR Switch to turn on the GPS function. Once turned on, various driving \ninformation will be displayed on the monitoring screen in the App, including date, driving track, speed, altitude, \nlane information, direction, front vehicle speed, and distance to the vehicle ahead.\n15\n10. DDPAI App and Dash Cam Firmware Upgrading\n1. Open the DDPAI App and connect to the Dash Cam's Wi-Fi. Click on \"Me\" at the bottom right corner of the app \nto enter the user center .\n2. Click on \"About DDPAI > Check for New Version\" in sequence. The app will automatically check whether the app \nversion and Dash Cam firmware version are up-to-date:\n\u2022 If there is a new version available for either the app or the Dash Cam firmware, please follow the app's \nprompts to upgrade to the latest version.\nDuring the Dash Cam firmware upgrade process, do not turn off the power, otherwise, it may result in an \nunsuccessful upgrade or damage to the Dash Cam. If there is a power outage during the upgrade process and the \nDash Cam fails to upgrade successfully, please contact customer service, download the latest upgrade package, \ncopy the upgrade package to the Dash Cam via a TF card, and then upgrade.\n\u2022 If the current version is already up-to-date, then there is no need to upgrade. \n11. File Browsing on PC\n1. Disconnect the power cable from the N3 Pro Dash Cam and turn off the Dash Cam.\n2. Open the card slot cover and gently push the TF memory card with your fingernail or another tool. The card will \npop out automatically.\n3. Remove gently the TF memory card from the Dash Cam's card slot and insert it into the accompanying card \nreader, then plug the card reader into the PC.\n4. Once the PC recognizes the TF memory card, open the DCIM folder in the storage card. From there, you can \nbrowse or edit the photo or video files saved on the TF memory card, which are organized into the following three \nfolders:\n\u2022 200video: includes normal recording videos and time-lapse videos recorded in parking monitoring mode. \nNormal recording videos start with a number, while time-lapse videos start with \"S\" and those ending with \n\"H\" are high-compression encoded recording videos.\n\u2022 201photo: includes voice-activated photos and photos generated by collision detection. Normal photos \nstart with \"N\", while collision detection photos start with \"G\".\n\u2022 202emergency: includes emergency video files saved when a collision occurs.\nDownload\nSnapshot\nTrack view\nExit16\n12. FAQ\nQ1: TF Card error?\n1. The TF card has not been formatted before use. Please format it referring to Formating TF Card.\n2. The Dash Cam supports up to 512GB, and it is recommended to use a high-speed Class 10 card from brands such \nas DDPAI, Kingston, Samsung, Toshiba, or Lexar .\nQ2: Rear camera abnormal, unable to switch?\nCheck if the rear camera indicator light is on (green light on constantly). If not, please check the following:\n1. Ensure there is no gap between the bracket and the Dash Cam.\n2. Check if the rear camera connection cable is fully inserted into the headphone jack on the bracket.\n3. If it still cannot be connected, please contact us at feedback@DDPAI.com.\nQ3: Forgot the Dash Cam's Wi-Fi password?\nPress and hold the reset key with a pin for 6s to 8s to restore the factory settings. The initial default Wi-Fi password \nis \"1234567890\".\nQ4: Unable to connect to Wi-Fi?\n1. Turn off VPN and other security software when connecting.\n2. When installing the App, grant all the requested application permissions.\n3. Reset the device and reinstall the DDPAI App.\n4. If still unable to connect, please contact us at feedback@DDPAI.com\nQ5: Real-time preview on iPhone is black screen?\nTurn on Local Network? \nFind DDPAI App in the settings on iPhone, and make the following settings:Turn on Local Network. \n17\nQ6: After starting the vehicle, is it necessary to unplug and then plug \nback in the USB adapter for the Dash Cam to turn on?\nFor Dash Cams powered by the car's cigarette lighter, some car models continue to supply power to the lighter \neven after the engine has been turned off. As a result, the Dash Cam may not detect the power on/off signal, and \nthe power cable needs to be manually unplugged and re-plugged for the Dash Cam to start.\nQ7: The GPS location is not successful?\n1. Ensure there is no gap between the bracket and the Dash Cam.\n2. Check if there is a GPS logo on the bracket.\n3. Try again in an area with a good GPS signal (outside the car).\nRestart or reset the device.\nQ8: How to turn on SR and view location and trajectory data?\nPlease see the App operation instructions (SR section).\nQ9: The date/time of the Dash Cam is not accurate?\nConnect the Dash Cam to the DDPAI App to synchronize the time automatically. If the time needs to be \nsynchronized every time, there may be a fault with the internal RTC of the Dash Cam. Please do not hesitate to \ncontact us.\nQ10: Video playback is stuttering?\n1. It is recommended to download the video for playback.\nIf the storage hasn't been formatted for a long time, it is recommended to format the storage or try using a \ndifferent TF card.\nQ11: The Dash Cam automatically shuts down after being idle on \nconstant power for 15 minutes?\nThe Dash Cam will enter sleep mode after a long time of inactivity. Restart the Dash Cam by unplugging and \nplugging the power cable again.18\n13. Product Specifications\nProduct Dash Cam\nProduct Model Mola N3 Pro\nOperating Voltage and Current 5V/2A\nRAM 2Gb\nStorage Capacity TF card storage; supports up to 512GB, loop recording\nProcessor  Dual-core flagship processing chip\nImage Sensor 5 million pixel image sensor\nGlobal Positioning System Optional GPS\nLens  5 high-definition optical lenses, F1.8 aperture\nG-Sensor Built-in 3-axis gyroscope, collision lock, sensitivity adjustable \nthrough the app\nPhoto Pixel 2560*1600\nRear-Facing Camera 1080P/25FPS19\nAppendix: Installing the Hardwire Kit\nTo avoid damage to the vehicle due to incorrect operation, please have the hardwire kit installed at a professional \nauto service center .\nIf you have purchased a hardwire kit, you can directly connect it to the N3 Pro through the fuse box for power \nsupply.\nInstall the voltage reduction cable with a hidden deployment method, ensuring a tidy space. The method of power \nsupply installation through the hardwire kit can achieve parking monitoring function.\nThe detailed steps for installing the hardwire kit (connecting to the \nfuse box for power supply) are as follows:\nThe installation of the hardwire kit requires connection to the vehicle's battery fuse box. Please ensure personal \nsafety and turn off the vehicle engine before proceeding with the installation.\nPrerequisites: The mobile App has been verified to be able to connect to the N3 Pro after startup. \nFor details, please see Startup Check.\nN3 Pro has been adjusted for shooting view and has been attached to the front windshield. For details, please see \nInstalling the N3 Pro onto Your Windshield.\n1. Take out the three types of wire terminals and the power connection wire as shown in the figure below.\nThe power connector Includes the following terminals:\n(1) Red ACC wire terminal\n(2) Yellow VCC wire terminal\n(3) Black GND wire terminal\n(4) The end of the power wire is a Type-C connector\n2. Install the Dash Cam and power it on\n\u2460 Insert the end of the power wire Type-C interface into the Dash Cam power interface, and route the power wire \nalong the front windshield, A-pillar, and driver's seat storage compartment to the fuse box. The power wire routing \nmethod is shown in the figure below.\n20\n\u2461 Connect the terminal cables:\na. Connect the black wire (GND) to the ground wire inside the vehicle fuse box,\nb. Connect the red wire (ACC) to the +12VDC power supply control of the fuse box,\nc. Connect the yellow wire (VCC) to the 12VDC power supply of the fuse box (this power supply is always on and \nnot controlled by the ignition).\n1. Open the fuse box (the location of it is varied from vehicles), test the continuous power, which means there will \nbe electricity even if the vehicle is not activated.\n2. Connect the red wire to the ACC (ignition wire)\nConnect the yellow wire to the VCC (constant electric current wire)\n3. Connect the black wire to the metal (negative electrode)\n3. Start the vehicle. After the Dash Cam is powered on, it will automatically start up and emit a startup prompt \ntone.\n4. Check if the Dash Cam is working properly.\nThe installation of the hardwire kit requires professional personnel from an auto service center . For detailed steps, \nplease see >Appendix: Installation of Hardwire Kit. Alternatively, scan the QR code below to watch the hardwire kit \ninstallation video.\nThe optimal installation \nposition\nAutomotive fuse box:It is usually located on the left side of the driver's seat or near the steering wheel. For \ndetails, please see the instruction manual\n21\nScan the QR code to watch the hardwire kit installation video.\nFCC Statement\nThis equipment has been tested and found to comply with the limits for a Class B digital device, pursuant to Part \n15 of the FCC Rules. These limits are designed to provide reasonable protection against harmful interference in a \nresidential installation. This equipment generates uses and can radiate radio frequency energy and, if not installed \nand used in accordance with the instructions, may cause harmful interference to radio communications. However, \nthere is no guarantee that interference will not occur in a particular installation. If this equipment does cause \nharmful interference to radio or television reception, which can be determined by turning the equipment off and \non, the user is encouraged to try to correct the interference by one or more of the following measures:\n-- Reorient or relocate the receiving antenna. \n-- Increase the separation between the equipment and receiver .  \n-- Connect the equipment into an outlet on a circuit different from that to which the receiver is connected. \n-- Consult the dealer or an experienced radio/TV technician for help.\nThis device complies with part 15 of the FCC Rules. Operation is subject to the following two conditions:\n1. This device may not cause harmful interference.\n2. This device must accept any interference received, including interference that may cause undesired operation.\nChanges or modifications not expressly Approved by the party responsible for compliance could void the user's \nauthority to operate the equipment.\n",
      "Quick start guide / Gu\u00eda de inicio r\u00e1pido / Guia de in\u00edcio r\u00e1pido /  \nGuide de d\u00e9marrage rapide / Guida rapida / Schnellstartanleitung / \nSnelstartgids\nS1201, S1202, S1203 AND S1204\n3\n1. Welcome\nThis booklet provides you the quick start information of your S12 digital \ncordless telephone. For a full explanation of all features and instructions, \nplease refer to the User Guide which can be available as described at the \nbottom page of this booklet.\nContents inside the box \nImportant\nOnly use the telephone line cord supplied.\nIf you have purchased a S12 multiple pack you will also have the following \nadditional items:\n\u2022 Cordless handset and charger\n\u2022 2 x rechargeable Ni-MH battery cells\n\u2022 Power adaptor for the charger\nEN \nNi-MH\nNi-MH\nQuick Start Guide\nCordless handset and base Power adaptor Telephone line cord\nCordless handset and charger \n(available only for multiple \npack)\nRechargeable Ni-MH \nbattery cells x 2\nQuick Start Guide\nWelcome4\n2. Getting started\nConnecting the base and handset\n1. Plug the telephone line cord into the socket marked  on the rear of the \nbase.\n2. Plug the power adaptor into the socket marked  on the rear of the base \nand plug the other end into the wall socket. Switch on the power supply.\nConnecting the charger and charging the handset\nTo close: To open:\n1\n2\nUse your \ufb01nger \nto unclip the \ncover and lift up. \nPosition the \nbattery cover \nas shown.\nLower the cover \nand clip into place.\nInsert your \n\ufb01nger here.\nGetting started5\n1. Plug the power adaptor into the socket marked  on the rear of the \ncharger and plug the other end into the wall socket.\n2. Remove the battery cover from the back of the handset and insert the \n2 x AAA NiMH rechargeable batteries supplied. Please note the \u2018+\u2019 and \n\u2018-\u2019 markings inside the battery compartment and insert in the correct \ndirection.\n3. Clip the battery cover back into place.\n4. If you are charging the batteries for the first time, place the handset on the \ncharger to charge for at least 24 hours continuously.\n5. When the handset is fully charged, the  icon will appear steadily on the \ndisplay. The display will show the handset name and handset number to \nindicate that it is registered to the base. \nImportant\nWarning! Use only the approved rechargeable Ni-MH battery cells (2 x AAA \nNi-MH 450mAh rechargeable batteries) supplied with your S12.\nGetting started6\n3. Getting to know your phone\nOverview of your handset\nA Earpiece\nB Display \nSee page 8 for an overview of display icons.\nC Up \nDuring a call, press to increase the earpiece volume. \nWhen the phone is ringing, press to increase the handset ringer volume. \nPress to scroll through the menu options, redial list, calls list and \nphonebook entries.\nD Menu / OK \nPress to open the menu. \nPress to select menu options and to confirm a displayed option.\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nGetting to know your phone7\nE Talk / Hands-free \nMake or answer a call. \nPress to switch hands-free on or off during a call.\nF * / Keypad lock  \nPress and hold to lock / unlock the keypad. \nTo unlock, press any key and follow the on-screen instructions.\nG Recall \nUse with switchboard / PABX services and some network services.\nH Redial \nIn standby mode, press to open the redial list.\nI Phonebook / Exit \nIn standby mode, press to open the phonebook menu. \nWhen viewing the redial/calls list or when editing, press to exit and return \nto standby.\nJ Calls list / Down \nIn standby mode, press to open the calls list. \nDuring a call, press to decrease the earpiece volume. \nWhen the phone is ringing, press to decrease the handset ringer volume. \nPress to scroll through the menu options, redial list, calls list and phonebook \nentries.\nK End Call / Exit / Switch Handset on/ off \nPress and hold for 3 seconds to switch off the handset.\nL Intercom \nUsed to make internal calls and transfer calls to other S12 handsets \nregistered to the base.\nM Microphone\nGetting to know your phone8\nHandset LCD display\nFlashes to indicate an incoming external call.\nExternal call in progress.\nFlashes to indicate an incoming internal call.\nInternal call in progress.\n3-way call in progress or transfer an external call to another S12 handset \nregistered to the base.\nEco mode is on.\nFlashes to indicate new voicemail messages received. Off when you have \nno voicemail messages.\nIndicates the signal strength. Flashes when handset is not registered to \nthe base or if you go out of range.\nHands-free mode is on.\nMenu is opened.\nPhonebook is opened.\nFlashes to indicate new calls list entries received.1\nAnswered call (when viewing the Calls list).1\nUnanswered call (when viewing the Calls list).1\nKeypad is locked.\nDisplayed number is longer than 12 digits.\nBattery is fully charged.\nBattery is partially charged.\nBattery is running low. Flashes when the battery is almost fully \ndischarged.\nBattery  \nstatus\nScrolls \nwhen \ncharging\nTime or Handset name\nIn standby mode, press and hold     to switch handset\nidle screen between handset name or time.\n0\nEco mode\nCircle around\nappears when\nEco mode is ON \n \nGetting to know your phone9\n1For this feature to work, you must subscribe to Caller ID Service from your \nnetwork operator. A subscription fee may be payable.\nOverview of the base\nA Page\n In standby mode, press to ring the handset(s).\n Press and hold to enter registration mode when registering handsets.\nA\nGetting to know your phone",
      "SuperAD: A Training-free Anomaly Classification and Segmentation Method for\nCVPR 2025 V AND 3.0 Workshop Challenge Track 1: Adapt & Detect\nHuaiyuan Zhang1 Hang Chen1 Yu Cheng1 Shunyi Wu1 Linghao Sun1\nLinao Han1 Zeyu Shi2 Lei Qi1* *\n1School of Computer Science and Engineering, Southeast University, China\n2School of Computer Science and Engineering, Nanjing University of Science and Technology, China\n{zhang hy, hangchen, chengyu, shunyiwu, linghaosun, linaohan, qilei}@seu.edu.cn\nshizeyu@njust.edu.cn\nAbstract\nIn this technical report, we present our solution to the CVPR\n2025 Visual Anomaly and Novelty Detection (VAND) 3.0\nWorkshop Challenge Track 1: Adapt & Detect: Robust\nAnomaly Detection in Real-World Applications. In real-\nworld industrial anomaly detection, it is crucial to accu-\nrately identify anomalies with physical complexity, such\nas transparent or reflective surfaces, occlusions, and low-\ncontrast contaminations. The recently proposed MVTec AD\n2 dataset significantly narrows the gap between publicly\navailable benchmarks and anomalies found in real-world\nindustrial environments. To address the challenges posed\nby this dataset\u2014such as complex and varying lighting con-\nditions and real anomalies with large scale differences\u2014we\npropose a fully training-free anomaly detection and seg-\nmentation method based on feature extraction using the DI-\nNOv2 model namedSuperAD. Our method carefully selects\na small number of normal reference images and constructs\na memory bank by leveraging the strong representational\npower of DINOv2. Anomalies are then segmented by per-\nforming nearest neighbor matching between test image fea-\ntures and the memory bank. Our method achieves compet-\nitive results on both test sets of the MVTec AD 2 dataset.\n1. Introduction\n1.1. Background\nUnsupervised anomaly detection and localization has\nemerged as a key technology in computer vision, with wide-\nranging applications in real-world scenarios such as indus-\ntrial quality inspection and autonomous driving. The cen-\ntral challenge of this task lies in training models solely\n*Corresponding Author\non normal samples, while enabling accurate identification\nand precise localization of previously unseen defects dur-\ning testing. In recent years, fueled by the advancement of\ndeep learning, numerous methods have achieved remark-\nable progress on mainstream benchmark datasets such as\nMVTec AD and VisA. However, as model performance\non these datasets approaches saturation\u2014for instance, seg-\nmentation AU-PRO scores of some algorithms on MVTec\nAD have surpassed 97%\u2014their limitations have become in-\ncreasingly apparent:\n1. Limited Scene Diversity : Existing datasets primarily\nfocus on objects with clear textures and simple struc-\ntures, lacking coverage of complex industrial scenarios\ninvolving transparent or reflective surfaces (e.g., glass-\nware, metal products), as well as bulk, overlapping items\n(e.g., granular materials).\n2. Idealized Defect Types: Most defects in current datasets\nare large, centrally located anomalies, overlooking\nreal-world industrial defects such as edge anomalies,\nsubpixel-level scratches (e.g., hairline cracks), and low-\ncontrast contaminations (e.g., transparent foreign ob-\njects).\n3. Insufficient Environmental Robustness : These\ndatasets often ignore variations in lighting conditions\n(e.g., dark-field, backlighting, overexposure), resulting\nin models with limited generalization ability when\ndeployed across different devices or under varying\nenvironments.\n1.2. Challenge Description\nAs a next-generation benchmark for industrial anomaly de-\ntection, the MVTec AD 2 dataset [6] systematically ad-\ndresses the aforementioned limitations by introducing eight\ncomplex and diverse scenarios. Its core features include:\n\u2022 Simulation of Physical Complexity:\n\u2013 Transparent and reflective surfaces : Categories such\narXiv:2505.19750v2  [cs.CV]  27 May 2025as Vial feature liquid refraction artifacts, and Sheet\nMetal includes mirror-like reflections, both of which\nchallenge the model\u2019s ability to reason about light\npropagation.\n\u2013 Bulk and overlapping objects : Examples like\nWallplugs and Walnuts involve random occlu-\nsions and truncated boundaries between objects, re-\nquiring semantic-level understanding.\n\u2013 High intra-class variability in normal samples : Cat-\negories such as Fabric (with diverse textures) and\nCan (with geometric pattern deformations) demand\nmodels to learn tight boundaries of the normal data\nmanifold.\n\u2022 Verification of Detection Limits:\n\u2013 Tiny objects and boundary anomalies: Plastic contam-\ninants occupying less than 0.1% of the image area in\nthe Rice category, and missing regions at the image\nboundaries in Wallplugs pose significant challenges\nto the model\u2019s perceptual capability at high resolution.\n\u2013 Implicit structural consistency : Fruit Jelly re-\nquire the model to assess the plausibility of ingredi-\nent distributions, despite the absence of explicit logical\nconstraints.\n\u2022 Cross-Domain Generalization Evaluation: Each scene\nincludes at least four lighting conditions (regular, under-\nexposed, overexposed, and additional light sources), sim-\nulating distribution shifts caused by device variations in\nreal-world production environments. This enables sys-\ntematic evaluation of model robustness.\nOn the MVTec AD 2 dataset, mainstream methods ex-\nhibit clear performance bottlenecks:\nLimited localization capability : EfficientAD [1] and\nPatchCore [10] achieve average AU-PRO 0.05 scores [6] of\nonly 58.7% and 53.8%, respectively. In highly complex\nscenarios such as Can and Rice, their performance drops\nbelow 30%.\nPoor robustness : MSFlow [14] shows a significant\nperformance degradation of 51.1% in AU-PRO 0.05 on the\nmixed lighting test setTESTpriv,mix compared to the standard\ntest set TESTpriv, highlighting its sensitivity to environmen-\ntal variations.\nIn summary, this paper targets the key challenges high-\nlighted in the MVTec AD 2 dataset and aims to:\n\u2022 Develop a novel and highly robust model capable of accu-\nrately detecting subtle defects within highly variant nor-\nmal samples;\n\u2022 Enhance localization performance on small targets and\nstructurally complex objects, such as transparent or over-\nlapping instances;\n\u2022 Strengthen the model\u2019s generalization ability under vary-\ning illumination conditions, thereby improving its practi-\ncal applicability in industrial deployment.\n2. Methodology\n2.1. Model Design\n2.1.1. Approach\nIn recent years, memory bank-based methods have demon-\nstrated remarkable performance on various anomaly detec-\ntion and segmentation benchmarks. PatchCore [10] intro-\nduces a greedy coreset selection mechanism to construct a\ncompact memory bank, significantly reducing the number\nof features while preserving the overall distribution of the\noriginal feature space. This design effectively balances de-\ntection accuracy and retrieval efficiency. DMAD [7] uni-\nfies two commonly encountered scenarios in industrial set-\ntings: one with only normal samples available and the other\nwith a limited number of labeled anomalies. It achieves this\nby constructing a normal memory bank and an expandable\nanomaly memory bank, which store features of normal and\nobserved anomalous patterns, respectively, thereby improv-\ning adaptability to real-world complexities.\nIt is worth noting that the success of these memory-based\napproaches heavily depends on the quality of feature extrac-\ntion. As a result, most methods rely on powerful pre-trained\nvisual backbones such as WideResNet [12] or Vision Trans-\nformer (ViT) [5]. Prior studies reveal that different lay-\ners in deep models capture distinct types of information:\nshallow layers tend to focus on local high-frequency details\n(e.g., textures, edges), whereas deeper layers encode more\nabstract semantic information. Therefore, combining both\nshallow and deep features enables the model to capture both\nglobal structure and local details, which is crucial for accu-\nrate anomaly detection.\nFor instance, PaDiM [3] employs a pre-trained ResNet\nto extract patch-wise features from four different layers and\nmodels the distribution of normal features at each spatial\nlocation using a multivariate Gaussian distribution, param-\neterized by the mean and covariance. APRIL-GAN [2],\non the other hand, leverages CLIP\u2019s powerful multimodal\nalignment capabilities. It extracts four-layer features from\nboth the test image and its corresponding normal reference\nimage, performs layer-wise matching, and averages the re-\nsults to localize anomalous regions.\nFortunately, recent advances in self-supervised learning,\nsuch as DINOv2 [9], have shown strong capability in cap-\nturing rich semantic information for visual tasks. Build-\ning upon this, our method constructs a class-specific normal\nfeature memory bank for each category in the MVTec AD 2\ndataset. During inference, the features from various regions\nof an input image are matched against those stored in the\nmemory bank to detect anomalies. We adopt the powerful\nDINOv2 backbone to extract multi-level features, aiming to\nachieve accurate and fine-grained anomaly segmentation.DINOv2-large\nDINOv2-large\nNN Search NN Search NN Search NN Search\nReference Images\nTest Image\nAnomaly Map\nFrozen Parameters\nElement-wise Addition\nNearest Neighbor Search\nFigure 1. The overall architecture of our proposed method.\n2.1.2. Architecture\nThe overall architecture of our proposed method is illus-\ntrated in Figure 1.\nFor each category in the dataset, we construct a memory\nbank consisting of 16 normal reference images. The selec-\ntion of these reference images follows a two-step procedure.\nFirst, we extract CLS token representations from all training\nimages using the DINOv2 model. Then, the same greedy\ncoreset selection method as used in Patchcore is applied to\ngroup these feature vectors into 16 clusters. This strategy\nmaximizes the coverage of diverse normal patterns within\na category without changing the distribution of all features,\nthereby enhancing the representativeness of the reference\nset and reducing false positive rates.\nFor each test image, we first extract multi-level features\nusing the DINOv2 model. For each level of features, we\ncompute the similarity between the test image features and\nthose stored in the memory bank to retrieve the nearest\nneighbor information. Our method is based on the assump-\ntion that normal regions in the test image tend to find sim-\nilar regions among the reference images, while anomalous\nregions lack such matches. By evaluating the similarity at\neach spatial location, we obtain an anomaly map for each\nlevel. Finally, these anomaly maps are averaged and upsam-\npled to the original resolution to generate the final anomaly\nsegmentation map.\n2.1.3. Training\nOur proposed method requires no training. For each cat-\negory in the MVTec AD 2 dataset, we construct a few-\nshot feature memory bank of normal reference images us-\ning samples from the training set. The number of reference\nsamples is fixed at 16. We adopt the pretrained DINOv2-\nViT-L-14 model as the feature extractor, which consists of\n24 transformer layers and approximately 300 million pa-\nrameters. Features are extracted from four specific layers\n(i.e., layer 6, 12, 18, and 24) to generate the final anomaly\nsegmentation map.\nTo ensure memory efficiency, input image resolutions\nare adjusted: for all categories except Sheet Metal, the\nshorter side is resized to 672 pixels while preserving the\noriginal aspect ratio. For Sheet Metal, due to the elon-\ngated image size, the shorter side is resized to 448 pixels,\nalso preserving the original aspect ratio. This configuration\nallows our method to run entirely on a single 24GB GPU\n(e.g., NVIDIA GeForce RTX 3090).\nFor the Vial and Wallplugs categories, we fur-\nther enhance segmentation performance by extracting fore-\nground features from the input images, as detailed in Sec-\ntion 2.2.1.\nFor the Fabric and Walnuts categories, there are\ncases where the central pattern is entirely normal while\nanomalies exist only at the edges (e.g., in the Fabric cat-\negory, a piece of fabric may be placed over the background\nfabric). After the initial detection, we apply post-processing\nto these two categories by filling the interiors of closed re-\ngions, further improving prediction accuracy.\n2.2. Dataset & Evaluation\n2.2.1. Dataset Utilization\nSince our proposed method is training-free, for each cate-\ngory, we only utilize images from its training set to con-\nstruct a few-shot memory bank. In this method, we inno-\nvatively propose an adaptive background mask generation\ntechnique based on Principal Component Analysis (PCA)\nand morphological optimization for the image preprocess-\ning stage. This enhances visual feature representation while\nsuppressing redundant background information.\nFirst, we apply PCA to reduce the dimensionality of fea-\ntures extracted from the input image and extract the first\nprincipal component. This technique captures the direc-\ntion of maximum variance in the feature space by perform-ing singular value decomposition on the covariance matrix,\nwhich is mathematically expressed as:\nPC1 = arg max\n\u2225v\u2225=1\nVar(Xv), (1)\nwhere X denotes the normalized feature matrix, and v is\nthe projection vector. The first principal component reflects\nthe primary mode of variation in the feature distribution.\nSubsequently, we binarize the projection values based on a\npredefined threshold \u03c4 to generate an initial mask:\nMinit[i] =\n(\n1 if PC1[i] > \u03c4\n0 otherwise . (2)\nSince it is initially unclear which part corresponds to\nforeground or background, we propose an adaptive deci-\nsion strategy based on variance analysis. Specifically, we\ninitially designate one region as foreground and the other\nas background, then compute the feature variance within\neach region and compare their medians. If the variance of\nthe foreground region is lower than that of the background\nregion, it suggests a potential misclassification, and we in-\nvert the mask accordingly. This strategy effectively mit-\nigates foreground-background misclassification caused by\nimproper thresholding.\nMinit =\n(\nMinit if MD(Var(Fmsk)) \u2265 MD(Var(Fn-msk))\n\u00acMinit otherwise ,\n(3)\nwhere Minit denote the initial binary mask (1 for masked\nregion, 0 for unmasked region), Fmsk \u2208 RNmsk\u00d7D and\nFn-msk \u2208 RNn-msk\u00d7D represent the feature matrices of the\nmasked and unmasked regions, respectively. Var (\u00b7) com-\nputes the per-channel variance (dimension: D \u00d7 1), and\nMD(\u00b7) returns the median value of the variance vector to\nsuppress the influence of outliers. The operator \u00ac denotes\nboolean mask inversion.\nTo address discrete noise in the initial mask, we apply 2D\nmorphological operations for post-processing. Specifically,\nwe use a k \u00d7 k square kernel to dilate the mask, enhancing\nregion connectivity. By combining dilation and erosion, we\neliminate holes and smooth the boundaries. This process is\nformally defined as:\nMfinal = Closing\n\u0000\nDil.\n\u0000\nMinit \u2208 {0, 1}H\u00d7W \u0001\n, K\n\u0001\n, (4)\nwhere K is the morphological kernel andH \u00d7W is the spa-\ntial dimension of the feature grid. Finally, the optimized\n2D mask is reshaped back to the feature vector dimension,\nyielding a boolean mask matrix used for element-wise fil-\ntering of the original features. This effectively suppresses\ninterference from low-information background regions.\nObject AU-ROC 0.05 F1 Score\nCan 58.61 0.18\nFabric 68.29 28.22\nFruit Jelly 80.93 48.27\nRice 92.92 68.44\nVial 69.04 35.88\nWallplugs 77.90 19.19\nWalnuts 89.23 75.05\nSheet Metal 76.80 40.13\nMean 76.71 39.42\nTable 1. AU-ROC 0.05 and segmentation F1 score (in%) on bina-\nrized images for TESTpublic set.\nIn all experiments, we set the number of PCA compo-\nnents to 1, the threshold \u03c4 to 1.0, and the morphological\nkernel size to3\u00d73. These parameters generalize well across\nvarious datasets and require no dataset-specific tuning. Ex-\nperimental results demonstrate that the proposed prepro-\ncessing method preserves the integrity of the main object\nfeatures while significantly reducing background noise in-\nterference, thereby improving model performance.\n2.2.2. Evaluation Criteria\nWe evaluate the model\u2019s performance primarily using the\npixel-level F1 score. This metric combines precision and\nrecall by computing their harmonic mean, thereby provid-\ning a balanced measure of the model\u2019s ability to detect\nanomalies at the pixel level. Specifically, the F1 score is\ncalculated as:\nF1 = 2\u00d7 Precision \u00d7 Recall\nPrecision + Recall, (5)\nwhere Precision denotes the proportion of predicted anoma-\nlous pixels that are truly anomalous, while Recall represents\nthe proportion of actual anomalous pixels that are correctly\nidentified by the model.\nDuring evaluation, we optimize the F1 score by adjust-\ning the decision threshold onTESTpublic dataset to determine\nthe optimal boundary for anomaly segmentation. This pro-\ncess ensures that the model achieves a balanced trade-off\nbetween precision and recall.\nThe use of the pixel-level F1 score as the evaluation\nmetric enables precise assessment of the model\u2019s capabil-\nity to identify anomalous regions in complex image data.\nNotably, the MVTec AD 2 dataset emphasizes the detec-\ntion of small defects, which may occupy only a few pixels\nin an image. In such cases, conventional metrics like the\nArea Under the Receiver Operating Characteristic Curve\n(AU-ROC) can be dominated by larger defects, thus fail-\ning to accurately reflect the model\u2019s performance on smaller\nanomalies. In contrast, the pixel-level F1 score places equalObject PatchCore [10] RD [4] RD+ [11] EfficientAD [1] MSFlow [14] SimpleNet [8] DSR [13] Ours\nCan 0.3 / 0.1 0.1 / 0.1 0.1 / 0.1 0.8 / 0.1 5.0 / 0.1 0.6 / 0.1 0.4 / 0.1 17.3/ 1.9\nFabric 11.5 / 9.8 2.6 / 2.2 2.9 / 2.3 7.6 / 1.0 22.0 / 4.1 21.6 / 10.2 7.9 / 5.0 77.4/ 65.3\nFruit Jelly 8.7 / 8.2 22.5 / 22.7 26.9 / 26.7 20.8 / 18.2 47.6/ 38.1 25.1 / 23.0 17.9 / 17.2 41.3 / 40.9\nRice 3.8 / 4.2 7.0 / 3.9 9.5 / 2.9 15.0 / 0.5 19.1 / 1.8 11.6 / 1.0 1.5 / 1.4 60.9/ 61.2\nSheet Metal 1.8 / 1.1 41.3 / 39.2 40.9 / 37.7 9.3 / 3.8 13.0 / 7.6 14.6 / 2.8 13.9 / 14.4 59.5/ 59.7\nVial 2.3 / 2.2 28.0 / 28.3 28.2 / 22.8 30.5 / 26.5 23.3 / 6.2 31.9 / 17.5 28.2 / 27.9 42.8/ 40.8\nWallplugs 0.0 / 0.0 1.9 / 0.8 1.3 / 0.9 4.4 / 0.3 0.1 / 0.2 1.0 / 0.3 0.4 / 0.4 13.7/ 6.7\nWalnuts 1.2 / 1.3 41.2 / 36.7 44.1 / 40.5 34.6 / 13.3 44.5 / 14.3 35.2 / 14.3 17.0 / 9.6 69.1/ 69.1\nMean 3.7 / 3.4 18.1 / 16.7 19.2 / 16.7 15.4 / 8.0 21.8 / 9.0 17.7 / 8.7 10.9 / 9.5 47.8/ 43.2\nTable 2. Performance comparison of segmentation F1 score (in%) on binarized images for TESTpriv / TESTpriv,mix set. The best results are\nhighlighted in bold.\nemphasis on detecting all anomalous regions, regardless of\ntheir size. This makes it particularly well-suited to the chal-\nlenges posed by the MVTec AD 2 dataset, as it can reliably\nevaluate the model\u2019s ability to correctly identify even the\nsmallest defects.\n3. Results\n3.1. Performance Metrics\nOur proposed method demonstrates excellent performance\nin pixel-level anomaly detection tasks. Specifically, on the\nTESTpublic dataset, the model\u2019s performance in terms of AU-\nROC0.05 and segmentation F1 score is summarized in Ta-\nble 1.\nMoreover, based on the official test results from the\nV AND 3.0 Challenge server, our model achieves an F1\nscore of 47.18% on the TESTpriv dataset and 42.51% on the\nTESTpriv,mix dataset. These results indicate that the model\nachieves a good balance between precision and recall, and\ncan effectively detect anomalous pixels in images. Notably,\nthe model maintains relatively stable performance even un-\nder distributional shifts in the data.\nTo more comprehensively evaluate the model\u2019s perfor-\nmance, we also consider other key metrics. On the TESTpriv\ndataset, the model achieves an AucPro0.05 score of 60.51%,\nwhile on the TESTpriv,mix dataset, the score is 58.37%.\nThese results suggest that the model has strong detection ca-\npabilities for anomalous regions across different thresholds\nand remains accurate even under variations in illumination\nand other environmental factors.\nIn addition, for image-level classification tasks, the\nmodel achieved ClassF1 scores of70.2% and 74.4% on the\nTESTpriv and TESTpriv,mix datasets, respectively, demonstrat-\ning strong capability in distinguishing between normal and\nanomalous samples at the image level.\n3.2. Comparison\nDue to the recent release of the MVTec AD 2 dataset and\nthe unavailability of Ground Truth for the TESTpriv and\nTESTpriv,mix test sets, we compare our proposed method with\nother approaches listed in the MVTec AD 2 dataset paper, as\nshown in Table 2. On both test sets, our method, SuperAD,\nconsistently outperforms previous methods. Moreover, our\nmethod requires no training, demonstrating superior gener-\nalization capabilities compared to the other approaches.\n4. Discussion\n4.1. Challenges & Solutions\nDuring our experiments, we observe that certain categories\n(such as Fruit Jelly, Vial and Wallplugs) exhibit\nhigh intra-class variability. When constructing the reference\nfeature memory bank using randomly selected images for\nthese categories, many normal regions are incorrectly iden-\ntified as anomalies due to the lack of sufficiently similar pat-\nterns in the memory bank. To address this issue, we propose\nselecting reference images using a greedy coreset selection\nstrategy rather than random sampling. This approach in-\ncreases the diversity of patterns within the memory bank\nand helps reduce false positives. A detailed explanation of\nthis method is provided in Section 2.1.2.\nWe further analyze the anomaly segmentation maps gen-\nerated from the similarity scores between the extracted fea-\ntures at four different layers and the reference images for\neach category. We observe that, due to the relatively clear\nseparation between foreground and background in most cat-\negories of the MVTec AD 2 dataset, false positives in back-\nground regions are generally limited. However, in the case\nof the Wallplugs category, some false detections still oc-\ncur in the background owing to its highly complex and di-\nverse patterns.\nTo gain deeper insights, we evaluate the effectiveness of\nPCA-based binary classification on the features extracted\nby DINOv2. Our results indicate that applying PCA to the\nshallow layers of DINOv2, with a threshold set to 1, effec-\ntively separates foreground and background regions. More\ndetails can be found in Section 2.2.1.\nBased on this observation, we apply a foreground feature(a) (b) (c) (d)\nGround Truth\nAnomaly map\nOriginal Img.\nFigure 2. Examples of typical failure cases: (a) false positives on\nair bubbles; (b) missed detection of blurred objects; (c) false posi-\ntives due to specular highlights; (d) missed detections of missing-\ntype anomalies.\nextraction preprocessing strategy to the Wallplugs cate-\ngory, which leads to improved segmentation performance\nfor this particularly challenging class.\n4.2. Model Robustness & Adaptability\nIt is worth emphasizing that our proposed method is entirely\ntraining-free, and thus does not require fine-tuning DINOv2\nfor any specific category. As a result, the generalization\nability of the model remains fully preserved.\nOur approach relies heavily on the powerful feature ex-\ntraction capabilities of DINOv2, which enables the extrac-\ntion of semantically rich representations suitable for com-\nparison across diverse categories.\nDuring the construction of the memory bank, we not\nonly select 16 reference images but also employ a greedy\ncoreset selection strategy to ensure that the selected images\nare as diverse as possible. This strategy effectively allevi-\nates the issue of limited pattern diversity within the memory\nbank, which might otherwise fail to represent the full range\nof intra-class variability. Consequently, this design further\nenhances the robustness of our model.\n4.3. Future Work\nAlthough our method achieves competitive performance,\nseveral prominent issues remain unresolved and warrant\nfurther investigation.\nFalse positives on air bubbles: As illustrated in Fig-\nure 2(a), Fruit Jelly category contains a wide variety\nof air bubbles with high uncertainty. These false positives\nare primarily caused by the diversity of bubble appearances,\nwhich often do not match the bubble patterns in the fixed\nmemory bank of normal features. In the future, the robust-\nness of the model to complex pattern variations could be\nenhanced to reduce such misclassifications.\nMissed detections of reflections and blurry objects:\nAs shown in Figure 2(b), inFruit Jelly category, light-\ncolored reflections and light scattering caused by the optical\nproperties of jelly can diminish the perceived abnormality\nof dark objects. The model may erroneously match such\nregions to normal patterns. Future work could improve the\nmodel\u2019s ability to recognize objects within transparent or\nsemi-transparent media to reduce missed detections.\nFalse positives on specular highlights: As depicted in\nFigure 2(c), in Vial and Can categories, specular high-\nlights caused by illumination often lead to false predictions.\nIncorporating advanced illumination compensation or high-\nlight removal techniques may enhance the model\u2019s robust-\nness to such lighting artifacts and reduce false positives.\nMissed detections of missing-type anomalies: As\nshown in Figure 2(d), in Sheet Metal and Vial cate-\ngories, missing-type anomalies are sometimes difficult to\ndetect due to their high visual similarity with the back-\nground. Future research may explore ways to enable model\nto better capture object completeness features, thereby im-\nproving its ability to detect missing-type anomalies.\n5. Conclusion\nIn this report, we propose a fully training-free anomaly de-\ntection and segmentation method that achieves robust per-\nformance under complex and varying lighting conditions.\nIn our method, we first employ a greedy coreset selection\nstrategy to select a small number of diverse normal ref-\nerence images. Then, leveraging the powerful represen-\ntational capacity of the DINOv2 model, we extract image\nfeatures from the selected references to construct a mem-\nory bank. For a test image, we extract multi-scale features\nand perform nearest neighbor matching with the memory\nbank at each scale to generate anomaly segmentation maps.\nThese maps are then averaged to produce the final result.\nOur method demonstrates that the large pre-trained\nmodel DINOv2 possesses excellent image representation\ncapabilities. Relying solely on these capabilities without\nany fine-tuning enables effective performance on dense pre-\ndiction tasks such as anomaly segmentation.\nReferences\n[1] Kilian Batzner, Lars Heckler, and Rebecca K \u00a8onig. Efficien-\ntad: Accurate visual anomaly detection at millisecond-level\nlatencies. In Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision , pages 128\u2013138,\n2024. 2, 5\n[2] Xuhai Chen, Yue Han, and Jiangning Zhang. April-gan:\nA zero-/few-shot anomaly classification and segmentation\nmethod for cvpr 2023 vand workshop challenge tracks 1&2:\n1st place on zero-shot ad and 4th place on few-shot ad.arXiv\npreprint arXiv:2305.17382, 2023. 2\n[3] Thomas Defard, Aleksandr Setkov, Angelique Loesch, and\nRomaric Audigier. Padim: a patch distribution modeling\nframework for anomaly detection and localization. In Inter-national conference on pattern recognition, pages 475\u2013489.\nSpringer, 2021. 2\n[4] Hanqiu Deng and Xingyu Li. Anomaly detection via reverse\ndistillation from one-class embedding. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 9737\u20139746, 2022. 5\n[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2\n[6] Lars Heckler-Kram, Jan-Hendrik Neudeck, Ulla Scheler, Re-\nbecca K \u00a8onig, and Carsten Steger. The mvtec ad 2 dataset:\nAdvanced scenarios for unsupervised anomaly detection.\narXiv preprint arXiv:2503.21622, 2025. 1, 2\n[7] Jianlong Hu, Xu Chen, Zhenye Gan, Jinlong Peng,\nShengchuan Zhang, Jiangning Zhang, Yabiao Wang,\nChengjie Wang, Liujuan Cao, and Rongrong Ji. Dmad:\nDual memory bank for real-world anomaly detection. arXiv\npreprint arXiv:2403.12362, 2024. 2\n[8] Zhikang Liu, Yiming Zhou, Yuansheng Xu, and Zilei Wang.\nSimplenet: A simple network for image anomaly detection\nand localization. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition , pages\n20402\u201320411, 2023. 5\n[9] Maxime Oquab, Timoth \u00b4ee Darcet, Th \u00b4eo Moutakanni, Huy\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 2\n[10] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard\nSch\u00a8olkopf, Thomas Brox, and Peter Gehler. Towards to-\ntal recall in industrial anomaly detection. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 14318\u201314328, 2022. 2, 5\n[11] Tran Dinh Tien, Anh Tuan Nguyen, Nguyen Hoang Tran,\nTa Duc Huy, Soan Duong, Chanh D Tr Nguyen, and\nSteven QH Truong. Revisiting reverse distillation for\nanomaly detection. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition , pages\n24511\u201324520, 2023. 5\n[12] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. arXiv preprint arXiv:1605.07146, 2016. 2\n[13] Vitjan Zavrtanik, Matej Kristan, and Danijel Sko \u02c7caj. Dsr\u2013\na dual subspace re-projection network for surface anomaly\ndetection. In European conference on computer vision, pages\n539\u2013554. Springer, 2022. 5\n[14] Yixuan Zhou, Xing Xu, Jingkuan Song, Fumin Shen, and\nHeng Tao Shen. Msflow: Multiscale flow-based framework\nfor unsupervised anomaly detection. IEEE Transactions on\nNeural Networks and Learning Systems, 2024. 2, 5",
      "Lazarus Group Targets Crypto-Wallets and Financial\nData while employing new Tradecrafts\nAlessio Di Santo (alessio.disanto@graduate.univaq.it)\nUniversit` a degli Studi dell\u2019Aquila, L\u2019Aquila, Abruzzo, Italy\nDate: November 26,2024\n\u201dNon videmus ea quae mox futura sunt\u201d\n(We do not see the things that will soon be) \u2014 Marcus Tullius Cicero\narXiv:2505.21725v1  [cs.CR]  27 May 2025Malware Analysis Report 1\nContents\n1 Executive Summary 2\n2 Introduction 2\n2.1 Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2.2 Infection Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n3 Methodology 4\n3.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3.2 Dynamic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n4 Analysis Results 6\n4.1 Malware Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n4.2 First-Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n4.2.1 Code Obfuscation . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n4.2.2 Code Analysis - error.js . . . . . . . . . . . . . . . . . . . . . . . 11\n4.3 Second-Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.3.1 Code Obfuscation . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.3.2 Code Analysis - sys info.py . . . . . . . . . . . . . . . . . . . . . . 22\n4.4 Third-Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.4.1 Code Obfuscation . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.4.2 Code Analysis - mlip . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.4.3 Code Analysis - pay . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4.4.4 Code Analysis - bow . . . . . . . . . . . . . . . . . . . . . . . . . 36\n4.5 Fourth Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n4.5.1 Code Obfuscation . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n4.5.2 Code Analysis - any.py . . . . . . . . . . . . . . . . . . . . . . . . 47\n4.5.3 Code Analysis - Windows Update Script.pyw . . . . . . . . . . . 52\n4.6 Fifth Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.6.1 Code Obfuscation . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.6.2 Code Analysis - TSUNAMI PAYLOAD . . . . . . . . . . . . . . . 58\n4.6.3 Executable Analysis - TSUNAMI INSTALLER . . . . . . . . . . 62\n4.7 Sixth Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n4.7.1 Code Obfuscation . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n4.7.2 Code Analysis - tsunami payload.exe . . . . . . . . . . . . . . . . 96\n5 Additional Analysis of Attacker\u2019s Infrastructure 100\n6 Mitigation Strategies 104\n7 Conclusion 105\nA Appendix 106\nA.1 IoCs, TTPs & Yara Rules . . . . . . . . . . . . . . . . . . . . . . . . . . 106\nA.2 Sigma Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\nA.3 Infection Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\nA.4 Diamond Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\nNovember 26, 2024Malware Analysis Report 2\n1 Executive Summary\n2 Introduction\n2.1 Objective\nThe objective of this Malware Analysis Report is to provide an in-depth understanding\nof the behavior, architecture, and intent of a malicious software instance. At its core,\nthis report serves as a crucial tool for identifying the characteristics and operations of the\nthreat, offering detailed insights that can be used to map the broader attack landscape.\nBy dissecting the capabilities and infrastructure of the malware, analysts are able to build\na clear picture of its functionality, origin, and potential impact.\nMapping a threat accurately is of paramount importance for defenders. A well-crafted\nmalware analysis report helps connect individual malicious artifacts with broader attack\ncampaigns and identifies common Techniques, Tactics, and Procedures(TTPs) employed\nby adversaries. This intelligence feeds into a larger knowledge base that allows cyber-\nsecurity teams to understand how threats evolve, recognize new campaigns with similar\nsignatures, and anticipate potential next steps of attackers. The report is not merely an\nexercise in detailing technical specifics but also a way of enriching the collective under-\nstanding of a Threat Actor\u2019s capabilities, motivations, and behaviors.\nActionable Threat Intelligence derived from malware analysis is particularly valuable\nbecause it enables proactive defenses. With a structured understanding of the malware\u2019s\nIndicators of Compromise (IOCs), behavioral patterns, and infrastructure, Threat Hunt-\ning and Monitoring teams are equipped with the context needed to seek out malicious\nactivity before it fully manifests. Threat Hunters can leverage this intelligence to identify\nadversarial presence across their environments more effectively, while Monitoring teams\ncan enhance detection logic and fine-tune alerts to identify these threats more accurately\nin real time. This coordinated approach bolsters an organization\u2019s defense posture, mak-\ning it possible to detect and respond to even well-structured, sophisticated threats that\nare designed to evade traditional security mechanisms.\nUltimately, a comprehensive malware analysis report provides not only a retrospective\nview of what a threat has done but also equips defenders with the tools and knowledge\nto better predict, detect, and prevent future attacks. This knowledge empowers security\nteams to make informed decisions, prioritize vulnerabilities, and improve their capabilities\nagainst Advanced Persistent Threats (APTs).\nNovember 26, 2024Malware Analysis Report 3\n2.2 Infection Chain\nFigure 1: Infection Chain Diagram\nNovember 26, 2024Malware Analysis Report 4\n3 Methodology\nAnalyzing the malware involved a comprehensive approach utilizing both static and dy-\nnamic analysis techniques to thoroughly understand its structure, behavior, and potential\nimpact. By combining these two approaches, it is possible to gain a comprehensive un-\nderstanding of the malware\u2019s capabilities and objectives. Static analysis provided insights\ninto its structure and obfuscation methods, while dynamic analysis revealed its real-time\nbehavior and interactions with the system. This dual approach was essential in developing\neffective detection and mitigation strategies against this sophisticated threat.\n3.1 Static Analysis\nStatic analysis is a fundamental technique in malware analysis that involves examining the\ncode of malicious software without executing it. This approach focuses on understanding\nthe structure, logic, and intent of the malware through methods such as disassembling,\ndecompiling, and reviewing its binary or script content. By analyzing the static properties\nof malware, such as strings, embedded resources, file headers, and imported functions,\nresearchers can gather valuable insights into its capabilities, communication patterns, and\npotential targets.\nThe main goal of static analysis is to dissect the malware\u2019s inner workings, identify\nhardcoded Indicators of Compromise (IoCs) like IP addresses, URLs, or file paths, and\ninfer its behavior without the risk of executing harmful code. This method is particu-\nlarly useful for uncovering obfuscation techniques, encrypted payloads, and multi-stage\narchitectures, which are often employed by modern malware to hinder direct analysis.\nHowever, static analysis comes with its challenges. Advanced malware frequently uses\nobfuscation, packing, or encryption to conceal its code and deter examination. Analysts\nmust rely on specialized tools and techniques, such as deobfuscation scripts, unpackers,\nand cryptographic analysis, to overcome these barriers. Moreover, analyzing assembly-\nlevel or machine code demands a high level of expertise, as the complexity of the malware\u2019s\nlogic can obscure its true intent.\nDespite its limitations, static analysis is invaluable as it allows analysts to preemp-\ntively assess a malware sample\u2019s potential threats, providing critical intelligence without\nthe inherent risks of execution. Combined with dynamic analysis, it forms a comprehen-\nsive approach to malware investigation, equipping defenders with the necessary under-\nstanding to develop effective detection and mitigation strategies.\n3.2 Dynamic Analysis\nDynamic analysis is a cornerstone of malware analysis, enabling researchers to observe the\nbehavior of malicious software in real-time by executing it within a controlled, isolated\nenvironment. This approach is particularly valuable for analyzing modern malware that\nemploys sophisticated obfuscation techniques, rendering static analysis alone insufficient.\nBy simulating realistic conditions, analysts can examine how malware interacts with the\nfile system, registry, processes, network, and system API s, providing direct insights into\nits functionality and intent.\nThe objective of dynamic analysis is to uncover the behavioral profile of the malware,\nrevealing actions such as data exfiltration, Command-and-Control communication, cre-\ndential theft, and persistence mechanisms. It also aids in identifyingIndicators of Compro-\nmise (IoCs), such as IP addresses, domains, and modified system configurations, which\nNovember 26, 2024Malware Analysis Report 5\nare crucial for detection and response efforts. This method is not without challenges,\nas modern malware often incorporates anti-analysis techniques designed to detect and\nevade Sandboxed Environments, Virtual Machines, or Debugging Tools. These measures\ninclude delaying execution, checking for artifacts indicative of analysis environments, and\nemploying runtime obfuscation to conceal its activities.\nDespite these difficulties, dynamic analysis remains a critical tool in the fight against\nadvanced threats. Its ability to reveal runtime behavior complements static analysis,\nproviding a comprehensive understanding of the malware\u2019s objectives and capabilities.\nWhile the process can be resource-intensive and time-consuming, its contributions to\ncybersecurity are indispensable, offering valuable intelligence to counteract and mitigate\nmalicious campaigns effectively.\nNovember 26, 2024Malware Analysis Report 6\n4 Analysis Results\n4.1 Malware Distribution\nOn November 13, 2024, an attempted social engineering attack was detected involving\nLinkedIn, a widely trusted professional networking platform. The target, a Web3 and\nblockchain developer, was approached by an individual posing as a representative of a\nreputable company in the NFT and blockchain space. The attacker initially framed their\napproach as a business opportunity, inviting the target to participate in an NFT gaming\nproject, as extensively reported by Luca Di Domenico on his Notion website.\nFigure 2: Attacker trying to engage its victim.\nThe interaction began with what appeared to be a standard recruitment message, con-\ntaining project details that aligned with the target\u2019s professional expertise and current\nindustry trends. The attacker followed up by requesting that the target download and\nrun a codebase hosted on Bitbucket, presented as part of a skill assessment process. How-\never, as communication progressed, subtle signs raised suspicion, prompting the target\nto further investigate the provided code.\nFigure 3: BitBucket malicious repository.\nUpon examination, the codebase was found to contain obfuscated scripts designed to\nperform unauthorized actions on the target\u2019s system. This discovery revealed the true\nNovember 26, 2024Malware Analysis Report 7\nnature of the message: a well-crafted attempt to execute malicious code under the guise\nof a professional opportunity. The following report outlines the timeline of events, initial\ndetection, and subsequent findings, detailing the approach used by the attacker and the\npotential risks identified.\nFigure 4: Obfuscated malicious code posed inside the error.js Middleware module.\n4.2 First-Stage\nFigure 5: First Stage\nThe initial JavaScript code is a highly obfuscated script crafted to execute malicious op-\nerations, including the deployment of additional payloads, collection of sensitive data and\nits subsequent exfiltration to a remote server under the attacker\u2019s control. The obfusca-\ntion layers serve to conceal its true intent, complicating analysis and detection efforts. By\ntargeting critical data such as credentials and cryptocurrency wallets, the script demon-\nstrates a deliberate focus on financial and personal information theft , aligning with its\nmalicious objectives.\n4.2.1 Code Obfuscation\nIn this section, there will be explored the various obfuscation techniques and decoy mech-\nanisms utilized in the code to hinder reverse engineering and analysis efforts. One of the\nprimary methodologies used is the adoption of meaningless and non-descriptive variable\nand function names. Variables such as 0x5647f0, 0x49e0, and functions like 0xb038d0\nare prevalent throughout the script. This practice obscures the code\u2019s intent, making it\nchallenging for a human reader to discern the purpose of different variables and functions.\nNovember 26, 2024Malware Analysis Report 8\nFigure 6: Variables are renamed to avoid leaking any useful insight.\nIn addition to meaningless naming, the code employs string encoding and lookup\ntables. Functions like 0xfee7 and 0x49e0 map obfuscated strings to their actual\nvalues using a lookup table, which is an array of strings that are themselves difficult to\ninterpret. This method effectively hides string literals and function names, complicating\nstatic analysis.\nFigure 7: Code employs lookup-tables for strings to reduce code understandability.\nFigure 8: Lookup-table content\nThe script makes extensive use of self-invoking functions and nested function wrap-\npers. These patterns complicate the control flow and make it harder to follow the sequence\nof execution. By encapsulating code within multiple layers of functions that immediately\ninvoke themselves, the script hides the true entry points and interconnections between\ndifferent parts of the code.\nNovember 26, 2024Malware Analysis Report 9\nFigure 9: An example of self-invoking and wrapped functions.\nAnother obfuscation technique introduced is the use of the function constructor for\ndynamic code execution. By constructing new functions at runtime, the script can gen-\nerate and execute code that is not visible in its static form, thereby concealing the actual\noperations being performed. This method hinders static analysis tools, which rely on\nexamining the code as it appears without executing it.\nFigure 10: Functions are instantiated at runtime to make it harder analyze source code.\nAnti-debugging and Anti-Tampering techniques are also employed. The script includes\nfunctions designed to detect if it is being debugged and alter its behavior, accordingly,\npotentially interfering with debugging efforts, by even invoking the debugger statement\ndynamically, which can cause debuggers to pause execution unexpectedly or enter infinite\nloops.\nFigure 11: Anti-Debugging functionalities\nNovember 26, 2024Malware Analysis Report 10\nIt also utilizes Opaque Predicates and Dead Code. These are conditions and code\nblocks that do not affect the overall program logic but are intended to confuse the analyst.\nOpaque predicates are conditions that always evaluate to true or false, making it difficult\nto determine the actual execution path, while Dead Code is never invoked.\nFigure 12: Example of Opaque Predicate.\nControl flow flattening is another technique used to obfuscate the code. By rearrang-\ning the normal execution flow and breaking it into smaller blocks with indirect jumps and\ncalls, the script makes it challenging to follow the logical sequence of operations. This\nmethod obscures the natural structure of the code, hindering attempts to map out its\nfunctionality. Numeric literals are often encoded in hexadecimal or expressed as compu-\ntations, making it harder to interpret constants directly. This adds an additional layer of\ncomplexity, as analysts must compute the actual numeric values to understand the code\u2019s\nbehavior.\nFigure 13: Numbers are hex-encoded to add complexity to code analysis.\nConfusing naming conventions are also used as a decoy strategy. The use of similar\nor repeating variable names with slight variations, such as 0x214ade and 0x2f409e,\ncan cause confusion. This practice makes it difficult to track variables and understand\ntheir roles in the code.\nFigure 14: Usage of confusing naming conventions for script imports.\nAdditionally, the script introduces unnecessary complex mathematical operations,\nincluding mathematical computations or expressions that serve no purpose can obfuscate\nthe actual logic and mislead analysts into thinking they are significant when they are not.\nBy nesting functions and using self-invoking patterns, the script creates multiple lay-\ners of execution that hide the entry point and make it harder to trace the execution\npath. Analysts may need to unravel several layers before reaching the core functionality,\nincreasing the effort required for analysis. The use of dynamic code generation with the\nfunction constructor serves as a decoy by obscuring the actual code being executed until\nruntime. This makes static analysis less effective, as the code\u2019s behavior cannot be fully\nunderstood without executing it.\nThe primary goal of these obfuscation techniques and decoy mechanisms is to prevent\neasy reading and understanding of the code. By making it difficult to interpret, the\nNovember 26, 2024Malware Analysis Report 11\nattacker aims to prevent quick detection of the malicious activities. The obfuscated code\ncan evade detection by static analysis tools that rely on pattern matching or signature-\nbased detection. Furthermore, by increasing its complexity, the attacker delays reverse\nengineering efforts. This added difficulties and pitfalls increases the time and effort\nrequired for analysts to de-obfuscate the code, which may allow the attacker more time to\nexploit the compromised system. The inclusion of decoy code and unnecessary complexity\nhelps hide the malicious intent within layers of confusing code, potentially leading analysts\ndown incorrect paths and causing them to misinterpret the code\u2019s purpose or miss critical\nmalicious components.\n4.2.2 Code Analysis - error.js\nBy investigating a refactored version of this code, it is possible to gather how the execution\nbegins with the invocation of the main function, which serves as the orchestrator of the\nscript\u2019s activities.\nFigure 15: Refactored main routine of the malicious JS file.\nInside this section the script first generates a UNIX timestamp to tag the exfiltrated\ndata uniquely. It then proceeds to collect information from various browsers by invoking\ncollectBrowserData for Chrome, Brave, and Opera browsers. The collectBrowser-\nData function determines the appropriate base directory for each browser based on the\noperating system and then calls collectExtensionData to harvest data from targeted\nextensions.\nNovember 26, 2024Malware Analysis Report 12\nFigure 16: Snippet of the refactored capabilities of collectBrowserData.\nFigure 17: Snippet of the refactored capabilities of collectExtensionData.\ncollectExtensionData scans through multiple browser profiles, attempting to find\nand collect data from extensions specified in the extensionIds array, which includes pop-\nular cryptocurrency wallets like MetaMask. For each profile and extension the identified\nthreat constructs the path to the extension\u2019s data directory and, if it exists, reads the\nfiles within. Each file is read and stored in an array along with its metadata, such as the\nfilename constructed from the browser prefix, profile number, extension ID, and original\nfilename. A complete list of all the extensions tracked is provided below:\n\u2022 nkbihfbeogaeaoehlefnkodbefgpgknn- MetaMask (A widely used cryptocurrency wallet\nfor Ethereum and ERC-20 tokens);\nNovember 26, 2024Malware Analysis Report 13\n\u2022 ejbalbakoplchlghecdalmeeeajnimhm - TronLink (The official wallet for the TRON\nblockchain);\n\u2022 fhbohimaelbohpjbbldcngcnapndodjp - LastPass: Free Password Manager (Helps users\nstore and manage passwords securely);\n\u2022 ibnejdfjmmkpcnlpebklmnkoeoihofec - Binance Chain Wallet (Official wallet for Bi-\nnance Chain, Binance Smart Chain, and Ethereum);\n\u2022 bfnaelmomeimhlpmgjnjophhpkkoljpa - Coinbase Wallet Extension (Allows users to\ninteract with decentralized applications (dApps) on the browser);\n\u2022 aeachknmefphepccionboohckonoeemg- Jaxx Liberty Wallet (A multi-currency, multi-\nplatform cryptocurrency wallet);\n\u2022 hifafgmccdpekplomjjkcfgodnhcellj - Exodus Wallet (Provides a user-friendly interface\nfor managing multiple cryptocurrencies);\n\u2022 jblndlipeogpafnldhgmapagcccfchpi- BitPay Wallet (Allows users to manage Bitcoin\nand other cryptocurrencies);\n\u2022 acmacodkjbdgmoleebolmdjonilkdbch - Nifty Wallet (Designed for interacting with\nEthereum and related dApps);\n\u2022 dlcobpjiigpikoobohmabehhmhfoodbb- Authy (A two-factor authentication (2FA) app\nto secure online accounts);\n\u2022 mcohilncbfahbmgdjkbpemcciiolgcge - Guarda Wallet (A non-custodial wallet sup-\nporting multiple cryptocurrencies);\n\u2022 agoakfejjabomempkjlepdflaleeobhb- Ledger Wallet (A hardware wallet extension for\nmanaging cryptocurrencies securely);\n\u2022 omaabbefbmiijedngplfjmnooppbclkk - OneKey Wallet (A hardware wallet extension\nproviding secure cryptocurrency storage);\n\u2022 aholpfdialjgjfhomihkjbmgjidlcdno - Math Wallet (Supports numerous blockchains\nand provides dApp support);\n\u2022 nphplpgoakhhjchkkhmiggakijnkhfnd - SafePal Wallet (Offers secure cryptocurrency\nmanagement with hardware and software solutions);\n\u2022 penjlddjkjgpnkllboccdgccekpkcbin- Yoroi Wallet (A light wallet for Cardano (ADA)\ncryptocurrency);\n\u2022 lgmpcpglpngdoalbgeoldeajfclnhafa- Phantom Wallet (A friendly Solana wallet built\nfor DeFi and NFTs);\n\u2022 fldfpgipfncgndfolcbkdeeknbbbnhcc - Brave Wallet (The built-in crypto wallet of the\nBrave browser);\n\u2022 bhhhlbepdkbapadjdnnojkbgioiodbic- Ronin Wallet (Used for the Axie Infinity game\nand manages NFTs and tokens on the Ronin network);\nNovember 26, 2024Malware Analysis Report 14\n\u2022 gjnckgkfmgmibbkoficdidcljeaaaheg - XDEFI Wallet (A cross-chain wallet extension\nsupporting multiple blockchains);\n\u2022 afbcbjpbpfadlkmhmclhkeeodmamcflc - MEW CX (MyEtherWallet Extension) (Pro-\nvides access to Ethereum accounts directly in the browser).\nFigure 18: Crypto-related browser extensions list.\nIf the collectSolana flag is true, the script also attempts to collect the Solana id.json\nfile from the user\u2019s home directory. This file often contains sensitive wallet information.\nAfter this information gathering activity is completed, the script calls sendData to\nexfiltrate the collected files to the attacker\u2019s server.\nFigure 19: Malicious function designed to exfiltrate data to remote C2 server.\nNovember 26, 2024Malware Analysis Report 15\nThe sendData function constructs a form data object containing the type, a unique\nhost identifier , the timestamp, and the array of collected files . It then uses the request\nmodule to perform an HTTP POST request to the attacker\u2019s server, effectively transmit-\nting the stolen data.\nReturning to the main function (Figure 15), the script also calls collectFirefoxData\nto target Mozilla Firefox profiles. This function navigates through Firefox\u2019s profile di-\nrectories, specifically those containing -release in their names, and searches for extension\ndata within the storage/default directory. It targets moz-extension directories and col-\nlects IndexedDB files used by extensions, which may contain sensitive information.\nFigure 20: Function designed to collect Firefox profiles and extensions.\nThe script further attempts to collect data from the Exodus cryptocurrency wallet by\ninvoking collectExodusData. Depending on the operating system, it constructs the\npath to the exodus.wallet directory and collects any files found within it. These may\ncontain wallet data , private keys, or transaction histories.\nNovember 26, 2024Malware Analysis Report 16\nFigure 21: Function designed to collect the Exodus Cryptowallet information.\nFor Windows systems, the script additionally targets Microsoft Edge by calling col-\nlectExtensionData with the appropriate path. This increases the scope of data collec-\ntion to include users who primarily use Edge. The script then performs a platform check\nto determine whether to collect login data. On macOS systems (platform starting with\n\u2019d\u2019), it calls collectLoginDataMac to collect the macOS keychain file (login.keychain\nor login.keychain-db) and the Login Data files from Chrome and Brave browsers. The\nkeychain may contain passwords, certificates, and secure notes, while the Login Data files\nstore saved login credentials.\nNovember 26, 2024Malware Analysis Report 17\nFigure 22: Function designed to collect the macOS Keychain and browser\u2019s login data.\nFor other platforms, the script calls collectLocalStateAndLoginDatafor Chrome,\nBrave, and Opera browsers. This function collects the Local State file , which contains\nbrowser settings and encryption keys, and the Login Data files from each browser profile.\nBy collecting these files, the attacker aims to access encrypted passwords and other\nsensitive data stored by the browsers.\nNovember 26, 2024Malware Analysis Report 18\nFigure 23: Function designed to collect credentials from different Browser.\nAfter completing the data collection, the script calls executeAdditionalCode to\ndownload and execute further malicious code.\nFigure 24: Function related to download and execution of the subsequent infection stages.\nNovember 26, 2024Malware Analysis Report 19\nIn executeAdditionalCode, the script checks if it is running on a Windows sys-\ntem and whether a Python interpreter exists at /.pyp/Python.exe. If it does, the script\ndownloads a Python script from the attacker\u2019s server and executes it using the available\ninterpreter. If the latter is not present, the script calls downloadAndExtractZip to\ndownload and extract a legit Python3.11 archive named p.zip. For non-Windows sys-\ntems, the script directly downloads a .py script and executes it using Python3. This\nallows the attacker to execute additional code on the victim\u2019s machine.\nFigure 25: Function designed to download and extract a compressed Python 3.11 inter-\npreter if not available on target machine.\nThis function uses the curl command to download an archive from the attacker\u2019s\nserver. If the download fails, it retries after 20 seconds. Upon successful download,\nit renames and extracts the archive into the user\u2019s home directory, then proceeds to\nexecute the additional code and remove the stored archive. Throughout the script, helper\nfunctions such as normalizePath and fileExists are used to handle file paths and check\nfor the existence of files or directories.\nFigure 26: normalizePath and fileExists functions snippet.\nThese functions ensure that the script can correctly navigate the file system across\ndifferent operating systems, enhancing its effectiveness and portability. At the end of the\nscript, an interval is set to repeat the main function every five minutes, up to a total of\nNovember 26, 2024Malware Analysis Report 20\nthree executions. By repeatedly executing the main function, the script ensures that it\ncan capture any new data that may have been added since the last execution, such as\nnewly saved passwords or wallet transactions. This repetition increases the chances of\ncollecting valuable information over time.\nFigure 27: Main function is executed every 5 minutes on the compromised host.\nBased on the observed behaviors and technical characteristics of the analyzedJavaScript\ncode, it is plausible to associate the subjected threat with the BeeverTail malware fam-\nily. The latter is recognized for its advanced data-stealing capabilities, particularly tar-\ngeting browser extensions and cryptocurrency wallets. The code operates by infiltrating\nsystems and scanning browser profiles across multiple web browsers, including Google\nChrome, Brave, Opera, Mozilla Firefox , and Microsoft Edge. It specifically targets ex-\ntensions associated with popular cryptocurrency wallets such as MetaMask, TronLink,\nand Exodus Wallet. By accessing data stored by these extensions, the malware aims to\nextract sensitive information like private keys, seed phrases, and wallet files , potentially\ncompromising users\u2019 cryptocurrency assets. Additionally, the malware harvests login cre-\ndentials and browser data by accessing files like Login Data and Local State from browser\nprofiles. These files may contain encrypted usernames, passwords, and session cookies.\nThe exfiltration of collected data to remote servers controlled by the attackers, typi-\ncally using HTTP POST requests, aligns with the data exfiltration methods employed by\nBeeverTail. Also the usage of port 1224, known URL path as /pdown/ and a Python-\nbased second-stage payload. To evade detection and hinder analysis, malware employs\nadvanced obfuscation techniques. It uses meaningless variable and function names, mak-\ning the code difficult to read and understand. Strings are encoded and utilized through\nlookup tables to conceal actual values and function calls. Control flow flattening is used\nto alter the logical flow of the program, complicating efforts to follow the execution path.\nMoreover, dynamic code execution is implemented using the function constructor and\nself-invoking functions , allowing the malware to execute code dynamically at runtime.\nThe analyzed code also demonstrates the capability to download and execute additional\nmalicious code from remote servers. By installing legitimate-looking software, such as a\nPython interpreter, it can run further scripts without raising suspicion. This modular\napproach allows the malware to enhance its capabilities, maintain persistence, and adapt\nto different environments, which is consistent with BeeverTail\u2019s behavior.\nThe malware known as BeeverTail has often been utilized as a delivery mechanism\nfor subsequent stages, notably deploying the malware family InvisibleFerret. The be-\nhavior exhibited by the error.js file, which was analyzed in this report, aligns closely\nwith this pattern. The specific set of Tactics, Techniques, and Procedures (TTPs) and\nIndicators of Compromise (IoCs) associated with this file have been extensively docu-\nmented as characteristic of the DPRK Threat Actor Lazarus Group.\nNovember 26, 2024Malware Analysis Report 21\n4.3 Second-Stage\nFigure 28: Moving from First to Second Stage.\n4.3.1 Code Obfuscation\nThe identified Second-Stage payload is located inside a Python script, stored as%TEMP%\n\\sysinfo script.py and downloaded fromhxxp[:]//86.104.74[.]51:1224/client/8/87, car-\nrying the initial stage of the InvisibleFerret malware family.\nFigure 29: Second-Stage payload content.\nAs observed in the preceding image, the malware employs a sophisticated obfuscation\nstrategy designed to hinder analysis. To reveal the underlying payload, analysts must\nreverse the provided string, decode it using base64, and decompress the resulting output.\nThis sequence of operations must be repeated fifty times before the actual malicious\npayload becomes accessible.\nThis obfuscation technique is consistently applied across nearly all subsequent Python\nscripts identified in the malware\u2019s progression. Even scripts initially stored in clear text\nat earlier stages are later written to disk using the same obfuscation mechanism. This\ndeliberate and systematic use of layered obfuscation underscores the attacker\u2019s intent to\nevade static detection and impede reverse engineering attempts.\nNovember 26, 2024Malware Analysis Report 22\n4.3.2 Code Analysis - sys info.py\nFigure 30: Second-Stage imported modules\nIdentified script defines several variables and sets up the environment. It uses theplatform\nmodule to determine the operating system type, which is stored in the variable ot. This\ninformation is subsequently used to decide how the payloads will be handled. The user\u2019s\nhome directory is determined and stored in the variable home, and a hidden folder named\n.n2 is created within this directory to store the downloaded payloads. By storing the\npayloads in a hidden folder, the script aims to avoid detection by the user.\nFigure 31: Remote connection configurations\nThe first payload is handled by the function download payload(). It checks if\nthe payload file, named %USERPROFILE%\\.n2\\pay, already exists in the hidden\ndirectory. If it does, it attempts to remove it. Then, the script ensures that the di-\nrectory .n2 is created if it does not already exist. The payload is downloaded from\n86.104.74[.]51:1224, with additional parameters ( sType and gType, campaign identi-\nfiers) passed in the URL. The downloaded content is saved in the hidden directory, and\nonce the download is successful, the script proceeds to execute the payload. If the system\nis Windows, the payload is executed using the subprocess.Popen() method with specific\nflags to suppress the console window and create a new process group, making the exe-\ncution less noticeable. Otherwise, for macOS systems, the payload is executed without\nthese flags.\nFigure 32: Malicious function designed to retrieve and run pay Python script.\nNovember 26, 2024Malware Analysis Report 23\nA specific condition is implemented for macOS systems, identified by platform as\nDarwin. After the first payload is downloaded and executed, the script terminates if it\nis running on macOS, implying that subsequent parts of the script are not meant to be\nexecuted on this platform.\nThe script then continues to download and execute two additional payloads through\nthe functions download browse() and download mclip(). Like the process described\nfor the first payload, each of these functions first checks whether the corresponding file\nalready exists, removing it if necessary. It also ensures that the hidden directory .n2 is\npresent. The second payload, named %USERPROFILE%\\.n2\\bow, still a Python\nscript, is downloaded from a different endpoint on the same server, and the content is\nsaved and executed in the same way as before.\nFigure 33: An additional Python payload is downloaded from the same C2 server.\nThe third payload, named%USERPROFILE%\\.n2\\mlip, follows the same down-\nload, save, and execute procedure, using yet another endpoint on the server and still\nemploying a Python script.\nFigure 34: A third Python script is then downloaded and executed.\nAdditionally, as illustrated in Figure 31 and Figure 32, the Threat Actor appears to\nhave left behind comments within the code that point to potential debugging targets.\nThe inclusion of a private IP address and an alternative URL for retrieving the pay\nscript suggests that the attacker might have been testing the functionality of this threat.\nAlternatively, this could indicate a rushed deployment, where programmers neglected to\nremove these debugging artifacts prior to release. Regardless of the reason, these elements\nprovide valuable intelligence, offering insight into the attacker\u2019s development process and\npotentially aiding in attribution or threat profiling.\nNovember 26, 2024Malware Analysis Report 24\n4.4 Third-Stage\nFigure 35: Moving from Second to Third Stage.\nAs previously noted, this specific infection-stage provides a clear indication of the new\ntradecrafts being employed by the Lazarus Group in this campaign. Notably, the\nintroduction of a new Python script, mlip, first identified only a few weeks prior to the\ndiscovery of this campaign, signifies a deliberate evolution in their operational approach.\nAdditionally, an unprecedented payload embedded within the bow script was identified\nduring this investigation, further underscoring the group\u2019s intent to expand their arsenal\nof malicious tools.\nThese developments suggest that the Threat Actor is actively seeking to extend their\ncapabilities, aligning with their shift in focus over recent years. WhileLazarus historically\ntargeted industry leaders, such as Sony and Blockbuster, their operations have increas-\ningly pivoted toward exploiting individuals and organizations within the cryptocurrency\nand technology sectors. This strategic redirection leverages a combination of social en-\ngineering, sophisticated malware, and multi-stage attack chains, marking a significant\ndeparture from their earlier campaigns focused on traditional industrial targets.\n4.4.1 Code Obfuscation\nAll of the Python scripts involved in this stage are obfuscated with the same technique\ndescribed in Section 4.3.1.\n4.4.2 Code Analysis - mlip\nmlip defines a malicious script designed to capture sensitive information from a user\u2019s\nsystem, specifically targeting cryptocurrency data such as private keys and mnemonic\nphrases. It functions as a keylogger and clipboard monitor, intercepting keystrokes and\nclipboard contents when the user interacts with certain web browsers, and then transmit-\nting this data to a remote server.\nNovember 26, 2024Malware Analysis Report 25\nAt the beginning of the script, the main section attempts to import several modules\nnecessary for its operation. If any of these modules are not present, the script automat-\nically installs them using pip. This ensures that all dependencies are met without user\nintervention.\nFigure 36: mlip imports and missing libraries installation.\nThis pattern repeats for modules likepsutil, win32process, win32gui, win32api, win32con,\nwin32clipboard, requests, and wx. The script uses these modules to interact with Windows\nsystem APIs, handle HTTP requests, and interact with GUI applications.\nAs first it initializes several global variables, including the server\u2019sIP address and port\nto which the stolen data will be sent, and a list of targeted web browsers. Thus, indicating\nthat the script specifically monitors these processes. Developers also left a commented-out\nHOST, highlighting how localhost was probably used for testing purposes.\nFigure 37: Hard-coded very useful information\nThe act win pn() function retrieves information about the active window, such as\nthe process ID, process name, and window caption. These information is used to determine\nif the user is interacting with one of the targeted browsers.\nFigure 38: act win pn() function code snippet\nThe script then defines several utility functions to check the state of control keys\nand to save logs. Indeed, save log() function is particularly important as it sends the\ncaptured data to the remote server using an HTTP POST request.\nNovember 26, 2024Malware Analysis Report 26\nFigure 39: C&C Server URL and exfiltration parameters.\nThe OnKeyboardEvent()function is a callback that is triggered on every keyboard\nevent. It checks if the active process is one of the targeted browsers and captures the\nkeystrokes. This function also intercepts clipboard data when the user pastes content\nusing Ctrl+V, invoking GetTextFromClipboard()to process the clipboard contents.\nAdditionally, the script sets up a keyboard hook using pyHook to monitor all keyboard\nevents.\nFigure 40: Callback function to trigger Keylogging activity.\nIn addition to keystroke logging, the script defines theTestFrameclass, which inherits\nfrom wx.Frame. This class sets up a clipboard viewer that monitors changes to the\nclipboard.\nNovember 26, 2024Malware Analysis Report 27\nFigure 41: TestFrame class initialization\nWithin this class, the OnDrawClipboard()method is called whenever the clipboard\ncontent changes. It processes the new clipboard data to detect potential private keys or\nmnemonic phrases.\nFigure 42: OnDrawClipboard()code snippet\nThe GetTextFromClipboard()method retrieves the clipboard text and checks if it\ncontains sensitive information.\nFigure 43: Function designed to capture and retrieve sensitive data.\nNovember 26, 2024Malware Analysis Report 28\nThe savepvkey() method searches for hexadecimal strings of specific lengths that\nmay represent private keys. Similarly, theismnemonic() method checks if the clipboard\ncontent consists of 12, 16, or 24 words, which are common lengths for mnemonic seed\nphrases in cryptocurrency wallets.\nFigure 44: savepvkey() and ismnemonic() implementations\nFinally, the main loop of the script creates an instance of the TestFrame class and\nstarts the application. This ensures that the clipboard monitoring continues to run as\nlong as the application is active.\nFigure 45: Main loop\nIn conclusion, the script operates by covertly logging keystrokes and clipboard contents\nwhen the user interacts with specific web browsers. It specifically targets data that\nresembles cryptocurrency private keys or mnemonic phrases. The captured data is then\ntransmitted to a remote server without the user\u2019s consent, representing a significant\nsecurity and privacy threat.\nUnused code in the script appears minimal, as most functions and classes are integral\nto its malicious functionality. However, certain error handling or exception cases might\nnot be fully fleshed out, potentially causing the script to fail silently under unexpected\nconditions.\nAdditionally, further OSINT investigations revealed how this code was built by in-\ncorporating code available on some Online-Forums ( ActiveState and Douban). In both\nthe provided websites, there is available the exact same code the attacker embedded in\nits threat to interact with the compromised system\u2019s clipboard.\nNovember 26, 2024Malware Analysis Report 29\nFigure 46: Code shown in Figure 41 was found on Online Python Forums.\n4.4.3 Code Analysis - pay\nProposed script is a malicious program designed to infiltrate a victim\u2019s computer, gather\nsensitive information, and establish persistent remote control . It combines several mali-\ncious functionalities, including system reconnaissance, data exfiltration, remote command\nexecution, keylogging, and clipboard monitoring. The malware is crafted to operate on\nboth Windows and non-Windows systems, adapting its behavior while also being able to\ndownload and execute the aforementioned bow Python script. This indeed highlight the\nenhanced resilience the Threat Actor employed in its tradecrafts.\nStarting from the main execution point, the script initiates its malicious activities by\nimporting essential modules and defining global variables that will be used throughout\nits operation. It begins by importing modules such as base64, socket, uuid, hashlib,\ngetpass, platform, and time. These imports are crucial for network communication, system\ninformation retrieval, and cryptographic functions.\nNovember 26, 2024Malware Analysis Report 30\nFigure 47: pay script\u2019s imports\nThe script defines sType and gType, constants in this campaign and used to uniquely\ndefine it within their various compromising activities.\nThe main function of the script is encapsulated within the run comm() function,\nwhich initiates the transmission of collected system and network information to the at-\ntacker\u2019s server. It does so by creating an instance of the Trans class and calling its\ncontact server() method.\nFigure 48: Snippet of run comm() function\nWithin the Trans class, the init method collects system and network information\nby instantiating the SysInfo class and calling its get info() method. This method ag-\ngregates system information such as the operating system, hostname, release version, and\nuser details , as well as network information like IP address and geolocation data. Ad-\nditionally, by comparing information provided in Figure 49 and Figure 51 it is possible\nto gather how the attacker set up two different ports to achieve two different malicious\npurposes. Port 1224 is used to extract geographical victim\u2019s information, while Port2247\nwill be used as a remote C2 Endpoint to bind an interactive shell between the Threat\nActor and the victim\u2019s system. It is also interesting to highlight how Figure 49 shows\ntwo commented host variable containing seemingly base64 encoded information. As it\nwill be discussed in Sec. 4.5.2, this same string is manipulated to retrieve the remote C2\nServer. Thus, denoting a possible on-the-fly change applied to the inner workings of their\nscripts, either due to changing their habits or experimenting obfuscation boundaries for\nAV detection.\nFigure 49: Trans class initialization\nNovember 26, 2024Malware Analysis Report 31\nThe SysInfo class leverages the HostInfo and Position classes to gather this informa-\ntion. The HostInfo class collects system-related data, while the Position class retrieves\nnetwork-related information.\nFigure 50: HostInfo class maps host information into a dictionary to be exfiltrated\nIn the HostInfo class, the getID() method generates a unique identifier for the vic-\ntim\u2019s machine by hashing theMAC address and username. This UUID is used to uniquely\nidentify the infected system.\nThe Position class retrieves the internal IP address and geolocation data by mak-\ning a request to hxxp[:]//ip-api[.]com/json, which returns the public IP and associated\ngeolocation information.\nFigure 51: Position class is designed to gather geographical information from victim\u2019s\nIP.\nAfter collecting all the necessary information, the Trans class\u2019s contact server()\nmethod sends this data to the attacker\u2019s server using an HTTP POST request Figure 49.\nFurthermore, developers introduced a dictionary, C, which contains a timestamp, the\ntype identifier , host identifier , a label sys info, and the collected system and network\ninformation. This data is then sent to the attacker\u2019s server at the specified HOST and\nPORT.\nFollowing the initial data exfiltration, the script attempts to establish a persistent\nconnection to the attacker\u2019s Command and Control (C2) server to receive further in-\nstructions. It defines the Client class, which handles the connection setup and maintains\nthe communication loop.\nNovember 26, 2024Malware Analysis Report 32\nFigure 52: Client class setups an interpretative connection to attacker\u2019s servers.\nThe make connection() method attempts to establish a socket connection to the\nattacker\u2019s server. If successful, it creates a Session object for low-level communication\nand a Shell object to handle commands. The Shell class contains methods for executing\nvarious commands received from the attacker, such as running shell commands, uploading\nfiles, and manipulating processes.\nThe Shell class is responsible for interpreting and executing various commands sent\nby the attacker, effectively acting as a remote shell. It maintains the session state, handles\nincoming commands, and dispatches them to the appropriate methods.\nFigure 53: Shell class provides the attacker with RAT capabilities.\nIn the Shell class\u2019s constructor, it initializes various attributes and defines a dictionary\nself.cmds that maps command codes to their corresponding methods. These methods\nhandle different functionalities such as executing shell commands, terminating processes,\nuploading files, and more.\nThe listen recv() method continuously listens for incoming commands from the\nattacker.\nNovember 26, 2024Malware Analysis Report 33\nFigure 54: Function listen recv() code snippet\nThe method receives data from the session, parses it, and dispatches it to the ap-\npropriate handler method based on the command code. It uses threading to handle\ncommands concurrently.\nThe shell() method starts the listener thread and keeps the shell active until it\u2019s\nterminated.\nFigure 55: Shell() translates attacker\u2019s command into ones to be executed on target.\nBelow are some of the handler methods in the Shell class:\n\u2022 ssh obj(self, args) : This method allows the attacker to execute arbitrary shell\ncommands on the victim\u2019s machine and returns the output;\n\u2022 ssh cmd(self, args): Terminates Python processes running on the victim\u2019s ma-\nchine;\nNovember 26, 2024Malware Analysis Report 34\n\u2022 ssh clip(self, args): Sends the contents of the clipboard to the attacker;\n\u2022 ssh upload(self, args): This method provides the attacker with the ability to\nsearch for and exfiltrate files from the victim\u2019s system;\n\u2022 ssh kill(self, args): Terminates specific processes, such as web browsers;\n\u2022 ssh any(self, args): These methods collectively enable the attacker to perform\na wide range of malicious activities on the victim\u2019s machine, from executing com-\nmands and terminating processes to uploading and downloading files.\nFigure 56: Function used to download any.py, which gets and runs AnyDesk.\nAn additional essential part of the malware\u2019s operation is its capability to search for\nand exfiltrate sensitive files from the victim\u2019s system. It defines patterns and exclusion\nlists to target specific files while avoiding others. The ld() function recursively lists files\nin directories, excluding those that match the specified patterns. It collects file paths\nthat are then used by the ups() function to upload the files to the attacker\u2019s server.\nFigure 57: Arrays embedding file\u2019s extensions to be serached on target system.\nThe ups() function handles the file upload process, sending the collected files to the\nattacker\u2019s server via HTTP POST requests.\nFigure 58: How files with known extensions are exfiltrated.\nNovember 26, 2024Malware Analysis Report 35\nThe malware also incorporates keylogging and clipboard monitoring capabilities. As\nfirst it ensures that these modules are present, then malware can interact with the Win-\ndows API to capture keystrokes and clipboard content. The keylogging functionality is\ninitiated in the run client() function, which starts a thread to hook keyboard and mouse\nevents.\nFigure 59: run client() deploys keyboard hooking functionality.\nThe hk loop() function sets up the hooks for keyboard and mouse events using\npyHook. Within the event handlers , the script captures keystrokes and writes them to a\nbuffer. It also captures clipboard content when the user performs copy or paste actions.\nIn the hkb() function, the script checks if control keys are pressed and handles special\nkeys accordingly. It also sets up timers to capture clipboard content shortly after copy\nor paste actions are detected.\nFigure 60: Main Hooking routine\nAt the end of the script, the run client() function is called within the main block\nto start the malware\u2019s execution.\nRegarding unused code, there are several sections where function calls are commented\nout, such as in the auto up() function. This function is intended to search for files\nwith patterns related to cryptocurrency wallets and configuration files, but the calls are\ncommented out, possibly to avoid immediate detection or to be activated under certain\nconditions.\nNovember 26, 2024Malware Analysis Report 36\nFigure 61: Crypto-Wallet related patten which have been commented out.\nAdditionally, the write txt() function is defined but does not perform any operation.\nIt may have been intended to log captured keystrokes or clipboard content to a file but\nremains unused.\nFigure 62: Unused function write txt()\nIn conclusion, the script is a complex piece of malware that performs multiple ma-\nlicious activities, including system information gathering ,data exfiltration, remote com-\nmand execution, file searching and uploading, keylogging, clipboard monitoring, and the\nability to retrieve and execute bow script. The latter provides additional resilience in\ncase sys info.py fails to correctly download it.\nThis threat also leverages various Python modules and Windows API functions to\ninteract with the system and maintain persistence by establishing a connection with the\nattacker\u2019s server. The presence of unused code suggests that the malware may have\nadditional capabilities that are not currently active but maybe intended for future use.\n4.4.4 Code Analysis - bow\nbow was previously employed as a Browser credentials\u2019 dumper. However, by deobfus-\ncating this script, beside the aforementioned well-known malicious functionality, designed\nto steal browser\u2019s credentials, there was found, embedded and obfuscated, an additional\nmalicious payload with the aim of delivery the Tsunami toolset.\nFigure 63: Snippet of the additional Tsunami suite embedded in Bow script.\nAs first, the credential stealing capabilities will be discussed, later also the newly\nidentified functionalities will be analyzed as well.\nNovember 26, 2024Malware Analysis Report 37\nBrowser Credentials Stealer\nbow is a malicious program designed to extract sensitive information such as saved pass-\nwords and credit card details from various web browsers installed on a user\u2019s system. It\ntargets multiple browsers, including Chrome, Brave, Opera, Yandex, and Microsoft Edge,\nacross different operating systems like Windows, Linux, and macOS. The script decrypts\nthe stored credentials and exfiltrates them to a remote server controlled by the attacker.\nStarting from the main execution point, the script begins by importing necessary\nmodules and setting up the environment. It attempts to import critical libraries required\nfor its operation, and if they are not present, it installs them using pip to ensure all\ndependencies are met. This includes libraries for HTTP requests, cryptographic functions,\nand OS-specific modules for accessing system resources.\nFigure 64: pip imports and management of missing libraries.\nThe script sets up several global variables, including sType, gType, host1 and home,\nwhich are used throughout the code for exfiltration and path resolution. It also determines\nthe hostname of the machine and constructs URLs for communication with the attacker\u2019s\nserver.\nFigure 65: Global variables definition and C2 server remote URL construction.\nThe script defines classes representing different browser versions it aims to target.\nEach class inherits from a base class BrowserVersion and specifies the browser\u2019s base\nname along with version identifiers for Windows, Linux, and macOS. An array avail-\nable browsers holds all the browser classes the script will attempt to extract data from.\nNovember 26, 2024Malware Analysis Report 38\nFigure 66: Classes defining all the targeted victims\u2019 browsers.\nThe core functionality resides within the ChromeBase class and its subclasses for each\noperating system. This provides methods for decrypting stored credentials and retrieving\ndata from browser databases.\nIn the ChromeBase class, the get decorator is used to dynamically update paths to\nthe browser\u2019s data directories based on the operating system and browser versions.\nFigure 67: Snippet of the ChromeBase class\nThe retrieve database() method in ChromeBase is responsible for copying the\nbrowser\u2019s login data database , decrypting stored passwords, and collecting them for ex-\nfiltration.\nFigure 68: retrieve database() targets Chrome locally stored credentials.\nSimilarly, the retrieve web() method extracts credit card information stored by the\nbrowser.\nNovember 26, 2024Malware Analysis Report 39\nFigure 69: retrieve web() targets credit cards information.\nFor Windows systems, the Windows class inherits from ChromeBase and implements\nWindows-specific methods for decrypting passwords. It uses the win32crypt module to\ninteract with Windows Data Protection API (DPAPI) for decryption.\nFigure 70: Windows class initialization and browsers\u2019 paths.\nFor Linux systems, the Linux class implements methods to retrieve the encryption\nkey from the GNOME Keyring using the secretstorage module.\nNovember 26, 2024Malware Analysis Report 40\nFigure 71: Linux class initialization and browsers\u2019 paths.\nFor macOS systems, the Mac class retrieves the encryption key from the Keychain\nusing system commands.\nFigure 72: Mac class initialization and browsers\u2019 paths.\nAt the end of the script, the main execution flow determines the operating system\nand initializes the appropriate class to perform the data extraction. It iterates over each\navailable browser, retrieves stored credentials, and sends them to the attacker\u2019s server.\nNovember 26, 2024Malware Analysis Report 41\nFigure 73: Main routine adapting its behavior within the identified OS.\nThe save() method in ChromeBase is responsible for exfiltrating the collected data\nby sending an HTTP POST request to the attacker\u2019s server.\nIn this method, self.pretty print() formats the extracted data into a readable string,\nwhich is then sent to the server specified by host2. The data includes timestamps, host\nidentifiers, and the collected credentials.\nFigure 74: save() function setups the exfiltration process.\nUnused code in the script is minimal, with some commented out sections at the end\nthat may have been used for debugging or cleanup purposes.\nFigure 75: Commented clean-up last lines.\nThis most probably indicates an intention to remove the script after execution, pos-\nsibly to cover its tracks, but it is commented out, so it doesn\u2019t execute.\nIn conclusion, this bow script component operates by methodically accessing browser\nstorage files, decrypting sensitive information , and sending it to a remote server with-\nout the user\u2019s consent. It uses platform-specific methods to handle encryption and file\npaths, making it adaptable to various operating systems and browsers. The code is well-\nstructured, leveraging object-oriented programming to encapsulate functionality for each\noperating system and browser type, which enhances its effectiveness as a malicious tool.\nAdditionally, with some further OSINT investigations, it has been possible to find an-\nother IoC related to the same Threat Actor, hosting this same script on another server\nin the past.\nFigure 76: Bow was hosted, in the past, on this server.\nNovember 26, 2024Malware Analysis Report 42\nTsunami\nWith reference to Figure 63, the first lines of the identified bow script were embedding\nan additional malicious obfuscated payload. By applying the same 50-iterations deobfus-\ncation process, as done for all the previously mentioned Python scripts, it was possible\nto gather its content.\nThe latter is a piece of malware designed to ensure that Python is installed on a\nWindows system and to persistently execute a secondary malicious script, referred to as\nthe TSUNAMI INJECTOR, by placing it in the system\u2019s startup folder. The script\nemploys obfuscation techniques to conceal the secondary payload and attempts to gain\nelevated privileges to install Python if it is not already present.\nStarting from the main execution point, the script begins by importing several mod-\nules necessary for its operation. These imports include standard libraries for system\ninteraction, such as subprocess, platform, tempfile, winreg, ctypes, random, base64, zlib,\ntime, sys, and os. The script also attempts to suppress warnings to avoid drawing at-\ntention during execution. This suppression ensures that any warnings generated by the\nscript are ignored, which is typical in malicious software to prevent the user from noticing\nunexpected behavior.\nFigure 77: Script\u2019s imports\nThe script begins by defining several global variables that are essential to its operation.\nThe DEBUG MODE flag is initialized as False, ensuring that the script suppresses debug\noutput during execution unless explicitly enabled. This configuration emphasizes the\nmalware\u2019s intent to operate covertly, minimizing any indicators of its presence.\nAmong the critical variables is the URL for downloading a Python installer, which\npoints to an official Python repository. This mechanism enables the script to ensure that\na Python interpreter is installed on the target system, a prerequisite for executing its\nsubsequent stages. The inclusion of this step highlights the malware\u2019s adaptability and\nits capability to dynamically establish its required runtime environment.\nThe script also determines the path to the AppData Roaming directory, a commonly\nutilized location in Windows for storing user-specific application data. This directory\nis leveraged to construct the storage path for the TSUNAMI INJECTOR , the sec-\nondary malicious payload. The variables specify the name, folder, and full path where\nthis payload will reside. Additionally, the TSUNAMI INJECTOR SCRIPT vari-\nable is allocated to contain the actual code of this secondary stage, which serves a critical\nNovember 26, 2024Malware Analysis Report 43\nrole in advancing the malware\u2019s objectives. A detailed examination of this payload and\nits functionality will be discussed in Sec. 4.5.3.\nFigure 78: Script\u2019s global variables\nThe obfuscate script() function takes the script data and a loop count to determine\nthe level of obfuscation. It replaces a placeholder variable RandVar with a random\ninteger to ensure that the obfuscated script differs on each execution (avoiding an easy\nfingerprinting through hashing). In this function, the script repeatedly compresses and\nencodes the data, then reverses the encoded string. The obfuscation loop runs for the\nspecified loop count, which is set to 50 in the main block , making the resulting script\nhighly obfuscated and difficult to analyze. This technique is the same one used until now\nfor all the identified Python scripts, here we can have a direct look on how the attacker\nimplemented this by itself.\nFigure 79: 50-iterations obfuscation technique implementation.\nUtility functions are defined to assist with the script\u2019s operations. The output function\nconditionally prints debug messages if DEBUG MODE is enabled.\nNovember 26, 2024Malware Analysis Report 44\nFigure 80: Debugging mode\nThe download file() function uses PowerShell to download a file from a given URL\nto a specified file path.\nFigure 81: Function designed to download remote utilities.\nBy utilizing PowerShell \u2019s Invoke-WebRequest cmdlet, the script avoids raising network-\nrelated flags that might occur with other methods.\nThe script proceeds to define functions under the Tsunami Infecter section, which\nhandle the installation of Python if it is not already present. Theis Python installed()\nfunction checks the Windows registry to determine if Python is installed on the system.\nFigure 82: Function designed to check whether a Python interpreter is available on target\nmachine.\nNovember 26, 2024Malware Analysis Report 45\nThis function attempts to open thePythonCore registry key under bothHKEY LOCAL\nMACHINE and HKEY CURRENT USER to check for installed Python versions. If no\nversions are found, it concludes that Python is not installed.\nThe execute Python with uac() function tries to run the Python installer with\nadministrative privileges using the Windows ShellExecute API :\nFigure 83: Function designed to runas to install a Python interpreter.\nBy specifying the runas verb, the script prompts the User Account Control (UAC)\ndialog to request elevated privileges. The installer is executed with silent installation\nparameters to avoid user interaction.\nThe install Python() function orchestrates the download and installation of Python\ninside a newly created temporary file path, and attempts to execute it with elevated\nprivileges. If the user denies the UAC prompt, the script waits for a random interval\nbetween 10 and 30 seconds before retrying, persistently attempting to install Python.\nFigure 84: Function designed to run Python installer and prompting for user administra-\ntive permissions via UAC.\nIn the main section of the script, the execution flow begins by checking if Python\nis installed. If Python is not installed, it proceeds to download and install it using\nthe methods previously described. Once Python is confirmed to be installed, the script\nwrites the obfuscated TSUNAMI INJECTOR to the Windows Startup folder to\nensure persistence. The obfuscate script() function is called with a loop count of 50,\nNovember 26, 2024Malware Analysis Report 46\nresulting in a heavily obfuscated script that is difficult to analyze or detect by security\nsoftware. The script is saved with a .pyw extension, which allows Python scripts to run\nwithout opening a console window, further hiding its execution. The script includes a\ncheck for DEBUG MODE, and if enabled, it waits for user input to keep the window\nopen. The entire script is also wrapped in a try-except block that silently passes any\nexceptions.\nFigure 85: Script\u2019s main routine\nIn conclusion, the script functions as a dropper that ensures Python is installed on the\ntarget Windows system, leveraging administrative privileges if necessary. It then installs\na persistent, obfuscated secondary payload in the startup folder to achieve persistence and\nexecute additional malicious activities each time the system boots. The use of obfuscation\nand silent error handling indicates an attempt to evade detection and analysis, which is\ncharacteristic of malicious software designed to compromise system security without the\nuser\u2019s knowledge.\nMoreover, as it is possible to see from the following image, the attacker posed, in the\nfirst lines of the Windows Update Script.pyw script a peculiar citation.\nFigure 86: Interesting citation available inside Windows Update Script.pyw .\nThe quote, Sometimes you never know the value of a moment until it becomes a\nmemory, is often attributed to Dr. Seuss, although its precise origins are uncertain. The\nphrase captures a universal truth about human experience: we often fail to recognize the\nsignificance of events as they happen and only appreciate them in hindsight. However,\nno additional insights about the usage of this were identified, either as associated to the\nthreat or the Threat Actor itself.\nNovember 26, 2024Malware Analysis Report 47\n4.5 Fourth Stage\nFigure 87: Moving from Third to Fourth Stage.\n4.5.1 Code Obfuscation\nIn this stage, as yet reported in the previous section, Windows Update Script.pyw\nwas obfuscated with the well-known 50-iterations process. On the other hand, any.py\nis not ubfuscated at all.\n4.5.2 Code Analysis - any.py\nany.py is a malicious program designed to manipulate the configuration of AnyDesk,\na popular remote desktop application, on a target system. The script aims to modify\nAnyDesk\u2019s configuration files to inject predetermined credentials, potentially allowing\nunauthorized remote access to the system. It also attempts to download and execute\nAnyDesk if it is not already present, and ensures that AnyDesk is running with the\nmanipulated configuration. Finally, the script cleans up bydeleting itself from the system.\nThese imports include modules for system interaction ( os, platform, subprocess, sys),\nnetworking (socket, requests), and data encoding/decoding ( base64, time).\nThe script then determines the operating system type and retrieves environment vari-\nables essential for its execution. Starting from the main execution point, the script begins\nby importing necessary modules that facilitate its operation. The os type variable holds\nthe name of the operating system, which is crucial for setting file paths and executing\nOS-specific commands. The appdata variable retrieves the path to the local application\ndata directory on Windows systems. Next, the script defines variables that are used to\nconstruct the URL of a remote server controlled by the attacker. Here, host is a base64-\nencoded string that, when decoded, provides the IP address of the attacker\u2019s server. The\nNovember 26, 2024Malware Analysis Report 48\nhn variable stores the hostname of the victim\u2019s machine, and sType is likely used to cat-\negorize the type of data being sent to the server. The script then decodes the host string\nto obtain the actual server address. In the following snippet, this string is manipulated\nby rearranging its parts before decoding. The slicing host[8:] + host[:8] swaps the first\neight characters with the rest as a rudimentary obfuscation technique. After decoding,\nhost1 contains the server address (95.164.17[.]24), andhost2 constructs the full URL with\na specific port ( 1224).\nFigure 88: any.py imports and global variables\nThe script then defines a function save conf() that reads the contents of a given\nfile and sends it to the attacker\u2019s server. This function checks if the file fn exists. If it\ndoes, it reads the file\u2019s contents into buf. If the latter is not empty, it constructs a data\npayload options containing the file content and sends it to the attacker\u2019s server via an\nHTTP POST request to the /keys endpoint. The script then sets up paths and variables\nnecessary for interacting with AnyDesk\u2019s configuration. It defines the home directory\nand initializes an empty list files. The variable any path specifies the default installation\npath of AnyDesk on Windows systems.\nFigure 89: Defining AnyDesk path and configuring C2 connection to share its settings.\nA function get anydesk path() is defined to locate or download AnyDesk if it is not\nalready installed.\nNovember 26, 2024Malware Analysis Report 49\nFigure 90: Funtion designed to establish AnyDesk presence on target system.\nThis function first checks if AnyDesk exists at the default path. If not, it attempts\nto download AnyDesk from the attacker\u2019s server ( host2 + /any ). The downloaded exe-\ncutable is saved in the user\u2019s home directory as anydesk.exe. The function then returns\nthe path to the AnyDesk executable. The script proceeds to determine the paths to Any-\nDesk\u2019s configuration files based on the operating system. For Windows systems, it sets\nconf path1 and conf path2 to the possible locations of AnyDesk\u2019s service.conf file. For\nnon-Windows systems, it sets the paths accordingly. If neither configuration file exists on\na Windows system, the script attempts to run AnyDesk. This step ensures that AnyDesk\nis running, potentially causing it to create the service.conf file, which the script intends\nto modify.\nFigure 91: Script maps AnyDesk\u2019s configurations related paths.\nIt then defines a PowerShell script as a multi-line string anydesk ps1.\nNovember 26, 2024Malware Analysis Report 50\nFigure 92: anydesk ps1 variable content\nThis script reads the AnyDesk configuration file, replaces certain lines with predefined\nvalues (specifically pwd hash, pwd salt, and token salt), and writes the changes back to\nthe file. It then forcefully terminates AnyDesk.\nThe core function that performs the configuration file modification is update conf.\nFigure 93: Function designed to update AnyDesk configurations.\nThis function first checks if the configuration file atd path exists. It then reads the file\nto see if it already contains the attacker\u2019s pwd salt. If not, it proceeds to modify the file.\nIt opens the existing configuration file for reading and a new file (d path + d ) for writing.\nIt copies all lines except those starting with ad.anynet.pwd hash=, ad.anynet.pwd salt=,\nor ad.anynet.token salt=. It then writes the attacker\u2019s predefined values for these settings\nto the new file.\nNovember 26, 2024Malware Analysis Report 51\nIf direct file modification fails (possibly due to permissions), the function attempts\nto execute the previously defined PowerShell script with elevated privileges. It writes\nthe PowerShell script to a file ( conf.ps1) and executes it using a subprocess call with\nStart-Process -Verb RunAs, which prompts for administrative rights.\nThe script then calls update conf() on both configuration file paths. After attempt-\ning to update the configuration files, the script defines a function restart anydesk to\nrestart the AnyDesk application.\nFigure 94: Configurations update and AnyDesk restart.\nThis function kills any running AnyDesk processes and restarts them subsequently.\nOn non-Windows systems, it uses the psutil library to iterate over running processes and\nterminate them. On Windows, it uses the taskkill command. After killing the process, it\nwaits for one second and restarts AnyDesk using the anydesk path determined earlier.\nThe script then saves the (possibly modified) configuration files to the attacker\u2019s\nserver. By calling save conf(), the script reads the contents ofconf path1 and conf path2\nand sends them to the server, allowing the attacker to retrieve the configuration files.\nFinally, the script restarts AnyDesk and deletes itself.\nFigure 95: Manipulation of the AnyDesk configuration and settings.\nDeleting itself is a common tactic in malware to reduce forensic evidence and avoid\ndetection.\nRegarding unused code, the script includes commented-out print statements and ex-\nception handling that does not report errors. These comments suggest that during de-\nvelopment, the script output errors for debugging purposes, but these were suppressed in\nthe final version to avoid revealing its activities.\nIn conclusion, the script is a malicious tool designed to manipulateAnyDesk\u2019s configu-\nration to insert known credentials, potentially granting the attacker unauthorized remote\naccess to the victim\u2019s system. It ensures AnyDesk is installed and running, modifies\nNovember 26, 2024Malware Analysis Report 52\nconfiguration files with predetermined values, restarts AnyDesk to apply changes, and\nexfiltrates the configuration files to the attacker\u2019s server. The script takes measures to\navoid detection by deleting itself after execution and suppressing error messages.\n4.5.3 Code Analysis - Windows Update Script.pyw\nThis specific Python script is designed to establish persistence on a Windows system by\ncreating scheduled tasks, downloading and executing additional malicious payloads , and\nbypassing security measures such as Windows Defender. The script employs various ob-\nfuscation techniques to conceal its activities and evade detection. It attempts to escalate\nprivileges by prompting the User Account Control (UAC) dialog to gain administrative\nrights for executing its payloads.\nStarting from the main execution point, the script begins by importing several modules\nnecessary for its operation. These imports provide functionalities for network communi-\ncation, file handling , system interaction, encryption, and obfuscation. The script defines\nalso a global variable RandVar, which is assigned a random integer value. This variable\nis used within the obfuscation process to ensure that each deobfuscated script instance is\nunique. Next, the script sets up several global variables that determine paths and names\nused throughout its execution.\nFigure 96: Script\u2019s imports and anti-fingerprinting variable RandVar.\nThe script introduces several critical global variables that govern its behavior and\nfacilitate the deployment of its malicious components. The DEBUG MODE flag is used\nto toggle debug output, remaining disabled in its default state to minimize any detectable\nartifacts during execution.\nPaths to the AppData directories, both Roaming and Local, are retrieved using the\nvariables ROAMING APPDATA PATH and LOCAL APPDATA PATH. These directo-\nries are commonly exploited by malware due to their accessibility and legitimate usage\nin Windows environments.\nFor the malicious payload, TSUNAMI PAYLOAD NAME dynamically generates a\nrandom 16-character string to obfuscate the filename and evade static detection. The\nvariables TSUNAMI PAYLOAD FOLDER and TSUNAMI PAYLOAD PATH are used\nto specify the temporary directory and complete file path for the payload\u2019s storage, rein-\nforcing the attack\u2019s stealth. Similarly, the names and paths for the malicious installer are\nNovember 26, 2024Malware Analysis Report 53\ndefined using TSUNAMI INSTALLER NAME, TSUNAMI INSTALLER FOLDER, and\nTSUNAMI INSTALLER PATH. These variables ensure precise control over the place-\nment and execution of the installer within the compromised system.\nLastly, the script embeds a multi-line string containing the payload\u2019s code, assigned to\nTSUNAMI PAYLOAD SCRIPT. This design ensures that the payload is readily available\nfor execution without requiring an immediate download, thus increasing the resilience and\neffectiveness of the attack.\nFigure 97: Global variables embedding additional payloads information and paths.\nThe script contains an embedded payload script as a multi-line string assigned to\nTSUNAMI PAYLOAD SCRIPT, designed to be obfuscated and executed later.\nFigure 98: Code snippet of the embedded TSUNAMI PAYLOAD SCRIPT.\nWithin this embedded script, the add windows defender exception() function\nattempts to add specific file paths to the Windows Defender Exclusion List by executing\nPowerShell commands. The create task() function creates a scheduled task named\nRuntime Broker that executes the malicious installer at user logon with administrative\nprivileges.\nThe obfuscate script() function is responsible for obfuscating the payload script\n(identical to the one shown in Figure 79). Windows Update Script.pyw as first\ndeploys and run this Python script to make arrangements for the next deploy of the\nTSUNAMI INSTALLER . Indeed, it will apply AV exclusions for the executable\nNovember 26, 2024Malware Analysis Report 54\npath and will also create a scheduled task to allow its run at each user\u2019s login. At this\npoint, the script will exploit the is task scheduled() to check if this scheduled task\nexists with a PowerShell query.\nFigure 99: Function designed to check whether Runtime Broker.exe is in a scheduled\ntask.\nThen, the script defines functions to decrypt and decode an obfuscated URL from\nwhich it downloads an additional malicious payload. These functions perform xor en-\ncryption/decryption (key: !!!HappyPenguin1950!!!) and base64 decoding to retrieve the\nactual URL. These are encrypted and store in the URLS array, which has a size of 1000\nstrings. Each one of these is composed of a Profile Name, a \u2019 \u2019 and a File Name (e.g.\nGlassesMagenta6644 MassageRecorded9001).\nFigure 100: Functions designed to decrypt the strings embedded in the URLS array.\ndownload installer url() shuffles the URLS list and then begin looking for existing\nprofiles and blacklisting non-existing ones. It also disablesSSL and employs asUser-Agent\nthe string Mozilla/5.0. In details, it retrieves from each single encrypted string the Profile\nName. Thus, looks for a document, named as the File Name value, which will contain\nthe path for the additional payload download, on Pastebin.\nNovember 26, 2024Malware Analysis Report 55\nFigure 101: download installer url() queries Pastebin profiles and find existing ones.\nFigure 102: Function designed to download and decode data from Pastebin.\nDuring the dynamic analysis of this sample, a hit was found among the 1000 possible\nprofiles when attempting to connect to hxxps[:]//Pastebin[.]com/u/TwelveThrows2886.\nAs expected, TwelveThrows2886 InductionInteriors4401 was the corresponding encrypted\nstring and thus the only available file in this profile was named exactly InductionInteri-\nors4401. This file ( hxxps[:]//pastebin.com/raw/suEqUQBY) hosts an encoded string.\nNovember 26, 2024Malware Analysis Report 56\nFigure 103: Pastebin profile contacted to retrieve the additional payload.\nFigure 104: Pastebin file containing the encoded URL for the additional payload location.\nThe decoded string translates to hxxp[:]//23.254.229.101/cat-video and delivers a file\nnamed cat video.mp4. This is instead a reversed gzip archive which contains Runtime\nBroker.exe and gets stored inside the following path:%APPDATA%\\Microsoft\\Windows\\\nApplications\\Runtime Broker.exe.\nThe script then defines functions to download the TSUNAMI INSTALLER and\nexecute the TSUNAMI PAYLOAD with elevated privileges. download installer()\ndownloads the malicious installer, decodes it, and saves it to the specified path. ex-\ntract payload()writes the obfuscated payload script to a temporary file.execute paylo\nad with uac() attempts to execute the payload with administrative privileges by invok-\ning ShellExecuteW with the runas verb.\nFigure 105: download installer() code snippet\nNovember 26, 2024Malware Analysis Report 57\nFigure 106: Function designed to employ runas to install Python as admin.\nIn the main section of the script, the execution flow is as follows.\nFigure 107: Script\u2019s main routine\nThe script checks if the scheduled task Runtime Broker exists. If it does and the\nTSUNAMI INSTALLER is not present, it downloads and installs this malicious\nexecutable. Otherwise, if it is present, it exits. Then, If a task for the TSUNAMI\nINSTALLER is not scheduled, it attempts to execute the TSUNAMI PAYLOAD,\nwith elevated privileges, to schedule it. Thus, this script repeatedly prompts the UAC\ndialog until the user grants administrative rights. Once the TSUNAMI PAYLOAD\nexecutes successfully, it exits the loop.\nNovember 26, 2024Malware Analysis Report 58\nWhile investigating the comments written inside this script, it is possible to find\na reference about an extensive explanation of how the decryption URL schema works,\nhosted on the attacker\u2019s Youtube Channel. However, this is just a joke since it redirects\nto the Never Gonna Give You Up video (basically RickRolling analysts).\nFigure 108: Developers RickRolling analysts.\nIn conclusion, the script is a sophisticated piece of malware that aims to compromise\na Windows system by installing malicious payloads , achieving persistence, and evading\nsecurity measures. It uses multiple layers of obfuscation and encryption to conceal its\nactions and relies on social engineering (prompting UAC dialogs) to gain elevated priv-\nileges. The script\u2019s modular structure allows it to perform various malicious activities\nwhile making analysis and detection challenging.\n4.6 Fifth Stage\nFigure 109: Moving from Fourth to Fifth Stage\n4.6.1 Code Obfuscation\nAs discussed in the previous section, the TSUNAMI CLIENT script is written to disk\nwith the well-known 50-iterations obfuscation schema. On the other hand, TSUNAMI\nINSTALLER executable is not a packed executable.\n4.6.2 Code Analysis - TSUNAMI PAYLOAD\nTSUNAMI PAYLOAD, as mentioned above, is a malicious program designed to\nestablish persistence on a Windows system by creating scheduled tasks and modifying\nNovember 26, 2024Malware Analysis Report 59\nWindows Defender settings to exclude certain files from scanning . The script attempts\nto run with administrative privileges to modify system settings , adds specific file paths\nto the Windows Defender Exclusion List , and creates a scheduled task that executes a\nmalicious payload named Runtime Broker.exe at user logon. This behavior allows the\nmalware to evade detection and maintain persistence across system reboots.\nStarting from the main execution point, the script begins by importing necessary\nmodules that facilitate interaction with the operating system and system-level functions.\nThese imports enable the script to execute subprocesses (such as PowerShell com-\nmands), interact with Windows API functions for privilege escalation checks, and ma-\nnipulate file paths.\nFigure 110: Script\u2019s imports\nThe script defines global variables that are crucial for its operation. DEBUG MODE\nflag is set to False, indicating that debug output is suppressed during normal execution.\nThe script retrieves the paths to the roaming and Local AppData directories using en-\nvironment variables. These paths are used to construct locations where the malicious\npayload and related files will be stored. The script specifies the name and paths for the\nTSUNAMI INSTALLER, which is actually a disguised malicious executable. As a\nfirst analysis it is possible to have a look at this executable name, which is all but random,\nsince it tries to mimic known Windows one RuntimeBroker.exe. The latter is indeed a le-\ngitimate system process designed to manage permissions for modern Universal Windows\nPlatform (UWP) applications. Its primary role is to act as a broker between these appli-\ncations and the operating system, ensuring that apps operate within their defined permis-\nsion boundaries. For instance, it monitors access to sensitive resources like location, mi-\ncrophone, and file systems, prompting the user when permissions are requested. The legit-\nimate RuntimeBroker.exe process is typically spawned by its parent process, svchost.exe,\nwhich is responsible for hosting various system services and its path is located in the Win-\ndows system directory, specifically at C:\\Windows\\System32\\RuntimeBroker.exe. This\nlocation is a key indicator of authenticity, as any instance of RuntimeBroker.exe found\noutside the System32 directory is likely malicious or suspicious, just like in this specific\ncase.\nFigure 111: Global variables declarations\nNovember 26, 2024Malware Analysis Report 60\nThrough continued code analysis, it becomes evident that the is admin() function is\nimplemented to verify whether the script is executing with administrative privileges. This\nis achieved by invoking the IsUserAnAdmin() function from the shell32 library. This\nfunction provides a straightforward mechanism to determine if the current user context\nhas the necessary elevated permissions to perform privileged operations. If administrative\nprivileges are not present, the script may encounter limitations in executing tasks that\nrequire such permissions, potentially resulting in failed operations or the bypassing of\nrestricted functionality. This check ensures that the script can conditionally adapt its\nbehavior based on the level of access available.\nThe script then defines functions that perform the core malicious activities. The\nadd windows defender exception() method adds specified file paths to the Windows\nDefender Exclusion List .\nFigure 112: Functions designed to check user\u2019s permissions and apply AV exclusions.\nThis function constructs a PowerShell command that invokes Add-MpPreference to\nexclude the specified filepath from Windows Defender scans . By doing so, the malware\nattempts to prevent its executable from being detected or removed by the antivirus\nsoftware.\nInstead, create task() function creates a scheduled task that ensures the malicious\npayload runs at every user logon. In this function, a multi-line PowerShell script is\nconstructed to define a new scheduled task. The task is configured with the following\nparameters:\n\u2022 Action: Executes the malicious payload located at TSUNAMI INSTALLER PATH.\n\u2022 Trigger: Set to trigger at user logon ( -AtLogOn).\n\u2022 Principal: Runs under the current user\u2019s context with interactive logon type and\nelevated privileges (RunLevel = 1 ).\n\u2022 Settings: Configured to allow the task to start even if the system is on battery\npower and to not stop the task if the system switches power states.\nNovember 26, 2024Malware Analysis Report 61\nThe task is registered using Register-ScheduledTask, ensuring that the malicious payload\nwill persist and execute whenever the user logs in.\nFigure 113: Function designed to add Runtime Broker.exe as a scheduled task.\nThe script first checks for administrative privileges by calling is admin(). If the\nscript is not running as an administrator, it outputs a warning message (ifDEBUG MODE\nis enabled). However, it proceeds with execution regardless of the privilege level, which\nmay result in certain functions failing silently due to insufficient permissions. These paths\ninclude:\n\u2022 The main malicious payload (Runtime Broker.exe) stored in theAppData Roam-\ning directory;\n\u2022 A secondary payload or client component also named Runtime Broker.exe in\nthe AppData Local directory;\n\u2022 msedge.exe which should host the XMRig cryptocurrency miner.\nBy adding these paths to the exclusion list, the malware attempts to prevent Win-\ndows Defender from scanning or quarantining these files, allowing malicious activities\nto proceed unhindered. The script iterates over the EXCEPTION PATHS and calls\nadd windows defender exception() for each. After modifying the Windows De-\nfender settings, the script proceeds to create the scheduled task by calling create task().\nThis ensures that the malicious payload is executed at every user logon, establishing per-\nsistence on the system. Finally, if DEBUG MODE is enabled, the script waits for user\ninput before exiting, which is useful for testing or analysis purposes.\nNovember 26, 2024Malware Analysis Report 62\nFigure 114: Main routine\nIn conclusion, the script functions as a persistence mechanism for a malicious payload\non a Windows system. It attempts to elevate privileges , modifies Windows Defender\nsettings to exclude its files from scanning, and creates a scheduled task that executes\nthe payload at user logon. The use of familiar names like Runtime Broker.exe and\nplacement within system-like directories aims to disguise the malware and avoid raising\nsuspicion. The script\u2019s ability to run without administrative privileges may limit its\neffectiveness, as certain operations require elevated permissions. The presence of unused\ncode suggests that the malware may have additional capabilities that are not active in\nthis version or that code has been removed or altered during obfuscation.\n4.6.3 Executable Analysis - TSUNAMI INSTALLER\nRuntime Broker.exe acts as a central orchestrator of malicious operations. This pro-\ncess engages in a broad spectrum of activities that exploit native system utilities and\nfunctions, establishing a foothold in the system , evading detection, enabling persistence\nand deploying a C2 TOR channel .\nStatic Analysis\nThis analysis reveals several advanced anti-analysis techniques implemented within sub-\njected executable. For instance, there are multiple matches indicating access to the\nProcess Environment Block (PEB) to detect the presence of a debugger, as logged in\nmatches for PEB access. This behavior aligns with previously observed anti-debugging\nand anti-analysis methods, emphasizing the malware\u2019s intent to evade dynamic sand-\nbox environments and indicates reliance on low-level system structures for evasion, likely\npreceding more overt malicious actions to ensure execution only in non-analytical en-\nvironments (e.g. exploiting isDebuggerPresent function). Also, it is possible to find\nexecution delays trough Sleep, Software Breakpoints checks, Debug Break, GetTickCount\nand QueryPerformanceCounter invokes.\nNovember 26, 2024Malware Analysis Report 63\nFigure 115: QueryPerfomanceCounter invoke\nFigure 116: DebugBreak invoke\nAnother significant discovery is the use of API calls such as VirtualAlloc and Vir-\ntualProtect to allocate and modify memory permissions dynamically. These suggest the\nmalware includes functionality for memory-based payload staging and execution, poten-\ntially leveraging reflective injection techniques. This capability allows the malware to\ninject code into other processes or execute shellcode directly from allocated memory,\nincreasing its stealth.\nFigure 117: ShellExecuteW invoke\nNovember 26, 2024Malware Analysis Report 64\nThe static analysis also identifies logic for delaying execution using API s like SleepEx,\nwith the intention of bypassing automated sandboxes or security tools that rely on time-\nouts to detect malicious behavior. These deliberate delays enable the malware to outlast\ndynamic analysis environments that may prematurely conclude monitoring, ensuring its\nfunctionality is triggered only in live systems.\nFigure 118: SleepEx invoke\nFurthermore, the file exhibits the capability to compress and decompress data using\nZlib (compress data via Zlib inflate or deflate) and encode/encrypt data using base64\nand xor. These functionalities strongly correlate with obfuscation techniques observed\nduring behavioral analysis, where repeated file and payload manipulation were recorded.\nFor example, Zlib compression is used in the malware\u2019s payload delivery mechanism to\nreduce file size and disguise its contents.\nFigure 119: Set of values possibly associated with xor activities.\nNew insights from the static analysis also highlight capabilities for obtaining sys-\ntem locale and geographical information, as seen in the following image. This discovery\nintroduces the possibility that the malware is region-specific or dynamically adapts its\nbehavior based on the host\u2019s location.\nNovember 26, 2024Malware Analysis Report 65\nFigure 120: GetLocaleInfoEx invoke\nFurther investigation of the malware\u2019s embedded strings has uncovered the presence\nof debugging information, left behind by the developers. These artifacts provide valuable\ninsights into the attacker\u2019s behavior and offer a deeper understanding of the development\nprocess behind this malicious tool. By analyzing these remnants, analysts can better\nfingerprint the attacker\u2019s techniques and gain additional intelligence about their testing\nenvironments, coding practices, and potential oversights. This evidence underscores the\noften iterative and sometimes rushed nature of malware development.\nFigure 121: Debugging strings left behind by malware developers.\nStatic Analysis - Runtime Broker.dll\nThe unusually large size of RuntimeBroker.exe prompted an examination of its raw\nhex code to uncover potential embedded components. This analysis revealed the presence\nof eighty-seven distinct executables embedded within the binary, including a substantial\ncollection of statically linked known .NET DLLs.\nAmong these embedded files, certain suspicious strings stood out, hinting at the pres-\nence of an unusual and potentially malicious library. A deeper examination for content\nrelated to Tsunami indeed revealed a subset of strings associated not only with the mal-\nware itself but also with Windows components being exploited to collect additional system\ninformation. This discovery underscores the likelihood that the binary conceals malicious\npayloads or extra functionalities, leveraging its considerable size and complexity to evade\ndetection and analysis. These findings suggest that the identified suspicious DLL may\nserve as a critical component of the malware, facilitating data gathering or other mali-\ncious operations. Further investigation into this library is imperative to fully understand\nits purpose and its role within the broader malicious framework.\nNovember 26, 2024Malware Analysis Report 66\nFigure 122: Tsunami strings embedded in Runtime Broker.exe.\nBy determining the address range associated with the most noteworthy strings and lo-\ncating the specific executable segment containing this memory region, it became possible\nto isolate and extract the embedded component for standalone analysis. This meticu-\nlous extraction process revealed the core module of RuntimeBroker.exe, identified as\nRuntimeBroker.dll.\nAnalyzing RuntimeBroker.dll independently provided a clearer view of its role\nwithin the larger binary. This module appeared to function as the central orchestrator,\npotentially handling key tasks such as Command-and-Control communication, process\ninjection, and the execution of additional embedded payloads . The identification and ex-\ntraction of this core component were critical steps in unraveling the underlying structure\nand functionality of the malware, shedding light on its operational complexity and mod-\nular design.\nFigure 123: Runtime Broker.dll overview\nSince the library was written in .NET, it was possible to load it into dnSpy and\nexamine its source code directly. Remarkably, debug information was still intact, and\nthe code appeared completely unobfuscated, with human-readable functions, variables.\nThis stark contrast highlights an inconsistency in the attacker\u2019s efforts to conceal their\noperations. While the error.js file, part of the initial stage, was heavily obfuscated,\nrequiring significant effort for static analysis, the library hosting the core functionality\nNovember 26, 2024Malware Analysis Report 67\nof the first malicious executable dropped on the target system lacked any obfuscation or\nstripping.\nThis divergence suggests that, although the Threat Actor has invested substantial\nresources in constructing a resilient, distributed, and flexible malicious architecture, their\nefforts to obscure their operations diminished in later stages of the infection chain. This\ncould indicate either a rushed development cycle or a deliberate decision to prioritize ob-\nfuscation in earlier stages, leaving subsequent stages exposed. Unfortunately, these clues\nalone are insufficient to definitively determine whether this lapse was due to oversight\nor a calculated choice. Nonetheless, it underscores a critical aspect of the operation,\nrevealing potential weaknesses in their approach to maintaining stealth and obfuscation\nconsistency throughout the chain.\nFigure 124: Runtime Broker.dll reversed content\nThe Main method initializes the program by invoking Meta.Init, setting its usage\ntype to TsunamiInstaller with a specified version, i.e. 1.0.0, before invoking the Start()\nmethod. The inclusion of an infinite loop at the end ensures that the program remains\nactive, executing indefinitely and ready to retry failed operations as needed.\nThe workflow begins with the Start() method, which initiates its operations by dis-\nabling key Windows security features through a call toProgram.DisableWindowsSecu\nrity(). This step is likely aimed at neutralizing Windows Defender and Firewall protec-\ntions, creating an environment where the malware can operate without interference from\nbuilt-in security mechanisms. Following this, the program installs and starts the Tor\nproxy using TorProxy.Install() and TorProxy.Start(), setting up an anonymized\ncommunication channel that obfuscates its connections to the Command-and-Control\nserver.\nThe program places a high priority on ensuring that its malicious payload is cur-\nrent and operational. It accomplishes this by repeatedly checking for updates with Pro-\ngram.CheckForUpdates(). If updates are not available or the check fails, the program\nattempts to execute theTSUNAMI CLIENT using Program.ExecuteTsunamiCli\nent(). This mechanism ensures that the payload remains functional and capable of\nNovember 26, 2024Malware Analysis Report 68\nadapting to the latest malicious features or patches. In the event that the client is not\nalready running, the program logs its attempts to execute it.\nEstablishing a connection with the Command-and-Control server is another critical\naspect of the workflow. The program uses the Tor proxy for this purpose, retrying every\nten minutes if initial attempts fail. This persistence underscores the malware\u2019s resilience\nin maintaining communication with its operators. Once connected, it attempts to trans-\nmit telemetry data using TelemetryUploader.SendApplicationLogs(), which likely\nincludes runtime logs and system information. This data is valuable for profiling the com-\npromised environment, assessing the malware\u2019s deployment, or monitoring its operational\nstate.\nThe program also incorporates a controlled shutdown mechanism. After completing its\ntasks, such as verifying updates and transmitting telemetry, it logs a message indicating\nreadiness to terminate and exits using Environment.Exit(0). This behavior suggests\na level of sophistication in managing its lifecycle, ensuring it avoids unnecessary detection\nor conflicts with subsequent stages of its operation. The structured flow of actions, from\ndisabling security to transmitting telemetry, demonstrates a calculated approach designed\nto maximize the malware\u2019s impact while maintaining stealth.\nFigure 125: Runtime Broker.dll Main method.\nAt this point, each implemented class and its respective functionalities will be thor-\noughly examined, following a cascading order from the first to the last as they appear\nin the execution flow of the Main method. This approach ensures a structured analy-\nsis, beginning with the foundational initialization and setup processes, and progressing\nthrough the subsequent operations, dependencies, and interactions. By dissecting the\nclasses in the order they are invoked, it becomes possible to trace the logic, dependencies,\nand intent of the program, providing a comprehensive understanding of its architecture\nand behavior.\nThe Meta class is a static utility designed to manage metadata for the application,\nproviding essential details such as the application\u2019s usage type, version, session ID, and\nserver URL . The Init() method initializes these values, setting up the necessary en-\nvironment for the application to operate. It assigns the UsageType and AppVersion\nbased on the parameters passed during initialization. The AppSessionID is dynam-\nNovember 26, 2024Malware Analysis Report 69\nically generated as a unique identifier for each session using the Guid.NewGuid()\nmethod, ensuring distinct identification for every instance. Additionally, the server\nURL is hardcoded to point to a .onion address, which indicates the use of the Tor\nnetwork for communication, reinforcing the application\u2019s emphasis on anonymized oper-\nations (hxxp[:]//n34kr3z26f3jzp4ckmwuv5ipqyatumdxhgjgsmucc65jac56khdy5zqd[.]onion).\nAccessors such as GetUsageType(), GetAppVersion(), GetAppSessionID(),\nand GetServerURL() provide controlled retrieval of these initialized values. These\nmethods enable other components of the application to query the metadata without\ndirectly modifying it, ensuring data consistency and encapsulation. The class uses private\nstatic fields to store these values, maintaining a centralized configuration structure that\nsupports the application\u2019s runtime needs.\nThe design of the Meta class reflects its critical role in orchestrating the application\u2019s\nconfiguration. By combining dynamic elements like the session ID with predefined set-\ntings such as the server URL, the class facilitates flexible yet consistent behavior across\ndifferent stages of the application. The inclusion of a .onion URL further aligns the\nclass with the application\u2019s broader strategy of leveraging Tor for secure and anonymized\ncommunication.\nFigure 126: Overview of Meta class\nNovember 26, 2024Malware Analysis Report 70\nThe DisableWindowsSecurity() method is designed to neutralize Windows secu-\nrity features by disabling both Windows Defender and Windows Firewall through calls\nto the AntiDefender class. The method begins by checking for the existence of an Anti\nMalware flag using the AntiDefender.FlagExists() method. This flag acts as an in-\ndicator that the disabling operations have already been executed in a previous instance,\nallowing the program to adjust its behavior accordingly.\nIf the flag exists, the program logs the detection and pauses execution for one minute,\nindicating a shorter delay when security features are presumed to have already been\naddressed. If the flag does not exist, the program proceeds to disable Windows Defender\nand Windows Firewall , as implemented in the respective methods of the AntiDefender\nclass. Following this, it logs the absence of the flag and introduces a longer delay of five\nminutes before continuing execution.\nThe use of conditional delays based on theflag\u2019s presence serves to reduce unnecessary\nre-execution of security-disabling routines while providing a persistent mechanism to dis-\nrupt or evade host protections . By incorporating these actions early in the workflow, the\nprogram ensures that security defenses are neutralized, enabling subsequent malicious op-\nerations to proceed unimpeded. The method\u2019s detailed logging further demonstrates an\nemphasis on tracking the program\u2019s progression, which aids in monitoring and debugging\nwithin the malware framework.\nFigure 127: Overview of the DisableWindowsSecurity() method\nThe AntiDefender class represents a set of functions aimed at disabling key Windows\nsecurity features, specifically Windows Defender and Windows Firewall . The methods\noperate by adding exceptions to these defenses for specific applications, enabling the\nmalware or potentially unwanted software tobypass detection and restriction mechanisms.\nThe DisableWindowsDefender() method is designed to add exclusions to Win-\ndows Defender for a predefined list of applications, ensuring that these files are ignored by\nthe antivirus. It retrieves the paths of these applications through the GetApplication-\nList() method and iterates over them, invoking the Shell.AddWindowsDefenderEx\nception() function for each entry. This action allows the specified files to evadereal-time\nscanning, reducing the likelihood of detection. Logging is incorporated to document the\nNovember 26, 2024Malware Analysis Report 71\nprocess, recording successful additions of exceptions.\nThe DisableWindowsFirewall() method performs a similar task but targets the\nWindows Firewall . It first checks whether a flag exists, indicating that the operation\nhas already been performed. If the flag is absent, it iterates over the same application\nlist, invoking Shell.AddWindowsFirewallException() for each entry. By adding\nfirewall exceptions, the method ensures that these applications can communicate over the\nnetwork without restrictions. Once the exceptions are added, it creates the flag file to\navoid re-executing the process in subsequent runs.\nThe CreateFlag() method generates a file named TsuAmFlag.txt in the system\u2019s\ntemporary directory. This file serves as an indicator that the firewall exception process\nhas already been completed. The method incorporates exception handling to ensure\nstability and logs the success or failure of the operation. The FlagExists() method\nchecks for the presence of thisflag file, returning a boolean value that determines whether\nthe DisableWindowsFirewall() method should proceed.\nThe GetApplicationList() method defines a hardcoded list of paths to applica-\ntions that require exceptions in both Windows Defender and the Firewall. These paths\ninclude various directories, such as temporary locations, application folders, and known\nWindows directories, where components like Runtime Broker.exe, System Runtime\nMonitor.exe, and msedge.exe are stored. By using the KnownFolder class to retrieve\nspecific system paths, the method adapts to the target system\u2019s environment dynamically.\nFigure 128: Overview of the AntiDefender class\nUpon further analysis of the AntiDefender class, it becomes evident that it contains\na hardcoded list of file paths that are subjected to the whitelisting process. The paths\nin question include critical system directories and filenames that mimic legitimate appli-\ncations, such as Runtime Broker.exe, System Runtime Monitor.exe , and other\nexecutables placed in standard or temporary directories.\nThis deliberate selection of paths indicates an effort to blend malicious components\nwith legitimate system files, reducing the likelihood of detection. By targeting common\nsystem directories such as AppData, WindowsApps, and the temporary folder, the mal-\nware leverages locations that are often overlooked or trusted by security mechanisms.\nNovember 26, 2024Malware Analysis Report 72\nThis whitelisting tactic ensures that key malware components can persist and execute\ntheir payloads without triggering alarms, further emphasizing the Threat Actor\u2019s focus\non stealth and persistence.\nA more detailed and comprehensive list of the paths corresponding to these folder\nidentifiers will be presented in the subsequent dissection during the dynamic analysis\nphase. This approach will enable the retrieval of runtime-resolved paths by observing the\nmalware\u2019s behavior in a controlled environment, ensuring a thorough understanding of\nhow these identifiers are translated into actual system directories.\nFigure 129: Hardcoded paths of additional payloads undergoing whitelisting process.\nThe Shell class provides utility functions to interact with the Windows system through\nPowerShell commands. It includes methods to execute arbitrary commands, add excep-\ntions to Windows Defender, and create firewall rules , primarily aiming to configure the\nsystem in favor of the malware\u2019s operations.\nThe ExecutePowerShellCommand() method serves as a generic utility to execute\nPowerShell commands . It creates a new Process instance with powershell.exe as the\nexecutable and the specified command as its argument. The process is configured to run\nwithout displaying a window ( CreateNoWindow = true ), enabling it to execute silently.\nThis generic command execution capability underpins the other methods in the class.\nThe AddWindowsDefenderException() method uses a PowerShell command to\nadd a specified path to Windows Defender Exclusion List , preventing the AV from scan-\nning or monitoring files in that location. The command is executed using powershell.exe\nwith elevated privileges (Verb = \u201drunas\u201d), ensuring that administrative access is granted\nfor modifying Defender settings. This functionality is critical for the malware to bypass\ndetection and ensure the persistence of its components.\nSimilarly, the AddWindowsFirewallException() method constructs a PowerShell\nNovember 26, 2024Malware Analysis Report 73\ncommand to create a firewall rule allowing inbound traffic for a specified program. The\nrule is labeled with a generic name, such asMicrosoft Edge WebEngine, to avoid suspicion.\nLike the Defender exclusion method, this command also runs with elevated privileges and\nsuppresses any visible command window. The use of netsh commands within PowerShell\nhighlights an effective approach to manipulate firewall rules programmatically.\nThis class demonstrates a deliberate focus on leveraging PowerShell for system mod-\nifications, a common tactic in malware to evade detection and achieve operational goals.\nBy embedding commands directly into the malware, the attackers reduce the reliance on\nexternal scripts, ensuring stealth and flexibility. The silent execution and elevation of\nprivileges further underline the emphasis on maintaining a low profile while performing\ncritical system changes.\nFigure 130: Overview of the Shell class\nNovember 26, 2024Malware Analysis Report 74\nThe TorProxy class provides a comprehensive implementation for managing a Tor\nproxy, encompassing its installation, execution, and usage for network operations such as\nHTTP requests and file downloads. The ExecutablePath property specifies the location\nof the Tor proxy executable as Runtime Broker.exe within the system\u2019s temporary\ndirectory. This choice of name and location raises suspicions of an attempt to masquerade\nas a legitimate Windows process, potentially aiding in evasion from detection mechanisms.\nThe Install() method is responsible for deploying the Tor proxy executable. It first\nchecks if the proxy is already running, avoiding redundant installations. If the executable\nis absent, it retrieves the Tor binary data from a resource loader and writes it to the\nspecified location. The method is equipped with detailed logging to capture success or\nfailure, reflecting the developer\u2019s attention to error handling and debugging capabilities.\nThe Start() method initiates the proxy process, configured to use a standard SOCKS\nport (9050) and a temporary directory for its data storage. If an instance of the proxy\nis already active, the method attempts to terminate it before restarting, ensuring no\nconflicts arise from multiple running instances. Again, logging is extensively used to\nprovide insights into process management.\nThe Shutdown() method complements this functionality by stopping the Tor proxy.\nIt performs a check to confirm the process is running and, if so, attempts to terminate it.\nDetailed logs document whether the shutdown succeeds or fails, providing transparency\nand aiding troubleshooting.\nNetwork communication is facilitated through the SendRequest() method, which\nallows HTTP requests to be routed through the Tor proxy. This asynchronous function\nsupports both GET and POST requests, with headers and payloads designed for JSON -\nbased data exchanges. By incorporating a custom SOCKS port handler, the method\nensures all traffic is anonymized. Comprehensive error handling and logging provide a\ndetailed account of the request outcomes, including response status codes and content\nsizes. Similarly, the DownloadFile() method enables file retrieval via the proxy. Using\nasynchronous streaming, it efficiently downloads files from specified URLs to designated\nfile paths. Its reliance on the Tor network for anonymizing traffic and the inclusion of\nrobust error handling underscore its capability for secure and reliable file transfers.\nThe overall design of the TorProxy class reflects a technically proficient implemen-\ntation, leveraging asynchronous programming to ensure efficient and non-blocking oper-\nations. However, the choice to disguise the executable as Runtime Broker.exe and\ndeploy it in a temporary directory suggests potential misuse for malicious purposes. These\ncharacteristics, combined with the use of Tor for anonymizing traffic, align with tactics\ncommonly seen in malware aimed at concealing Command-and-Control communications,\ndata exfiltration, or secondary payload delivery.\nNovember 26, 2024Malware Analysis Report 75\nFigure 131: Snippet of TorProxy class\nThe CheckForUpdates() method is a robust implementation designed to manage\nupdates for the Tsunami Client application. It combines multiple functionalities to ensure\nthe client executable is current, secure, and operational. The process begins by verifying\nthe existence of the designated directory for the Tsunami Client . If the directory is\nmissing, it is created, and the operation is logged, ensuring the required environment is\nproperly configured.\nThe method then requests the hash of the latest client version from the server via\nan HTTP GET request , routed through a Tor proxy for anonymized communication.\nThe response from the server contains the hash and a success status. If the request is\nsuccessful, the received hash is compared against the one of the currently installed client\nexecutable, computed using the SHA-256 algorithm. This step ensures the integrity of\nthe existing file and determines whether an update is required. If the executable is missing\nor the hashes do not match, the method identifies the need for an update.\nBefore proceeding, the method checks whether the current version of the client is\nrunning. If it is, the method attempts to terminate the process to ensure a clean update\nenvironment. If termination fails, an error is logged, and the update process is aborted.\nOnce the update is confirmed, the method downloads the latest compressed version of the\nclient executable from the server using theTor proxy. The file is temporarily stored in the\nsystem\u2019s temporary directory, and its contents are read, reversed, and decompressed using\na GZIP library. The decompressed data is then written to the client executable\u2019s path,\nreplacing the old version with the updated one. Finally, the temporary file is deleted,\nwith any failure to delete it logged as a warning.\nThroughout the process, the method incorporates comprehensive error handling and\nlogging. Each step, whether successful or failed, is documented to ensure transparency\nand facilitate debugging. For instance, it logs successes for tasks such as fetching the hash\nand downloading the compressed file, and records warnings or errors for issues like hash\nNovember 26, 2024Malware Analysis Report 76\nmismatches, decompression failures, or file system errors. The use of SHA-256 hashing\nunderscores the method\u2019s focus on verifying update integrity, preventing corrupted or\nmalicious files from being applied.\nThe reliance on the Tor proxy for communication adds a layer of obfuscation, making\nit difficult to trace server interactions. The ability to dynamically download and apply\nupdates allows for the deployment of new payloads or modifications, enhancing the adapt-\nability and persistence of the system. The integration of GZIP compression minimizes\nthe size of update payloads, optimizing bandwidth usage while maintaining functionality\nthrough proper decompression. The CheckForUpdates() method exemplifies careful\nand efficient design, incorporating advanced techniques for process management, error\nhandling, and file integrity verification.\nFigure 132: Overview of CheckForUpdate()method\nThe susscessive analysis of the ResourceManager component reveals the presence of\ntwo notable embedded resources: a tor.exe file and a tsunami payload.exe. While the\nfirst file, tor.exe, is actively extracted and utilized by the malware during execution, the\nlatter appears to be embedded without any direct reference to its extraction or deployment\nwithin the program\u2019s logic. This discrepancy raises questions about the attacker\u2019s intent\nand the role of the unused tsunami payload.exe.\nThe active usage of tor.exe aligns with the malware\u2019s reliance on the Tor network for\nanonymized communication. Conversely, the embedded tsunami payload.exe stands\nout as an anomaly. Despite being included within the resource bundle, no references\nto its extraction or execution were identified in the program\u2019s workflow. This omission\nis particularly intriguing given the malware\u2019s reliance on hash-based comparison for de-\nploying the most recent version of the Tsunami Client. This update mechanism ensures\nthat only the latest and potentially most secure version of the tool is deployed during the\nattack. The presence of this forgotten executable, a seemingly outdated or redundant\npayload, raises questions about its intended purpose.\nNovember 26, 2024Malware Analysis Report 77\nOne plausible explanation is that tsunami payload.exe could have been a place-\nholder or backup resource intended for testing or as a contingency in case of a failure in\nthe update process. Alternatively, its inclusion may have been unintentional, resulting\nfrom oversight or rushed development during the malware\u2019s construction. The lack of\nreferences to its deployment leaves its intended role ambiguous and opens the possibility\nthat it was meant to serve in a future iteration of the malware but was left dormant in\nthis version.\nNevertheless, its presence allows for standalone analysis. This dormant payload pro-\nvides an additional opportunity to uncover details about the attacker\u2019s broader toolkit\nor objectives. Its embedded status, while curious, does not detract from the malware\u2019s\noperational efficiency but instead offers valuable insights into the development practices\nand potential missteps of the threat actor.\nFigure 133: tsunami payload.exe availability with no reference to its deployment.\nThe TorServer class provides functionality for establishing and managing communi-\ncation with a remote server over the Tor network. It facilitates tasks such as session\ninitialization, environment information submission , and data transmission. The imple-\nmentation exhibits a deliberate focus on maintaining persistent and anonymized commu-\nnication, leveraging the Tor proxy for network routing.\nThe Connect() method serves as the entry point for establishing communication with\nthe remote server. It sequentially calls the SendInit() and SendEnvironmentInfo()\nmethods to initialize the session and transmit the host system\u2019s environment details. The\nmethod ensures that both steps are successful, logging any failures and terminating the\nconnection attempt if errors occur. Upon successful completion, a session key is obtained,\nwhich is critical for subsequent interactions.\nThe SendInit() method initializes the connection by sending an empty payload ({})\nto the server\u2019s API initialization endpoint. The server responds with a session key, which\nis parsed and stored for later use. This acts as anauthentication token, binding subsequent\nrequests to a specific session. The method logs the outcome of the initialization, ensuring\ntransparency in the connection process.\nThe SendEnvironmentInfo() method collects detailed system information, includ-\ning application version, system specifications (e.g., processor, RAM, display size, operat-\ning system), and geographic location(e.g., city and country). This information is compiled\ninto a dictionary and transmitted to the server via the SendData() method. The lat-\nter ensures that critical system attributes are accurately collected and sent, potentially\naiding in profiling the victim\u2019s environment for tailored malicious activities.\nNovember 26, 2024Malware Analysis Report 78\nThe SendData() method is a generalized function for transmitting data to the server.\nIt serializes the data into aJSON object, incorporating the session key for authentication.\nThe payload is then sent via the TorProxy.SendRequest() method, which routes\nthe request through the Tor network. Analyzed method provides detailed logging for\nsuccessful transmissions, including the size of the data sent and the response received.\nThis class also defines several constant URLs for various API endpoints, including\nthose for telemetry, browser passwords, session data, and other assets. These endpoints\nreflect a comprehensive framework for data exfiltration and telemetry reporting, likely\nintended for managing stolen information and maintaining control over the infected sys-\ntem.\nA noteworthy aspect of this class is its use of Tor for anonymizing communication.\nBy routing all requests through the Tor network, it obscures the server\u2019s location and the\nnature of the communication, complicating detection and attribution efforts. The imple-\nmentation of detailed logging and error handling ensures that failures are documented,\nfacilitating debugging and operational resilience.\nThe TorServer class demonstrates a well-structured approach to managing communi-\ncation within a malicious framework. Its integration of session management, environment\nprofiling, and anonymized data transmission reflects a high degree of sophistication. This\nclass is likely a critical component of a broader malware architecture designed for data\nexfiltration, telemetry, and maintaining remote control over compromised systems.\nFigure 134: JSON -based template with acquired information to exfiltrate.\nNovember 26, 2024Malware Analysis Report 79\nFigure 135: API endpoint paths for each single activity the malware takes care of.\n\u2022 hxxp[:]//n34kr3z26f3jzp4ckmwuv5ipqyatumdxhgjgsmucc65jac56khdy5zqd[.]onion/\nassets/v2/dotnet6-installer-ur\n\u2022 hxxp[:]//n34kr3z26f3jzp4ckmwuv5ipqyatumdxhgjgsmucc65jac56khdy5zqd[.]onion/\napi/v1/discord-accounts\n\u2022 hxxp[:]//n34kr3z26f3jzp4ckmwuv5ipqyatumdxhgjgsmucc65jac56khdy5zqd[.]onion/\napi/v1/browser-passwords\n\u2022 hxxp[:]//n34kr3z26f3jzp4ckmwuv5ipqyatumdxhgjgsmucc65jac56khdy5zqd[.]onion/\napi/v1/init\n\u2022 hxxp[:]//n34kr3z26f3jzp4ckmwuv5ipqyatumdxhgjgsmucc65jac56khdy5zqd[.]onion/\nassets/v2/tsunami-client/file\n\u2022 hxxp[:]//n34kr3z26f3jzp4ckmwuv5ipqyatumdxhgjgsmucc65jac56khdy5zqd[.]onion/\napi/v1/browser-sessions\n\u2022 hxxp[:]//n34kr3z26f3jzp4ckmwuv5ipqyatumdxhgjgsmucc65jac56khdy5zqd[.]onion/\napi/v1/telemetry\n\u2022 hxxp[:]//n34kr3z26f3jzp4ckmwuv5ipqyatumdxhgjgsmucc65jac56khdy5zqd[.]onion/\nassets/v2/tsunami-client/hash\n\u2022 hxxp[:]//n34kr3z26f3jzp4ckmwuv5ipqyatumdxhgjgsmucc65jac56khdy5zqd[.]onion/\napi/v1/environment-info\nAs observed in previous instances, nearly all components within the identified list,\nexcept for the Discord and Browser related paths, are actively utilized in at least one of\nthe malicious functions implemented in the analyzed DLL. This notable exception raises\nsimilar questions to those posed earlier, as it may represent a remnant of a prior iteration\nof the module, initially developed for a different purpose and subsequently repurposed or\nadapted to fit its current scope. It might be a leftover artifact from an earlier stage of\nNovember 26, 2024Malware Analysis Report 80\ndevelopment, where the module was designed with broader or alternative functionalities.\nThis could imply that the malware\u2019s architecture has evolved, discarding certain features\nwhile adapting others to serve the campaign\u2019s objectives. Alternatively, it might offer a\nglimpse into future intentions, signaling the attacker\u2019s plans to incorporate Discord and\nBrowser focused features into subsequent versions of the module.\nSuch patterns reflect the iterative nature of the Threat Actor\u2019s development process,\nwhere modularity and flexibility play key roles. The inclusion of potentially deprecated\nor yet-to-be-deployed components demonstrates the evolving scope of their malicious\ntoolkit. While it is possible that the Discord and Browser related paths was left in\nunintentionally due to rushed development, it cannot be dismissed as a mere oversight.\nInstead, it provides valuable insight into the attacker\u2019s design philosophy and the lifecycle\nof their malicious tools.\nThis dormant paths, much like other unexplored functionalities or components, high-\nlights the importance of monitoring the malware\u2019s development over time. Analyzing\nsuch artifacts can reveal potential shifts in the attacker\u2019s focus, providing early warning\nof new techniques or targets that may emerge in future campaigns.\nFigure 136: Discord and Browser paths are not read by any function.\nBy proceeding with the code analysis, it is possible to focus on theComputerInfo class.\nThe latter is designed to gather detailed system information, leveraging both managed\n.NET functionality and native Windows API s. It provides methods to extract data\nabout hardware, operating system, and display settings, as well as geolocation and public\nIP address information. The methods combine command-line utilities, registry queries,\nand API calls to compile a comprehensive profile of the host system.\nThe class includes methods such as GetProcessorName(), GetProcessorCore-\nCount(), and GetGraphicsCardName() to retrieve details about the system\u2019s CPU\nand GPU. These methods execute Windows Management Instrumentation Command-\nline (WMIC ) queries through the command prompt and parse the output. For instance,\nNovember 26, 2024Malware Analysis Report 81\nGetProcessorName() retrieves the CPU name by running the WMIC command for\nprocessor details, extracting and formatting the output string. Similarly, GetProces-\nsorCoreCount() uses WMIC to determine the number of CPU cores, and GetGraph-\nicsCardName() queries the GPU name.\nTo determine if a dedicated GPU exists, the DedicatedGraphicsCardExists()\nmethod uses WMIC to fetch video controller descriptions and searches for keywords\nlike Nvidia or Radeon in the output. This method provides insight into the graphical\ncapabilities of the system, which can be useful for tailoring payloads or assessing the\ntarget\u2019s computational power.\nThe class includes GetTotalMemoryGB(), which retrieves the system\u2019s physi-\ncal memory using the GetPhysicallyInstalledSystemMemory() function from ker-\nnel32.dll. This API call ensures accurate memory reporting in GB, independent of the\nsystem\u2019s OS or configuration. Display size is obtained through the EnumDisplaySet-\ntings() function from user32.dll, which retrieves the screen resolution for the primary\nmonitor.\nOperating system details are retrieved via methods such as GetOperatingSystem-\nName() and GetOperatingSystemID(). The former uses WMIC to fetch the OS\ncaption and formats it as a user-friendly string. The latter queries the Windows registry\nfor the product ID using predefined paths, demonstrating its ability to gather licensing\ninformation or unique identifiers tied to the operating system.\nThe geolocation capabilities of the class are implemented in GetLocation(), which\ncombines public IP retrieval with location services such as ipinfo.io. The method sends\nHTTP requests to these API s, fetching data about the system\u2019s public IP, country, and\ncity. The GetPublicIP() method offers similar functionality, querying multiple online\nservices for the public IP address .\nInternally, the class uses helper methods to parse and extract relevant information\nfrom the outputs of WMIC commands , registry queries, and API responses. The use\nof both .NET libraries and unmanaged code illustrates a hybrid approach, enabling the\nclass to access a wide range of system information.\nThis class serves as a robust tool for profiling the host system, with applications\nranging from hardware and software inventory to geolocation and network assessment.\nWhile such capabilities can be legitimate in administrative or diagnostic contexts, in this\ncontext they are used to fingerprint the targeted machine and possibly to tailor an evental\ndeploy of the previously referenced XMRig Miner .\nNovember 26, 2024Malware Analysis Report 82\nFigure 137: Snippet of the ComputerInfo class\nThe ExecuteTsunamiClient() method manages the execution of the Tsunami\nClient, with a focus on ensuring the necessary runtime environment, such as .NET 6 ,\nis installed and operational. It begins by verifying if the .NET 6 framework is present\non the system. If not, it attempts to retrieve the installer URL from the server via a Tor\nproxy, provided the server is online. This step underscores its reliance on dynamic depen-\ndencies, highlighting its adaptability but also its dependency on external infrastructure.\nIf the server is offline or the installer URL cannot be retrieved, the method logs an error\nand aborts the process, reflecting the criticality of .NET 6 to the client\u2019s functionality.\nOnce the URL is obtained, the method invokes the DotNet6.Install() function to\ndownload and install the framework. Any failure during this installation process is logged,\nemphasizing robust error reporting.\nAfter ensuring the runtime environment is ready, the method attempts to launch the\nNovember 26, 2024Malware Analysis Report 83\nTsunami Client executable. If successful, it logs the initiation of the client and sets the\nClientRunning flag to true, indicating operational status. Conversely, a failure to start\nthe client is logged as an error, ensuring transparency in operation status.\nThis method demonstrates a structured approach to dependency management and\nexecution control. The integration of dynamic installation for .NET 6 enables the mal-\nware to adapt to a variety of environments, ensuring compatibility regardless of the target\nsystem\u2019s initial configuration. Its reliance on the Tor proxy for obtaining dependencies\nhighlights an emphasis on obfuscating communication, aligning with tactics commonly\nemployed by malicious software.\nThe presence of robust error handling and detailed logging provides insights into its\noperational logic but also reveals its potential misuse. By ensuring dependencies are\ndynamically resolved and operational status is closely monitored, the method reflects a\ndesign aimed at maintaining resilience and adaptability, potentially in support of a larger\nmalicious framework.\nFigure 138: Overview of the ExecuteTsunamiClient()\nThe TelemetryUploader class appears to be designed for aggregating and transmitting\napplication logs to a remote server under the guise of legitimate telemetry functionality.\nThe SendApplicationLogs()method processes runtime logs by categorizing them into\nSuccess, Info, Warning, and Error types, creating both a summary and a detailed report\nof the application\u2019s activity. These logs are dynamically categorized based on the ap-\nplication\u2019s role (e.g., ClientAppLogs or InstallerAppLogs) to ensure contextual relevance,\nfurther suggesting a tailored approach to data collection.\nA telemetry object encapsulates the session ID, log categories, and detailed runtime\ndata, which is transmitted to a remote server via the TorServer.SendData() method.\nThe robust design, detailed logging, and anonymized communication suggest that its\nlikely intent is to gather intelligence from compromised hosts, either for system profiling,\noperational oversight, or further exploitation. The sophistication of this class underlines\nthe need for thorough investigation and monitoring to mitigate its potential impacts.\nNovember 26, 2024Malware Analysis Report 84\nFigure 139: Overview of the TelemetryUploader class\nThe UserInteractions class is a utility designed to monitor and analyze user activity\nand system interaction states. It relies on Windows API calls to retrieve idle time, detect\nfullscreen applications, and assess the user\u2019s last input . Despite being implemented in\nthe source code, this class remains unused within the provided execution flow, raising\nquestions about its intended purpose and whether it was meant for testing, debugging,\nor future expansion.\nThe class includes methods such as GetIdleTime() and GetLastInputTime(),\nwhich determine the duration since the last user interaction. These methods leverage\nthe GetLastInputInfo() function from User32.dll to fetch the timestamp of the most\nrecent input. GetIdleTime() calculates the elapsed time in milliseconds, while Get-\nLastInputTime() provides this information in seconds, incorporating error handling to\nmanage API call failures.\nThe FullScreenEnabled() method evaluates whether the currently active applica-\ntion is running in fullscreen mode. It retrieves the dimensions of the primary display\nusing the ComputerInfo.GetDisplaySize() method and compares them with the\ndimensions of the foreground window, obtained via GetWindowRect() and GetFore-\ngroundWindow() from User32.dll. By constructing and comparing rectangles, this\nmethod determines if the foreground window occupies the entire screen.\nThe class relies on two internal structs, RECT and LASTINPUTINFO, which act as\ndata containers for API calls. RECT stores the dimensions of a window, while LASTIN-\nPUTINFO holds details about the last user input. These structures facilitate seamless\nintegration with the Windows API, enabling the class\u2019s functionality.\nDespite its sophisticated design, the absence of this class from the operational code-\nbase suggests it was either deprecated, unfinished, or reserved for future use. The presence\nof such a class indicates an interest in user activity profiling, potentially to tailor malicious\nactions based on the victim\u2019s behavior. For example, detecting fullscreen mode might\nNovember 26, 2024Malware Analysis Report 85\nsignal a gaming or media application, potentially delaying certain malware activities to\navoid detection.\nThe unused state of the UserInteractions class could also hint at incomplete develop-\nment or a deliberate exclusion from the main code to reduce detection risk. Its capabilities\nalign with broader reconnaissance and behavioral monitoring goals, but without active\ninvocation, it remains an artifact that offers insights into the malware\u2019s potential design\nobjectives and development process.\nFigure 140: Overview of the unused UsersInteractions class\nThe CaesersCipher class implements a classical Caesar cipher encryption and decryp-\ntion algorithm, providing basic functionality for shifting letters in a string by a specified\nnumber of steps. Despite its simplicity and potential utility, this class remains unused\nwithin the provided codebase, suggesting it may have been intended for testing, debug-\nging, or as part of a feature that was ultimately removed or deferred.\nThe Encrypt() method transforms a given string by shifting each alphabetical char-\nacter forward in the alphabet by the specified number of steps ( step). It preserves the\ncase of the letters, ensuring uppercase and lowercase characters are shifted within their\nrespective ranges, and leaves non-alphabetic characters unchanged. For example, the\nletter \u2019A\u2019 shifted by one step would become \u2019B\u2019, while \u2019z\u2019 shifted by one step would wrap\naround to \u2019a\u2019.\nSimilarly, the Decrypt() method reverses the transformation by shifting charac-\nters backward by the specified number of steps, also preserving case and ignoring non-\nalphabetic characters. The implementation uses modular arithmetic to handle the wrap-\nping of letters at the boundaries of the alphabet.\nThe unused state of this class raises questions about its intended role within the\nmalware. Its implementation suggests it might have been designed for lightweight obfus-\ncation of strings or data, such as encoding configuration settings, URLs, or commands to\nevade simple detection mechanisms. However, the simplicity of the Caesar cipher makes\nit unsuitable for robust cryptographic purposes, as it is easily broken through frequency\nanalysis or brute force due to the limited keyspace.\nNovember 26, 2024Malware Analysis Report 86\nThe inclusion of the CaesersCipher class, despite its non-use, provides insight into\nthe potential development process of the malware. It could indicate that the developers\nexperimented with or considered alternative encryption mechanisms before settling on\nmore complex or secure methods elsewhere in the code. Alternatively, it might reflect a\nplaceholder or backup implementation, highlighting the iterative nature of the malware\u2019s\ndevelopment lifecycle.\nFigure 141: Overview of the unused CaesarsCipher class\nDynamic Analysis\nThe execution of Runtime Broker.exe shows, as first, the executable being accessed from\nthe %APPDATA%\\Roaming\\Microsoft\n\\Windows directory. This unconventional execution path immediately raises suspicions,\nas it deviates from standard system directory conventions, as previously mentioned.\nSubsequent interactions with system libraries like KernelBase.dll and kernel32.dll sug-\ngest that the process is preparing its runtime environment, loading functions critical for\nsystem-level interactions. These methods likely include capabilities for memory manipu-\nlation, process injection, or thread management, which are common in malicious processes\naiming to extend their reach within the system.\nFigure 142: Runtime Broker.exe loading system libraries.\nNovember 26, 2024Malware Analysis Report 87\nThere are also Registry operations appearing particularly noteworthy. Analyzing reg-\nistry operations reveals access to HKLM\\System\\CurrentControlSet\\Services\\bam\\State\n\\UserSettings, a registry key that tracks user-level application activity. This query sug-\ngests reconnaissance activities aimed at gathering information about system usage pat-\nterns or identifying running applications for potential injection or exploitation. What\nhas been recorded and shown below indicates that the process accessed HKLM\\System\n\\CurrentControlSet\\Control\\Session Manager , a key integral to managing system boot\nconfigurations. By querying this key, the malware likely intends to evaluate or modify\nstartup behaviors, ensuring that it executes automatically upon system reboot.\nFigure 143: Runtime Broker.exe querying interesting registry keys.\nAdditional file operations involve interactions with apphelp.dll, a library often associ-\nated with compatibility and application support in Windows. This may indicate attempts\nto exploit or modify application compatibility settings as part of its malicious strategy.\nFigure 144: Runtime Broker.exe interacting with apphelp.dll.\nThen, there are several attempts to access specific registry keys underHKLM\\Software\n\\Policies\\Microsoft\\Windows\\Display and HKLM\\SOFTWARE\\Microsoft\\Windows NT\n\\CurrentVersion. These actions frequently result in a NAME NOT FOUND detail, indi-\ncating the queried registry entries do not exist. The desired access permissions are pre-\ndominantly read-related, with some operations querying values and enumerating subkeys.\nThis phase suggests that the process is performing system reconnaissance, as previously\nidentified in the analysis of Runtime Broker.dll .\nNovember 26, 2024Malware Analysis Report 88\nFigure 145: Executable querying extensively the HKLM hive.\nLater, activities shift toward file handling and memory management. Operations like\nCreateFileMapping and FASTIO RELEASE FOR SECTION SYNCHRONIZATION ap-\npear, signaling interaction with memory-mapped files. These are common in processes\nattempting to share memory between applications or manage large datasets efficiently.\nAdditionally, thread creation events (Thread Create) indicate that new execution threads\nare being initialized, hinting at multitasking or concurrency within the process. The\ninteraction with system libraries, such as rpcss.dll, and the presence of FAST IO DIS-\nALLOWED suggest potential privilege or capability constraints imposed on the process.\nFigure 146: Executable interacting with rpcss.dll.\nThe process queries and opens multiple registry keys under paths such as HKLM\n\\Software\\Microsoft\\Windows\\CurrentVersion and HKLM\\System\\CurrentControlSet.\nThe successful results for these actions indicate that the queried keys exist, and the desired\naccess permissions, predominantly read permissions, are granted. These operations likely\naim to retrieve system or application configurations, such as file paths, environment\nsettings, or user preferences.\nNovember 26, 2024Malware Analysis Report 89\nFigure 147: Executable continues to map the HKLM hive looking for keys of interest.\nRegistry-related events dominate this range, with key activities includingRegQueryKey,\nRegOpenKey, and RegCloseKey. The keys being accessed, such as those under Con-\ntrol\\Hvsi and Nls\\Sort, suggest the process is targeting configurations related to hardware-\nassisted virtualization and system sorting behaviors, respectively. These entries might be\nleveraged for compatibility checks, feature detection, or runtime behavior adjustments.\nFigure 148: Executable interactions with Control\\Hvsi and Nls\\Sort.\nThe occasional NAME NOT FOUND details for specific queries, such as in the Reg-\nQueryValue operation under Control\\Hvsi\\IsHvsiContainer, indicate that some queried\nvalues are absent, perhaps revealing conditional checks within the process\u2019s logic. Indeed,\nthis registry key is associated withHypervisor-based Security Isolation (HVSI ) and is typ-\nically used to indicate whether a system or process is running inside an HVSI container.\nHypervisor-based Security Isolation (HVSI ) is a feature enabled by virtualization-based\nsecurity (VBS) and Hyper-V on Windows systems. It isolates critical system components\nand certain processes within containers that are protected by the hypervisor. This en-\nhances security by preventing unauthorized access and code execution, even in the event\nof a kernel compromise. This allows the subjected executable to both adapt its behavior,\nbasing on the security measures available on the system, and acquire system\u2019s security\nconfiguration to later exfiltrate to the remote Threat Actor.\nFigure 149: Runtime Broker.exe tries to identify the presence of HVSI container.\nThere is also evidence of deeper system exploration, such as the retrieval of data\nrelated to kernel32.dll. This could imply attempts to verify core system library availabil-\nNovember 26, 2024Malware Analysis Report 90\nity or extract runtime parameters that depend on the system\u2019s localization and sorting\nconfiguration.\nThe process, at this point, attempts to open or query specific files, related to Pow-\nerShell instances. Each of them posed in a different folder, and related to different\napplication (e.g. Chocolatey). Additional details are provided in the following image.\nFigure 150: Runtime Broker.exe tries to map PowerShell.exe instances.\nThere are also interactions with files related to system patching and PowerShell,\nsuch as sysmain.sdb in the C:\\Windows\\apppatch directory and powershell.exe in the\nC:\\Windows\\System32\\WindowsPowerShell\\v1.0 path. These successful interactions,\nlike FASTIO RELEASE FOR SECTION SYNCHRONIZATION and QueryStandardInfor-\nmationFile, suggest that the process is inspecting system utilities and environment details,\npossibly for compatibility checks or preparatory tasks.\nFigure 151: Additional system queries made by Runtime Broker.exe.\nAs depicted in the accompanying image, the establishment of these exclusions occurs\nin two distinct phases, executed by separate components. Initially, upon the execution of\nRuntime Broker.exe , all six new firewall rules are applied (paths correspond to the\none identified in Figure 129). Subsequently, after a delay of approximately 15 seconds,\nthese same exclusions are reapplied by a child PowerShell process, spawned by Runtime\nBroker.exe. This evidence underscores the heightened level of resilience and redundancy\nembedded by the developers across their toolset.\n\u2022 %APPDATA%\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\\System Runtime\nMonitor.exe\nNovember 26, 2024Malware Analysis Report 91\n\u2022 %APPDATA%\\Microsoft\\Windows\\Applications \\Runtime Broker.exe\n\u2022 %LOCALAPPDATA%\\Microsoft\\Windows\\Applications\\Runtime Broker.exe\n\u2022 %APPDATA%\\Microsoft\\Windows\\Dependencies\\System Runtime Monitor.exe\n\u2022 %LOCALAPPDATA%\\Microsoft\\Windows\\WindowsApps\\msedge.exe\n\u2022 %TEMP%\\Runtime Broker.exe\nAt the onset of the malware\u2019s activity, some of the most noteworthy behaviors pertain\nto the manipulation of Firewall policies and Antivirus exclusions. These actions provide\nanalysts with critical insights into the additional payloads that the Threat Actor intends\nto deploy within the target systems. One of the initial observations involves six inbound\nallow rules introduced by the Runtime Broker.exe executable within the Windows\nFirewall. These rules are deceptively labeled as Microsoft Edge WebEngine as previously\nidentified in the analysis of the Runtime Broker.dll .\nFigure 152: Windows Firewall exclusions\nA similar fail-safe rationale is evident in the implementation of AV exclusions. Prior\nto the active execution of Runtime Broker.exe, the TSUNAMI PAYLOAD script was\nresponsible for modifying Defender\u2019s policies and registering Runtime Broker.exe as\na scheduled task (Figure 144). As illustrated in the subsequent image, both the three file\npaths managed by the TSUNAMI PAYLOAD and an additional four paths introduced\nlater are excluded from Defender\u2019s scans. This layered approach ensures that, even in\nscenarios where the Python script might fail to execute its intended tasks, the executable\ncan independently enforce the exclusions. Such robust and redundant design highlights\nthe meticulous planning and sophistication employed by the malware\u2019s developers.\nFigure 153: Defender\u2019s exclusions\nFurthermore, it is also interesting how the TSUNAMI CLIENT refers to the XM-\nRig Miner path as %LOCALAPPDATA%\\Microsoft\\Windows\\Applications\\msedge.exe,\nat the same time, this path is not embedded inside the Runtime Broker.exe code,\nwhich instead whitelists %LOCALAPPDATA%\\Microsoft\\WindowsApps\\msedge.exe. It\nis not possible, as per the achieved analysis, to distinguish between the existence of two\ndifferent payloads or a change in the attacker\u2019s behavior which was not consistent between\nthese two applications.\nAfter around 34 seconds of execution, identified threat went silent for around 4 min-\nutes. This behavior is consistent within the expected malware capabilities and what\nNovember 26, 2024Malware Analysis Report 92\nidentified during the static analysis. Runtime Broker.exe slows down its execution\nto avoid being detected within Sandbox analyses, which usually employ shorter analysis\ntime frames.\nFigure 154: Malware execution stops around 17:49:50 to the restart at 17:54:20.\nOnce the malware got unfrozen, one of the first activities it carries out on the system\nis to drop tor.exe inside path %TEMP%\\Runtime Broker.exe . This executable was\nindeed previously whitelisted from Defender\u2019s scan engine and allowed to receive inbound\nconnections from Windows Firewall.\nFigure 155: Runtime Broker.exe drops an embedded malicious executable in\n%TEMP%\\Runtime Broker.exe.\nOnce deployed, this additional payload is also executed to achieve a TOR connections\ntowards remote networks.\nFigure 156: %TEMP%\\Runtime Broker.exe is executed\nFrom the initiation of the execution until it was terminated, spanning a total duration\nof eight minutes and resulting in the logging of over 241,000 events, the initial Runtime\nBroker.exe process actively transmitted data from the host\u2019s port 63300 to port 9050,\ndesignated as the TOR SocksPort . This activity, as depicted in Figure 156, confirms\nNovember 26, 2024Malware Analysis Report 93\nthat port 9050 was specifically utilized by the %TEMP%\\Runtime Broker.exe as a\nInter Process Communication (IPC ) alternative, compared to standard ones (i.e. named\npipes).\nFigure 157: Runtime Broker.exe sends acquired data to the local TOR SocksPort.\nThe behavior involving the parent process sending data through a child process that\nruns Tor represents an interesting and deliberate design choice to use Tor as a local\nproxy to exchange data between processes. This setup offers various technical gains as\nwell as drawbacks when compared to traditional Inter-Process Communication (IPC )\nmechanisms, such as named pipes, shared memory, or sockets.\nThe use of Tor as a means to handle local Inter-Process Communication (IPC )\npresents significant advantages. The primary gain lies in the inherent obfuscation and\nanonymization that the Tor network provides. By routing data between processes over\nTor, the malware developer ensures that even local communication appears as part of\na legitimate Tor network flow. This not only obfuscates the purpose of the commu-\nnication but also effectively anonymizes its endpoints, making network-based detection\ndifficult. This is particularly effective because network analysis often focuses on identify-\ning unusual connections to external addresses, while Tor is widely recognized for privacy\npurposes, which may lead security tools to treat it with less scrutiny. Furthermore, by\ncommunicating over a local SOCKS proxy on port 9050, the malware can easily convert\ninternal messages into externally routable data, offering a seamless transition between\nlocal activity and external control or exfiltration.\nThis separation between the parent process (responsible for payload execution or in-\nformation gathering) and the child process running Tor as a proxy also creates a modular\napproach. In software design, modularity provides flexibility and scalability, which allows\neach component to be independently modified or updated without affecting the overall\nfunctionality. In this scenario, the Tor proxy module handles network anonymity, while\nthe parent process focuses on the core malicious operations. This architecture also de-\ncouples the anonymization and routing logic from the malicious payload itself, allowing\nfor greater flexibility and code reuse. The Tor process can be used by multiple malicious\nmodules, potentially even in parallel, to handle diverse communication needs, which in-\ncreases the versatility of the malware.\nAnother important advantage is the simplicity of implementation for cross-platform\ncompatibility. Tor-based local communication relies on network sockets, which are in-\nNovember 26, 2024Malware Analysis Report 94\nherently cross-platform. This means the malware developer can easily adapt the code\nto work on different operating systems (e.g., Windows, Linux, macOS) with minimal\nchanges. This contrasts sharply with named pipes, which are Windows-specific and re-\nquire entirely different implementations if the malware is to function on a non-Windows\nenvironment. By using Tor and network sockets, the malware becomes highly adaptable,\nreducing development overhead for maintaining multiple versions of the same malware\nfor different operating systems.\nHowever, despite these advantages, using Tor as a local proxy for IPC also comes\nwith some drawbacks that must be considered. One of the fundamental drawbacks is the\ninherent overhead associated with using the Tor network. Tor\u2019s routing mechanism is\ndesigned to provide anonymity by encrypting and routing traffic through multiple nodes,\nwhich introduces latency and computational overhead. Even though the Tor proxy in\nthis scenario is operating locally, it still retains the characteristics of the network\u2019s design,\nwhich may result in slower communication between processes compared to the direct\nnature of named pipes or shared memory. Standard IPC mechanisms, like named pipes\nor shared memory, are optimized for low-latency, high-throughput data exchange between\nprocesses on the same machine. Tor, on the other hand, is optimized for privacy, which\nmeans performance is not a priority.\nAdditionally, using Tor introduces complexity, both in deployment and maintenance.\nThe Tor client requires certain configurations, such as creating and managing the data\ndirectory, handling key files, and maintaining network state. This setup may increase\nthe chance of detection by endpoint monitoring tools that look for non-standard direc-\ntory structures or unauthorized executables, especially when these executables exhibit\nbehavior associated with network anonymization. In a scenario where security policies\nare configured to monitor for unauthorized use of Tor or similar software, such behavior\nmay raise an alarm, leading to further investigation.\nFrom a technical standpoint, using Tor also poses risks of failure related to network\ncomponents. For example, if the local Tor process crashes or is terminated by end-\npoint security software, the entire communication channel would be disrupted, effectively\ndisabling any data flow between the parent and child processes. In contrast, IPC mecha-\nnisms like named pipes or shared memory are more tightly integrated into the operating\nsystem, and thus less prone to being disrupted by network-related issues. This depen-\ndence on the local Tor process introduces an additional point of failure that may make\nthe malware less resilient in certain environments.\nUsing Tor as a local means of inter-process data exchange also complicates the task of\nmaintaining persistence. Persistence mechanisms like registry modifications or scheduled\ntasks must be crafted to not only ensure that the malware payload is reinstated after\na reboot, but also that the Tor component remains operational. If the Tor client is\nblocked, disabled, or deleted, the entire communication strategy collapses. This makes the\nmalware inherently more brittle compared to implementations relying on more nativeIPC\napproaches, where persistence and functionality could be maintained more seamlessly\nwithin the operating system\u2019s standard features.\nFurthermore, the use of Tor introduces a visibility challenge for the malware itself.\nNetwork security analysts and advanced detection tools often flag Tor-related processes\nor network activity for closer examination, given Tor\u2019s common use by malware for\ncommand-and-control communication. In an environment where network monitoring is\nperformed actively, the presence of a Tor client, even if it is just used locally, can serve as\nan Indicator of Compromise (IoC ) and might invite forensic analysis of the host system.\nNovember 26, 2024Malware Analysis Report 95\nTraditional IPC methods, such as named pipes, tend to blend in with other operating\nsystem activity, making them inherently more covert from an analyst\u2019s perspective.\nIn conclusion, the decision to useTor as a local proxy for Inter-Process Communication\ninvolves a trade-off between the desire for anonymity and modularity versus the efficiency\nand resilience provided by standard IPC mechanisms. The advantages of using Tor\ninclude enhanced anonymity, modular separation of network responsibilities, and cross-\nplatform adaptability. However, these benefits come at the cost of increased complexity,\nreduced communication efficiency, and the risk of raising suspicions due to the inherently\nrecognizable and often monitored presence of Tor components. This approach is effective\nin highly targeted attacks where the benefits of obfuscation and anonymity outweigh\nthe drawbacks, but it may be counterproductive in environments with strong network\nmonitoring and endpoint protections, where the presence of Tor can itself trigger alerts.\nAt the same time, the Tor Client performs remote connections towards TOR nodes\nand employing DGA domains to hide its real destination.\nFigure 158: TOR Client connecting towards TOR Network.\nBy trying to load the executable inside ILSpy, it is also possible to gather the presence\nof the DotNetTor DLL (v.2.3.3.0) as an additional reference to the discussion provided\nabove.\nFigure 159: Runtime Broker.exe implements DotNetTor library.\nIn conclusion, the observed execution demonstrates the malware\u2019s primary objective:\nto comprehensively map the victim\u2019s system asset , exfiltrate valuable information , and\ndeploy additional payloads. However, it is evident that the malware\u2019s capabilities extend\nbeyond those exhibited during this analysis. This observation suggests that either pro-\nlonged analysis durations are required or that certain features, such as Process Injection\nor Shellcode Execution, necessitate activation via attacker-issued commands.\nNovember 26, 2024Malware Analysis Report 96\n4.7 Sixth Stage\nFigure 160: Moving from Fifth-Stage to Sixth-Stage.\n4.7.1 Code Obfuscation\nWith respect to different six executables identified as possible additional threats, only one\nof them was actively deployed on the analyzed system,%TEMP%\\RuntimeBroker.exe,\ntor.exe and is not a packed executable. On teh other hand, it is of interest to analyze\nthe embedded and not used tsunami payload.dll\n4.7.2 Code Analysis - tsunami payload.exe\nAs with the previously identified executable, this additional payload similarly embeds\na .NET DLL within its code. This practice reflects a recurring design choice by the\nthreat actors, indicating a preference for incorporating modular components directly into\ntheir executables. By embedding such a library, the attackers can encapsulate specific\nfunctionalities, likely to maintain modularity and ensure that critical operations remain\nwithin the same binary, reducing dependencies on external files.\nThe inclusion of a .NET DLL suggests that the payload is leveraging the capabilities\nof the .NET framework to implement complex functionalities, which may include system-\nlevel operations, network communication, or further stages of malicious behavior. This\napproach enables the attackers to streamline their deployment process, as the embedded\nlibrary eliminates the need for downloading or unpacking additional resources during\nruntime, which could otherwise expose the malware to detection.\nHowever, the embedded nature of the.NET DLL also presents opportunities for static\nanalysis. Analysts can isolate and extract the library for closer examination, poten-\ntially uncovering the specific functionalities it provides or its interactions with the larger\npayload. Such insights could offer valuable intelligence into the attacker\u2019s objectives,\nmethodologies, or even allow for the creation of signatures to detect the malware.\nNovember 26, 2024Malware Analysis Report 97\nThe reuse of this technique in multiple payloads underscores the attackers\u2019 methodical\napproach to constructing their malware, emphasizing modularity and reusability across\ntheir toolset. It also raises questions about the specific role and necessity of embedding\nsuch a library in this particular case, suggesting either a deliberate redundancy to ensure\nfunctionality or a potential oversight during the payload\u2019s development.\nFigure 161: Overview of the TSUNAMI PAYLOAD embedded .NET DLL\nThe code demonstrates clear intentions to disable system security features , establish\npersistence through a scheduled task, initiate Tor-based communication, and send teleme-\ntry data to a remote server.\nThe Main method initializes the program by calling the Meta.Init function with the\nusage type set to TsunamiPayload, signaling its role within the malware\u2019s architecture.\nIt then invokes the Start() method, which orchestrates the core functionality of the pay-\nload. It begins by disabling Windows Defender and Firewall through the DisableWin-\ndowsSecurity() function, which leverages the AntiMalware class. This ensures that\ncritical security mechanisms are neutralized, allowing the malware to operate with min-\nimal resistance and performs it in the same way as it was achieved previously by the\nRuntime Broker.dll .\nPersistence is established by creating a scheduled task named Runtime Broker. Using\nthe TaskService library, the malware registers this task to execute the previous stage\nRuntime Broker.exe , TSUNAMI INSTALLER , located in AppData Roaming.\nThis ensures the payload is executed at every user logon, effectively embedding itself into\nthe system\u2019s startup process. The configuration of the task, such as enabling it to run\nwith administrative privileges (RunLevel = 1 ) and allowing multiple instances, highlights\nthe attacker\u2019s efforts to ensure resilience and continuous operation.\nAfter establishing persistence, the TorProxy component is installed and started, while\nTelemetryUploader.SendApplicationLogs()is used to share telemetry data within\nthe C2 server . All of these actions perfectly mimic what was previously achieved with\nRuntime Broker.dll .\nError handling within the Start method ensures the program remains functional even\nif certain operations, such as creating the scheduled task, fail. However, the logging of\nNovember 26, 2024Malware Analysis Report 98\nsuccess messages for failed operations (Logger.LogSuccess()in the catch block) appears\nto be a misleading or incorrectly implemented feature, possibly intended to confuse or\nmislead analysts.\nFigure 162: Tsunami payload.dll Main method\nIn summary, the tsunami payload.dll performs a narrowed subset of the actions\nseen in its preceding stage, while embedding a significant portion of the same source code.\nDespite this overlap, a few critical differences are notable. One of the most significant\nchanges is the method of persistence, which is now achieved through the creation of a\nscheduled task specifically targeting the TSUNAMI INSTALLER. This mechanism\nensures that the installer is executed at every user logon, embedding the payload firmly\ninto the system\u2019s startup sequence.\nAnother key distinction lies in the selectivewhitelisting of executables. Unlike previous\nstages, where broader security exceptions were made, this stage restricts the whitelist to\na more curated set of executables. This modification could reflect an attempt to minimize\ndetection or streamline the malware\u2019s operations by focusing only on components deemed\nessential for its functionality.\nThese changes highlight a potential evolution in the attacker\u2019s methodology, aiming\nfor efficiency and stealth while maintaining the core capabilities of the malware. The per-\nsistence mechanisms, combined with the adjusted scope of whitelisting, indicate a refined\napproach to ensuring the payload\u2019s longevity and operational success on compromised\nsystems.\nNovember 26, 2024Malware Analysis Report 99\n\u2022 %APPDATA%\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\\System Runtime\nMonitor.exe\n\u2022 %APPDATA%\\Microsoft\\Windows\\Applications \\Runtime Broker.exe\n\u2022 %LOCALAPPDATA%\\Microsoft\\Windows\\Applications\\Runtime Broker.exe\n\u2022 %LOCALAPPDATA%\\Microsoft\\Windows\\WindowsApps\\msedge.exe\nNovember 26, 2024Malware Analysis Report 100\n5 Additional Analysis of Attacker\u2019s Infrastructure\nBy moving around attacker\u2019s Webserver hosted at 86.104.74[.]51 it has been possible\nto gather additional information on tits setup, by looking at the PHPInfo page. This\nprovides a detailed overview of the attacker\u2019s server environment, exposing vulnerabilities\nand potential exploitation points that are critical for tracking their infrastructure. By\ncorrelating this information with the activity and characteristics of the identified IPs,\na coherent picture of the attacker\u2019s tactics, techniques, and infrastructure management\nemerges.\nThe server hosting thePHPInfo page operates on Windows Server 2016 and employs a\nlightweight XAMPP stack, consisting of Apache 2.4.58 and PHP 8.0.30. This configura-\ntion points to a possible development or staging environment, as indicated by the paths\n(C:/xampp/php, C:/xampp/apache ) and default settings, such as postmaster@localhost\nfor the server administrator. The exposure of the PHPInfo page itself demonstrates poor\noperational security, which either reflects an oversight or deliberate disregard for stealth,\npotentially indicating a rushed or less sophisticated deployment.\nFigure 163: Overview of the PHPInfo() available on attacker\u2019s main Webserver.\nFurther examination reveals that key configurations, such as the enabledallow url fopen\ndirective and permissive upload and execution parameters (upload max filesize=1024M ),\ncould facilitate malicious activities like remote file inclusion or large payload execution.\nThe combination of high resource allowances, enabled error reporting, and lack of critical\nfunction restrictions suggests that the server is configured to handle resource-intensive\nor long-running scripts, such as those used for data exfiltration or payload unpacking.\nThe presence of multiple enabled PHP extensions, including cURL, zlib, and bz2, further\ndemonstrates capabilities for advanced data handling and compressed payload manipu-\nlation, which are hallmarks of modern malicious operations.\nThe PHPInfo file also provides insight into the network environment, exposing regis-\ntered streams and protocols that include HTTP2, SSL/TLS, and other transports. These\ndetails suggest that the server is equipped for complex and secure network communication,\na requirement for modern Command-and-Control (C2) frameworks. Such configurations\nenhance the attacker\u2019s ability to execute multi-layered campaigns, though they also offer\nindicators that can be leveraged for detection and tracking.\nFigure 164: Some additional parameters\nNovember 26, 2024Malware Analysis Report 101\nFurthermore, this same configuration file allows to gather very interesting additional\ninsights also on the Windows system running behind this Webserver. The asset it-\nself is a Windows-based server operating with administrative privileges, named WIN-\nBS656MOF35Q, and configured to allow Remote Desktop Protocol (RDP) access. The\npresence of SESSIONNAME set to RDP-Tcp#0 indicates that the attacker is actively\nmanaging the server using RDP, originating from a client machine named DESKTOP-\nV0U7LU6. The use of RDP for connecting to the server implies that the attacker requires\nmanual control, allowing them to directly execute commands, manage files, and make\nreal-time adjustments to the malicious infrastructure.\nThe client machine name, DESKTOP-V0U7LU6, appears to follow a default Win-\ndows naming convention, suggesting that this client system is either newly configured or\nintentionally generic. This default configuration could indicate a throwaway device being\nused for malicious purposes while minimizing any personalized trace that might link back\nto the attacker\u2019s identity or reveal additional information. This is a common tactic used\nto maintain operational security (OPSEC ), as a non-descriptive system name helps avoid\ndrawing attention during investigations or when interacting with compromised systems.\nFigure 165: Information about the underlying Windows Server system.\nThe server hardware itself is a powerful Windows machine, with thePROCESSOR AR\nCHITECTURE set to AMD64 and NUMBER OF PROCESSORS set to 32. The pro-\ncessor is identified as Intel64 Family 6 Model 79 Stepping 1, GenuineIntel , highlighting\nthat this asset has substantial computational resources, possibly indicating a server-grade\nmachine or a high-end workstation. This level of computing power suggests that the sys-\ntem is capable of supporting demanding operations, such as encryption, network relays,\nor multi-threaded control of a large number of compromised clients.\nThe attacker has configured the server usingXAMPP, a popular development environ-\nment that includes Apache, PHP, and MySQL. This configuration is evident from paths\nlike DOCUMENT ROOT set to C:/xampp/htdocs and the use of PHP version 8.0.30,\nApache 2.4.58, and OpenSSL 3.1.3. The use of XAMPP is particularly significant as it\npoints to a development or testing server configuration that may not be appropriately\nsecured for a production environment. XAMPP is designed for ease of use, and default\nconfigurations often lack the security features necessary to protect the system in a live\ndeployment. This provides a window of opportunity for defenders, as these configurations\nmay expose vulnerabilities or lead to misconfigurations that could be exploited to regain\ncontrol of the system or disrupt the attacker\u2019s operations.\nNovember 26, 2024Malware Analysis Report 102\nFigure 166: Server\u2019s Software lists\nThe server IP address is confirmed by the SERVER NAME, SERVER ADDR, and\nHTTP HOST variables. The SERVER SIGNATURE reveals the software stack being\nused, which includes Apache and PHP, while running on a Windows ( Win64) environ-\nment. This stack\u2019s details are critical for identifying potential vulnerabilities that may be\nexploited by defenders. Additionally, the use of HTTP on port 80 ( SERVER PORT set\nto 80) implies that the server may not enforce secure ( HTTPS) communications, leaving\nit potentially vulnerable to Man-in-the-Middle (MITM ) attacks.\nThe presence of a web-based dashboard (HTTPREFERER set tohxxp[:]//86.104.74[.]\n51/dashboard/) implies that the server is being used to host a control panel, which may\nbe central to managing the infrastructure or interacting with compromised clients. Such\ndashboards are often used in Command-and-Control (C2) operations, providing an inter-\nface for the attacker to manage their campaigns, send commands, and exfiltrate data. The\nfact that this dashboard is accessible over HTTP further suggests lax security and could\nprovide an opportunity for defenders to exploit weaknesses in the interface or intercept\nunencrypted data.\nIn addition to XAMPP, the presence ofNode.js and NVM (Node Version Manager) in-\nstalled on the server, with directories likeC:\\Program Files\\nodejs and C:\\Users\\Adminis\ntrator\\AppData\\Roaming\\nvm included in the system Path, suggests that the attacker is\nusing JavaScript-based tools or services. Node.js is often used for executing lightweight\nscripts, hosting web services, or automating various aspects of a campaign. The inclusion\nof both XAMPP and Node.js illustrates the versatility of the attacker\u2019s infrastructure,\nwhich is configured to support multiple scripting environments, potentially allowing for\nrapid adaptation to different tasks and objectives. This highlights the server\u2019s capabil-\nity to execute multiple types of workloads, from traditional web hosting to script-based\noperations.\nThe environment variables also reveal that the attacker is operating with admin-\nistrative privileges, as indicated by the USERNAME being set to Administrator and\nUSERPROFILE pointing to C:\\Users\\Administrator. Administrative privileges give\nthe attacker a high degree of control over the server, allowing them to install addi-\ntional tools, make system modifications, and persist within the system. Such privi-\nleges also suggest that the attacker might have used privilege escalation techniques to\ngain control over the server, possibly leveraging existing vulnerabilities or weak con-\nfigurations. The presence of PowerShell modules in the PSModulePath (C:\\Program\nFiles\\WindowsPowerShell\\Modules) implies that PowerShell scripts are available, which\nare frequently used by attackers to automate various post-exploitation tasks, including\nenumeration, data exfiltration, and lateral movement within the network.\nNovember 26, 2024Malware Analysis Report 103\nFigure 167: Server\u2019s Environmental variables\nThe CLIENTNAME, SESSIONNAME, and LOGONSERVER values collectively con-\nfirm that the attacker has direct, manual access to the server, which might indicate an\ninterest in maintaining control of the asset beyond automated scripts. This manual in-\ntervention could involve more sophisticated or targeted operations that require real-time\ndecision-making or adjustment based on network conditions, responses from defenders, or\nthe progress of their activities. The presence of RDP (RDP-Tcp#0) further emphasizes\nthe attacker\u2019s active presence on the system, managing and operating the infrastructure\nthrough a graphical user interface.\nThe network details, including the REMOTE ADDR value of 85.190.233[.]54, sug-\ngest active client connections to the server, which could represent either compromised\nvictims or an intermediate attacker device interacting with the hosted infrastructure.\nThis interaction indicates ongoing activity, potentially involving monitoring or control-\nling compromised clients through the web-based dashboard or other means.\nTemporary paths such asTEMP and TMP set to C:\\Users\\ADMINI\u02dc1\\AppData\\Local\n\\Temp\\2 are indicative of locations that may be used by the attacker for staging payloads\nor storing intermediary files before exfiltration. These directories are commonly used due\nto their writable nature and are easily accessible by all processes, making them ideal for\ntemporarily holding malicious payloads without raising suspicion.\nFigure 168: Server\u2019s remote RDP connection details and Temp folders paths.\nIn conclusion, the asset under analysis is a Windows server, powerful and versatile,\nconfigured for both web hosting and script execution, with active RDP-based control by\nan attacker using administrative privileges. The server leverages XAMPP for web ser-\nvices, Node.js for scripting, and has direct, potentially insecure web interfaces that expose\nmanagement capabilities through an HTTP-based dashboard. The asset is accessible via\nRDP from a generic client machine, indicating an effort by the attacker to maintain an\nactive, low-profile presence. While the setup provides the attacker with significant flexi-\nbility and capability, it also exposes several security weaknesses. The use ofXAMPP with\ndefault configurations, a publicly accessible HTTP dashboard, and reliance on RDP all\npresent potential points of vulnerability that could be exploited by defenders to disrupt\nthe attacker\u2019s control over the infrastructure, gather further intelligence, or mitigate the\nongoing malicious activities.\nNovember 26, 2024Malware Analysis Report 104\n6 Mitigation Strategies\nTo mitigate the risks posed by threats of this nature, organizations must adopt a compre-\nhensive and proactive approach to cybersecurity. Enhancing employee awareness through\nregular training can significantly reduce the effectiveness of social engineering tactics, as\neducated staff are less likely to fall prey to deceptive schemes like fictitious job offers.\nImplementing advanced security solutions capable of detecting and responding to obfus-\ncated and multi-stage malware is essential. Regular system updates and the application\nof security patches can close vulnerabilities that attackers might exploit.\nStrengthening authentication processes by adopting multi-factor authentication can\nadd an additional layer of security, for sensitive accounts, making unauthorized access\nmore difficult especially for attackers exfiltrating administrative credentials from low-\nprivilege systems. Monitoring network activity for anomalies and establishing robust in-\ncident response plans can further enhance an organization\u2019s ability to detect and respond\nto intrusions promptly. Collaborating with cybersecurity professionals and participating\nin information-sharing initiatives can help organizations stay informed about emerging\nthreats and adapt their defenses accordingly.\nBy fostering a security-conscious culture and investing in advanced protective mea-\nsures, organizations can better safeguard themselves against sophisticated cyber adver-\nsaries like the Lazarus Group. Remaining vigilant and adaptive is crucial in the ever-\nevolving landscape of cyber threats, ensuring that defenses evolve in tandem with the\ntactics employed by attackers.\nAdditionally, by taking into account identified IoCs and TTPs, reported inside the\nAppendix section (App. A.1), both a proactive approach and a Threat Intelligence based\none can be implemented. These allows to track possible already established compromise\nand block malicious files which could be exploited by the Threat Actor to have a foothold\ninside the victim\u2019s network.\nNovember 26, 2024Malware Analysis Report 105\n7 Conclusion\nThis report highlights a sophisticated and meticulously constructed multi-stage threat\ncampaign, demonstrating technical expertise and a focused intent on long-term system\ncompromise and financial data theft. The campaign unfolds through a series of infection\nstages, each building upon the last with enhanced functionality and advanced obfus-\ncation techniques. This layered approach reflects the attackers\u2019 careful planning and\nunderstanding of security mechanisms, ensuring that each stage remains both functional\nand resistant to detection.\nObfuscation emerges as a cornerstone of this campaign, with techniques such asmulti-\nlayered encoding and control flow manipulation employed to hinder reverse engineering\nand evade standard detection methods. These methods not only complicate analysis\nbut also underscore the attackers\u2019 efforts to protect their malware from scrutiny and\ncountermeasures. The modular design of the malware further enhances its adaptability,\nallowing it to dynamically incorporate additional components, update its functionalities,\nand tailor its operations to specific environments. This flexibility demonstrates a level of\nsophistication that is characteristic of advanced threat actors.\nThe malware\u2019s focus on targeting sensitive data is particularly notable. It employs a\nrange of techniques, including credential harvesting, clipboard monitoring, and direct file\nextraction, to exfiltrate information such as browser-stored credentials, cryptocurrency\nwallet details , and system configurations. This breadth of capability reflects a deliberate\nintent to maximize the value of compromised systems. Persistence mechanisms, such as\nthe creation of scheduled tasks and the use of startup folder scripts , further reinforce this\nintent, ensuring the malware remains operational even after system restarts.\nAn intriguing aspect of the analysis is the integration of open-source components and\nlegitimate tools, such as Python and AnyDesk, into the malware\u2019s architecture. By em-\nbedding publicly available utilities, the attackers not only extend the malware\u2019s capabil-\nities but also exploit the trust associated with these legitimate tools to evade detection.\nHowever, the presence of unused code, debugging information , and redundant artifacts\nwithin the malware suggests a degree of oversight or a rushed deployment. These rem-\nnants offer valuable insight into the attackers\u2019 development processes and potential areas\nof improvement.\nThe Tactics, Techniques, and Procedures observed in this campaign strongly align\nwith those associated with the Lazarus Group, a North Korean state-sponsored Threat\nActor known for targeting financial institutions and engaging in cyber-espionage. The\ncampaign\u2019s focus on financial and cryptocurrency-related data, combined with its ad-\nvanced design and execution, aligns with the group\u2019s established objectives and opera-\ntional patterns.\nThe analysis underscores the growing sophistication of modern cyber threats and the\nnecessity for enhanced defensive measures. It highlights the importance of proactive\nthreat hunting, robust monitoring for Indicators of Compromise, and comprehensive user\neducation to mitigate risks. This campaign exemplifies the evolving nature of advanced\npersistent threats, revealing a highly adaptive adversary capable of leveraging both tech-\nnical innovation and strategic planning to achieve its objectives.\nNovember 26, 2024Malware Analysis Report 106\nA Appendix\nA.1 IoCs, TTPs & Yara Rules\nThe entire set of IoCs, TTPs and few Yara Rules, gathered through-out this entire\nanalysis, are available inside the following AlienVault OTX pulse.\nFigure 169: Overview of the AlienVault OTX pulse\nNovember 26, 2024Malware Analysis Report 107\nA.2 Sigma Rules\n1 title: Detection of Suspicious AnyDesk File Modification and Termination via\nPowerShell\n2 id: 1234abcd-5678-efgh-ijkl-9012mnopqrst\n3 description: Detects suspicious PowerShell activity involving AnyDesk file\nmodification and process termination when specific command patterns are\nobserved.\n4 status: experimental\n5 author: Alessio Di Santo\n6 date: 2024-11-26\n7 logsource:\n8 category: process_creation\n9 product: windows\n10 detection:\n11 selection:\n12 Image: \u2019*\\powershell.exe\u2019\n13 CommandLine|all:\n14 - \u2019ad.anynet.pwd_hash=\u2019\n15 - \u2019ad.anynet.pwd_salt=\u2019\n16 - \u2019ad.anynet.token_salt=\u2019\n17 - \u2019taskkill /IM anydesk.exe /F\u2019\n18 condition: selection\n19 fields:\n20 - CommandLine\n21 - ParentCommandLine\n22 - ParentImage\n23 - Image\n24 - User\n25 level: high\n26 tags:\n27 - attack.persistence\n28 - attack.t1562.001\n29 - attack.t1098\n30 falsepositives:\n31 - Legitimate administrative maintenance involving AnyDesk\n32 mitre:\n33 - T1562.001\n34 - T1098\nNovember 26, 2024Malware Analysis Report 108\n1 title: Detection of Suspicious Scheduled Task for Runtime Broker.exe\n2 id: abcd1234-efgh-5678-ijkl-9012mnopqrst\n3 description: Detects the creation of a scheduled task targeting Runtime Broker\n.exe located in %APPDATA%\\Microsoft\\Windows\\Applications for persistence.\n4 status: experimental\n5 author: Alessio Di Santo\n6 date: 2024-11-26\n7 logsource:\n8 category: process_creation\n9 product: windows\n10 detection:\n11 selection:\n12 Image: \u2019*\\powershell.exe\u2019\n13 CommandLine|all:\n14 - \u2019New-ScheduledTaskAction -Execute\u2019\n15 - \u2019Register-ScheduledTask\u2019\n16 - \u2019TaskName \"Runtime Broker\"\u2019\n17 - \u2019LogonType Interactive\u2019\n18 - \u2019*\\Microsoft\\Windows\\Applications\\Runtime Broker.exe\u2019\n19 condition: selection\n20 fields:\n21 - CommandLine\n22 - ParentCommandLine\n23 - ParentImage\n24 - Image\n25 - User\n26 - FileName\n27 level: high\n28 tags:\n29 - attack.persistence\n30 - attack.t1053.005\n31 falsepositives:\n32 - Legitimate scheduled task creation by administrators targeting similar\npaths\n33 mitre:\n34 - T1053.005\nNovember 26, 2024Malware Analysis Report 109\n1 title: Detect Specific Windows Firewall Rule Exclusions\n2 id: 5678abcd-ef01-2345-ghij-klmnopqrstuv\n3 status: experimental\n4 description: Detects suspicious Windows Firewall rule additions that include\nspecific paths for exclusion, such as \u2018Runtime Broker.exe\u2018, \u2018msedge.exe\u2018,\nand \u2018System Runtime Monitor.exe\u2018.\n5 author: Alessio Di Santo\n6 date: 2023-11-26\n7 logsource:\n8 product: windows\n9 service: sysmon\n10 detection:\n11 selection:\n12 EventID: 1\n13 CommandLine|contains|all:\n14 - \u2019netsh advfirewall firewall add rule\u2019\n15 - \u2019action=allow\u2019\n16 CommandLine|contains:\n17 - \u2019\\System Runtime Monitor.exe\u2019\n18 - \u2019\\Microsoft\\Windows\\Applications\\Runtime Broker.exe\u2019\n19 - \u2019\\Microsoft\\Windows\\Applications\\msedge.exe\u2019\n20 - \u2019C:\\Users\\*\\AppData\\Local\\Temp\\Runtime Broker.exe\u2019\n21 condition: selection\n22 fields:\n23 - CommandLine\n24 - Image\n25 - ParentCommandLine\n26 - User\n27 - HostName\n28 falsepositives:\n29 - Legitimate configuration of Windows Firewall rules for trusted\napplications.\n30 - Administrative scripts for deploying or updating legitimate software.\n31 level: high\n32 tags:\n33 - attack.defense-evasion\n34 - attack.t1562.004\n35 - windows-firewall\n36 - netsh\n37 - known-folder-paths\n38 modifications:\n39 - Tailored rule to focus on known suspicious paths being excluded via\nfirewall rules.\n40 - Excludes benign patterns based on environment-specific baselines.\nNovember 26, 2024Malware Analysis Report 110\n1 title: Detection of Malicious Windows Defender Exclusion Paths\n2 id: 5678efgh-1234-abcd-ijkl-9012mnopqrst\n3 description: Detects suspicious usage of the Add-MpPreference PowerShell\ncommand to add specific paths to Windows Defender exclusion list.\n4 status: experimental\n5 author: Alessio Di Santo\n6 date: 2024-11-26\n7 logsource:\n8 category: process_creation\n9 product: windows\n10 detection:\n11 selection:\n12 CommandLine|contains:\n13 - \"Add-MpPreference -ExclusionPath\"\n14 paths:\n15 CommandLine|contains:\n16 - \"\\System Runtime Monitor.exe\"\n17 - \"\\Microsoft\\Windows\\Applications\\Runtime Broker.exe\"\n18 - \"\\Microsoft\\Windows\\Applications\\msedge.exe\"\n19 condition: selection and paths\n20 fields:\n21 - CommandLine\n22 - ParentCommandLine\n23 - ParentImage\n24 - Image\n25 - User\n26 level: high\n27 tags:\n28 - attack.persistence\n29 - attack.t1562.001\n30 - attack.defense_evasion\n31 falsepositives:\n32 - Legitimate administrative usage\n33 mitre:\n34 - T1562.001\n35 - T1070.006\n36 - T1098\nNovember 26, 2024Malware Analysis Report 111\n1 title: Malicious System Information Collection via WMIC and Registry Queries\n2 id: e3b8c5f4-1d2e-43d9-8748-82b8cbe3c28a\n3 description: Detects suspicious WMIC and registry queries used for system\nreconnaissance or enumeration. Intended for use with SIEM aggregation to\nidentify all activities over time.\n4 status: experimental\n5 author: Alessio Di Santo\n6 date: 2024-11-26\n7 logsource:\n8 category: process_creation\n9 product: windows\n10 detection:\n11 selection_wmic_processor_name:\n12 CommandLine|contains: \u2019wmic path Win32_Processor get Name\u2019\n13 selection_wmic_processor_cores:\n14 CommandLine|contains: \u2019wmic path Win32_Processor get NumberOfCores\u2019\n15 selection_wmic_videocontroller:\n16 CommandLine|contains: \u2019wmic path Win32_VideoController get Name\u2019\n17 selection_wmic_os:\n18 CommandLine|contains: \u2019wmic os get Caption\u2019\n19 selection_reg_query_productid_32bit:\n20 CommandLine|contains: \u2019reg query \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\\nWindows NT\\CurrentVersion\" /v ProductID\u2019\n21 selection_reg_query_productid_64bit:\n22 CommandLine|contains: \u2019reg query \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\\nMicrosoft\\Windows NT\\CurrentVersion\" /v ProductID\u2019\n23 condition: selection_wmic_* or selection_reg_*\n24 fields:\n25 - CommandLine\n26 - ParentCommandLine\n27 - ParentImage\n28 - Image\n29 - User\n30 level: high\n31 tags:\n32 - attack.discovery\n33 - attack.t1082\n34 falsepositives:\n35 - Legitimate administrative tools or scripts\n36 mitre:\n37 - T1082\nNovember 26, 2024Malware Analysis Report 112\nA.3 Infection Chain\nNovember 26, 2024Malware Analysis Report 113\nA.4 Diamond Model\nNovember 26, 2024",
      "arXiv:2505.23661v1  [cs.CV]  29 May 2025\nOpenUni: A Simple Baseline for Unified Multimodal\nUnderstanding and Generation\nSize Wu\u22171 Zhonghua Wu\u22172 Zerui Gong\u22171\nQingyi Tao2 Sheng Jin3 Qinyue Li2 Wei Li1 Chen Change Loy1\n1 S-Lab, Nanyang Technological University 2 SenseTime Research 3 SenseTime Research and Tetras.AI\nsize001@e.ntu.edu.sg wei.l@ntu.edu.sg ccloy@ntu.edu.sg\nABSTRACT\nIn this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying\nmultimodal understanding and generation. Inspired by prevailing practices in unified model learning,\nwe adopt an efficient training strategy that minimizes the training complexity and overhead by\nbridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a\nset of learnable queries and a light-weight transformer-based connector. With a minimalist choice\nof architecture, we demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as GenEval, DPG-\nBench, and WISE, with only 1.1B and 3.1B activated parameters. To support open research and\ncommunity advancement, we release all model weights, training code, and our curated training\ndatasets (including 23M image-text pairs) at https://github.com/wusize/OpenUni. 1\n0 2 4 6 8\nLLM Parameters (Billions)\n0.70\n0.75\n0.80\n0.85GenEval Performance\nOpenUni-B-512\nOpenUni-L-512\nOpenUni-L-1024\nBLIP3-o-4B\nBLIP3-o-8B\nBLIP3-o-8B*\nMetaQuery-B\nMetaQuery-L\nMetaQuery-XL\nHarmon-0.5B\nHarmon-1.5B\nJanus-Pro-1B\nJanus-Pro-7B\nOpenUni\nBLIP3-o\nMetaQuery\nHarmon\nJanus Pro\n(a) Evaluation of text-to-image generation: performance of Ope-\nnUni variants and baselines on GenEval versus parameter count.\nMMBench MME-P MMMU\nPerformance\n81.1\n2,221\n48.6\n78.6\n1,528\n46.6\n78.6\n1,574\n53.1\n65.5\n1,155\n38.9\n79.2\n1,567\n41.0\nMetaQuery-L\nBLIP3-O 4B\nOpenUni-L (Ours)\nHarmon-1.5B\nJanus Pro 7B\n(b) Multimodal understanding: comparison on MMB ENCH ,\nMME-P and MMMU.\nFigure 1: OpenUni delivers strong performance with efficient parameter usage across bothgeneration and understanding\ntasks.\n1 Introduction\nThe landscape of multimodal artificial intelligence has been dominated by the recent progress of multimodal large\nlanguage models (LLMs) [ 1, 2, 3, 4, 5, 6, 7, 8] and diffusion models [ 9, 10, 11, 12, 13, 14, 15, 16], driven by\narchitectural innovations and computational scalings of transformers [17]. To further advance the frontier of multimodal\n\u2217Equal contribution.\n1This is an ongoing project.intelligence, a natural leap forward would be integrating the two minds of understanding and generation into a single\nbrain, demonstrated by GPT4-o [18]\u2019s impressive instruction-following ability of content generation.\nExisting research efforts that unify multimodal understanding and generation can typically be divided into two\nsubtracks. One type of work [19, 20, 21, 22, 23, 24, 25] explores native multimodal models from scratch and shares\nthe parameters of LLM for both tasks. Another line of work stitches LLMs and generation models to build unified\nframeworks [26, 27, 28, 29, 30]. More recently, MetaQuery [31] and BLIP3-o [29] directly align frozen multimodal\nLLMs with diffusion models, effectively instilling generation ability into an already established multimodal system.\nThese works unveil the potential of a simple connection module that transfers the knowledge of well-trained LLMs\nto generation models, for controllable and high-quality visual generation. Inspired by these studies, we present\nOpenUni, an open-source framework for unified multimodal understanding and generation, with minimum architectural\ncomplexities and computational overhead.\nSpecifically, OpenUni adheres to the simplest design choices presented in MetaQuery [31], using only learnable queries\nand a light-weight connector between a multimodal LLM (MLLM) and a diffusion model. A two-stage training recipe\nis adopted to build OpenUni. In the first (pre-training) stage, we align the LLM and diffusion model on 23M image-text\npairs, by training only the learnable queries and the connector. The 23M training images used in this stage are sourced\nfrom public datasets and re-captioned by LLMs, which will also be released. In the finetuning stage, we unlock the\ndiffusion model and train OpenUni on the 60k high-quality images contributed by BLIP3-o [29].\nWe implemented three model variants, namely OpenUni-B-512 and OpenUni-L-512 and OpenUni- L-1024, character-\nized by different model sizes and image resolutions. For image understanding, OpenUni inherits its base MLLM\u2019s strong\nperformance on multimodal question-answering benchmarks. For image generation, our smaller variant OpenUni-\nB-512 achieves a score of 0.84 on GenEval, on par with BLIP3-o-8B [ 32], using only 1.1B activated parameters,\nwhile significantly outperforming prior unified models like Janus-Pro [ 23]. Meanwhile, OpenUni-L-1024 archives\nthe best performance (0.86) among open-source unified models on GenEval with 3.1B activated parameters. Besides,\nOpenUni exhibits competitive performance on the WISE benchmark that assesses world knowledge comprehension,\nsurpassing models that utilize LLMs of similar scales. To support research and reproducibility, we release the full\nframework as an open-source baseline with minimal training complexity, modular design, publicly available training\ndata, and an out-of-the-box training pipeline.\n2 Related Work\nMultimodal LLMs for Visual Understanding. Built upon a visual encoder [ 33, 34] and an LLM [ 35, 36, 6],\nmultimodal LLMs [ 1, 2, 3, 4, 37, 38, 39, 40] produce language responses based on visual inputs, allowing new\ncapabilities like visual reasoning, dialogue, and instruction following. Despite their powerful visual understanding\ncapabilities, most existing multimodal LLMs are limited to text outputs. Therefore, equipping these LLMs with visual\ngeneration ability would be the key step towards next-generation multimodal intelligence. In this work, we build\nOpenUni upon InternVL3 [37], adapting its original world knowledge to image generation.\nText-to-Image Generation with Diffusion Models. Diffusion models [14, 9, 15, 11, 13, 16, 41, 42] have become the\ndominant paradigm for image generation, producing high-quality visual content conditioned on language descriptions.\nPioneering works [ 9, 11] typically formulate image generation as Denoising Diffusion Probabilistic Models [ 43]\n(DDPM), based on a U-Net [ 44] architecture. In more recent works [ 45, 46, 47, 48], Flow Matching (FM) forgoes\nexplicit diffusion simulation by learning an ODE-driven continuous transformation from noise to data, effectively\nsubsuming diffusion processes as a special case and permitting more direct probability transport paths to improve\nefficiency. For model architecture, state-of-the-art frameworks [41, 48, 49, 47, 46, 50] replace the U-Net backbone\nwith diffusion transformers [17] (DiTs). Among these models, SANA [46, 50] enhances both training and sampling\nefficiency with increased compression ratio [ 51] and linear attention [ 52]. In this work, we choose SANA as the\ndiffusion module of OpenUni to reduce computation cost.\nFrozen LLMs. Freezing the pre-trained weights of LLMs and incorporating task-specific modules has been an economic\nand effective approach to expanding their functionality [53, 54, 31, 29]. As an early attempt, F-LMM [ 53] builds a\nmask head on top of frozen LLMs, endowing the LLMs with grounding ability while preserving their reasoning and\ninstruction-following capabilities. For image generation, LlamaFusion [54] introduces extra transformer modules for\nvisual generation, alongside LLMs\u2019 original architecture. More recently, MetaQuery [31] and BLIP3-o [29] effectively\nbridge multimodal LLMs and diffusion models with a set of learnable queries. Inspired by these works, OpenUni is\nbuilt upon frozen multimodal LLMs.\nUnifying Multimodal Understanding and Generation. There are two design philosophies regarding unifying\nmultimodal understanding and generation. One subtrack [19, 20, 21, 22, 23, 24, 25] explores native multimodal models\n2Table 1: Architecture specifications and number of training images of OpenUni, MetaQuery [31] and BLIP3-o [29]. *In\nBLIP3-o, the DiT that predicts CLIP features is regarded as the connector.\nModel MLLM #Connector Params Diffusion Model #Images\nMetaQuery-B [31] LLaV A-OV-0.8B [4] 316M SANA-1.6B-512 [46] 25M\nMetaQuery-L [31] Qwen2.5VL-3B [56] Unknown SANA-1.6B-512 [46] 25M\nMetaQuery-XL [31] Qwen2.5VL-7B [56] Unknown SANA-1.6B-512 [46] 25M\nBLIP3-o-4B [29] Qwen2.5VL-3B [56] 1.4B* SDXL (2.6B) [11] 30M\nBLIP3-o-8B [29] Qwen2.5VL-7B [56] 1.4B* SDXL (2.6B) [11] 30M/50M\nOpenUni-B-512 InternVL3-1B [37] 54M SANA-0.6B-512 [46] 23M\nOpenUni-L-512 InternVL3-2B [37] 225M SANA-1.6B-512 [46] 23M\nOpenUni-L-1024 InternVL3-2B [37] 225M SANA-1.5-1.6B-1024 [50] 23M\nTable 2: Detailed hyperparameters in pre-training (stage I) and fine-tuning (stage II).\nSetting Stage I Stage II\nDiffusion Model Frozen Trainable\nLearning Rate 10\u22124 10\u22125\nBatch Size 512 256\nOptimizer AdamW [58] AdamW [58]\nGrad. Clip 1.0 1.0\nWeight Decay 0.05 0.05\nBetas (0.9, 0.95) (0.9, 0.95)\nSchedule Cosine Cosine\nTraining Steps 100,000 10,000\nWarm-up Steps 1,000 100\nfrom scratch and shares the parameters of LLM for both tasks. This type of work typically struggles to accommodate\nthe two inherently heterogeneous tasks that require representations at different levels of granularity. Disentangled visual\nencoders [22, 23] or mixture of experts [24, 25, 55] are usually adopted to handle conflicting pathways.\nThe more resource-efficient approach stitches well-trained LLMs and generation models to build unified frameworks,\nconnecting them with intermediate ViT features [26, 27, 28, 30] or learnable queries [31, 29]. Among these works,\nMetaQuery [31] and BLIP3-o [29] build unified frameworks upon frozen multimodal LLMs, effectively transferring the\nknowledge learned in understanding tasks to visual generation. Our OpenUni follows the simple architecture introduced\nby MetaQuery [31] and achieves significantly higher performance with fewer learnable parameters, setting a clean and\nstrong baseline for this research direction.\n3 OpenUni\n3.1 Model\nArchitecturally, OpenUni follows the design of MetaQuery [31], comprising N learnable queries, a multimodal LLM, a\ntransformer-based connector, and a diffusion model. In our implementation, we set N = 256. The visual understanding\nability of the multimodal LLM is fully retained since its weights remain frozen. During image generation, the learnable\nqueries extract conditioning information from the user\u2019s prompt during the LLM\u2019s forward pass; this information is\nthen processed by the connector and passed to the diffusion model via its cross-attention module.\nLightweight Connector. The architecture of OpenUni\u2019s connector is adapted from SigLIP\u2019s visual encoder [ 34].\nDifferent from pioneering works [ 31, 29] that feature a heavy connecting module between the LLM and diffusion\nmodel, OpenUni\u2019s connector only comprises six transformer layers.\nModel Variants. In this work, we build three model variants. OpenUni-B-512 is based on InternVL3-1B [ 37] and\nSANA-0.6B-512px [46] while OpenUni-L-512 adopts InternVL3-2B [37] and SANA-0.6B-512px [46]. In addition,\nwe increase the resolution of OpenUni-L\u2019s image generation by changing the diffusion model to SANA-1.5-1.6B-\n1024px [50]. This higher-resolution variant is named OpenUni-L-1024. The model specifications of OpenUni are\nprovided in Table 1, alongside comparisons with the pioneering works MetaQuery [31] and BLIP3-o [29].\nPrompt Format. For text-to-image generation, we use the following prompt template to format user instruction: \u201cUser:\nGenerate an image <caption>\\n Assistant:\u201d. <caption> represents the image description. During training,\n<caption> is randomly set to empty for 10% data samples to enable classifier-free guidance (CFG) [57] in inference.\n3Table 3: Results from the GenEval benchmark for text-to-image generation. Here, BLIP3-o-8B* indicates the model\nthat is trained with 30 million additional proprietary data samples. We highlight the best results in bold.\nType Method Single Obj. Two Obj. Counting Colors Position Color Attri. Overall \u2191\nGen. Only\nLlamaGen [63] 0.71 0.34 0.21 0.58 0.07 0.04 0.32\nLDM [64] 0.92 0.29 0.23 0.70 0.02 0.05 0.37\nSDv1.5 [64] 0.97 0.38 0.35 0.76 0.04 0.06 0.43\nPixArt-\u03b1[65] 0.98 0.50 0.44 0.80 0.08 0.07 0.48\nSDv2.1 [64] 0.98 0.51 0.44 0.85 0.07 0.17 0.50\nDALL-E 2 [15] 0.94 0.66 0.49 0.77 0.10 0.19 0.52\nEmu3-Gen [66] 0.98 0.71 0.34 0.81 0.17 0.21 0.54\nSDXL [11] 0.98 0.74 0.39 0.85 0.15 0.23 0.55\nDALL-E 3 [16] 0.96 0.87 0.47 0.83 0.43 0.45 0.67\nSD3-Medium [45] 0.99 0.94 0.72 0.89 0.33 0.60 0.74\nUnified\nChameleon [19] - - - - - - 0.39\nSEED-X [67] 0.97 0.58 0.26 0.80 0.19 0.14 0.51\nLMFusion [54] - - - - - - 0.63\nShow-o [21] 0.95 0.52 0.49 0.82 0.11 0.28 0.68\nEMU3 [66] - - - - - - 0.66\nTokenFlow-XL [68] 0.95 0.60 0.41 0.81 0.16 0.24 0.63\nJanus [22] 0.97 0.68 0.30 0.84 0.46 0.42 0.61\nJanus-Pro-1B [23] 0.98 0.82 0.51 0.89 0.65 0.56 0.73\nJanus-Pro-7B [23] 0.99 0.89 0.59 0.90 0.79 0.66 0.80\nHarmon-0.5B [30] 0.99 0.80 0.57 0.87 0.55 0.48 0.71\nHarmon-1.5B [30] 0.99 0.86 0.66 0.85 0.74 0.48 0.76\nMetaQuery-B [31] - - - - - - 0.74\nMetaQuery-L [31] - - - - - - 0.78\nMetaQuery-XL [31] - - - - - - 0.80\nBLIP3-o-4B [29] - - - - - - 0.81\nBLIP3-o-8B [29] - - - - - - 0.83\nBLIP3-o-8B* [29] - - - - - - 0.84\nOpenUni-B-512 0.99 0.91 0.74 0.90 0.77 0.73 0.84\nOpenUni-L-512 0.99 0.91 0.77 0.90 0.75 0.76 0.85\nOpenUni-L-1024 0.99 0.92 0.76 0.91 0.82 0.77 0.86\n3.2 Training Recipe\nWe adopt a two-stage training strategy, where we first align the LLM and the diffusion model in a pre-training stage.\nThen we fine-tune the aligned modules using high-quality training data. The training hyperparameters are listed in\nTable 2.\nStage 1: Pre-training. The primary goal of this stage is to train the learnable queries and the lightweight connector to\neffectively bridge the multimodal LLM and the diffusion transformer. Parameters of both the LLM and the diffusion\nmodel are frozen in this stage. The connector learns to translate the LLM\u2019s output features (elicited by the 256 learnable\nqueries) into conditioning signals that the diffusion model can interpret. We use a large composite dataset comprising\nseveral publicly available image/text collections: text-to-image-2M[59], LAION-Aesthetic- 6M[60], Megalith-10M[61],\nRedCaps-5M[62]. All of these images are captioned by LLMs. This results in a pre-training corpus of roughly 23\nmillion image-text pairs.\nStage 2: High-Quality Finetuning. To refine the generative capabilities of the entire system (connector and diffusion\nmodel) for improved instruction adherence, image quality, and robustness to diverse prompts, we leverage the instruction\ntuning dataset released by BLIP3-o [29]. The dataset consists of 60,000 high-quality image-text pairs generated by\nprompting GPT-4o with diverse captions and using models like DALL-E3 and Midjourney for image synthesis.\n4 Evaluation\nThis section details the evaluation setup, benchmarks, and results for OpenUni. We evaluated OpenUni\u2019s capabilities in\nboth image generation and multimodal understanding, comparing them against state-of-the-art models. Our evaluation\naims to demonstrate OpenUni\u2019s ability to achieve competitive performance with a simpler and light-weight architecture.\n4Table 4: Results from the DPG-Bench for text-to-image generation.Here, BLIP3-o-8B* indicates that the model is\ntrained with 30 million additional proprietary data samples. We highlight the best results in bold.\nType Method Global Entity Attribute Relation Other Overall \u2191\nGen. Only\nSDv1.5 [64] 74.63 74.23 75.39 73.49 67.81 63.18\nPixArt-\u03b1 [65] 74.97 79.32 78.60 82.57 76.96 71.11\nLumina-Next [12] 82.82 88.65 86.44 80.53 81.82 74.63\nSDXL [11] 83.27 82.43 80.91 86.76 80.41 74.65\nPlayground v2.5 [69] 83.06 82.59 81.20 84.08 83.50 75.47\nHunyuan-DiT [42] 84.59 80.59 88.01 74.36 86.41 78.87\nPixArt-\u03a3 [13] 86.89 82.89 88.94 86.59 87.68 80.54\nEmu3-Gen [66] 85.21 86.68 86.84 90.22 83.15 80.60\nDALL-E 3 [16] 90.97 89.61 88.39 90.58 89.83 83.50\nSD3-Medium [45] 87.90 91.01 88.83 80.70 88.68 84.08\nUnified\nShow-o [21] - - - - - 67.27\nJanus [22] 82.33 87.38 87.70 85.46 86.41 79.68\nJanus-Pro-1B [23] 87.58 88.63 88.17 88.98 88.30 82.63\nJanus-Pro-7B [23] 86.90 88.90 89.40 89.32 89.48 84.19\nMetaQuery-B [31] - - - - - 80.04\nMetaQuery-L [31] - - - - - 81.10\nMetaQuery-XL [31] - - - - - 82.05\nBLIP3-o-4B [29] - - - - - 79.36\nBLIP3-o-8B [29] - - - - - 80.73\nBLIP3-o-8B* [29] - - - - - 81.60\nOpenUni-B-512 85.87 87.33 86.54 86.91 89.43 80.29\nOpenUni-L-512 81.37 87.67 88.64 88.18 89.77 81.54\nOpenUni-L-1024 87.01 90.02 89.63 90.28 88.62 83.08\nTable 5: Results from the WISE benchmark evaluating world knowledge in text-to-image generation. Here, BLIP3-o-\n8B* indicates the model that is trained with an additional 30 million proprietary data. We highlight the best results in\nbold.\nType Method Cultural Time Space Biology Physics Chemistry Overall \u2191\nGen. Only\nSDv1.5 [64] 0.34 0.35 0.32 0.28 0.29 0.21 0.32\nSDv2.1 [64] 0.30 0.38 0.35 0.33 0.34 0.21 0.32\nEmu3-Gen [66] 0.34 0.45 0.48 0.41 0.45 0.27 0.39\nFLUX.1-schnell [47] 0.39 0.44 0.50 0.31 0.44 0.26 0.40\nSD3-Medium [45] 0.42 0.44 0.48 0.39 0.47 0.29 0.42\nSDXL [11] 0.43 0.48 0.47 0.44 0.45 0.27 0.43\nSD3.5-Large [45] 0.44 0.50 0.58 0.44 0.52 0.31 0.46\nPixArt-\u03b1 [65] 0.45 0.50 0.48 0.49 0.56 0.34 0.47\nFLUX.1-dev [47] 0.48 0.58 0.62 0.42 0.51 0.35 0.50\nUnified\nShow-o [21] 0.28 0.40 0.48 0.30 0.46 0.30 0.35\nJanus [22] 0.16 0.26 0.35 0.28 0.30 0.14 0.23\nJanus-Pro-1.5B [23] 0.20 0.28 0.45 0.24 0.32 0.16 0.26\nMetaQuery-B [31] 0.44 0.49 0.58 0.41 0.49 0.34 0.46\nMetaQuery-L [31] 0.56 0.57 0.62 0.48 0.63 0.42 0.55\nMetaQuery-XL [31] 0.56 0.55 0.62 0.49 0.63 0.41 0.55\nHarmon-1.5B [30] 0.38 0.48 0.52 0.37 0.44 0.29 0.41\nBLIP3-o-4B [29] - - - - - - 0.50\nBLIP3-o-8B [29] - - - - - - 0.52\nBLIP3-o-8B* [29] - - - - - - 0.62\nOpenUni-B-512 0.37 0.45 0.58 0.39 0.50 0.30 0.43\nOpenUni-L-512 0.51 0.49 0.64 0.48 0.63 0.35 0.52\nOpenUni-L-1024 0.49 0.53 0.69 0.49 0.56 0.39 0.52\n5Table 6: Results on image understanding benchmarks. Since the parameters of the MLLM (InternVL3 [37]) are frozen,\nOpenUni preserves its excellent performance on the following benchmarks. We highlight the best results in bold.\nModel MMBench SEED MM-Vet MME-P MMMU RWQA TEXTVQA POPE\nEMU2 Chat [26] - 62.8 48.5 - 34.1 - 66.6 -\nChameleon-7B [19] 19.8 27.2 8.3 202.7 22.4 39.0 0.0 -\nChameleon-34B [19] 32.7 - 9.7 604.5 38.8 39.2 0.0 -\nSeed-X [67] 70.1 66.5 43.0 1457.0 35.6 - - -\nVILA-U [20] - 59.0 33.5 1401.8 - 46.6 48.3 85.8\nLMFusion [54] 72.1 63.7 - 1603.7 41.7 60.0 - -\nShow-o-512 [21] - - - 1097.2 26.7 - - 73.8\nEMU3 [66] 58.5 68.2 37.2 - 31.6 57.4 64.7 85.2\nMetaMorph [28] 75.2 71.8 - - - 58.3 60.5 -\nTokenFlow-XL [73] 76.8 72.6 48.2 1551.1 43.2 56.6 77.6 86.8\nJanus-1.3B [22] 69.4 63.7 34.3 1338.0 30.5 - - 87.0\nJanus-Pro-7B [23] 79.2 72.1 50.0 1567.1 41.0 - - -\nHarmon-0.5B [30] 59.8 62.5 - 1148.0 34.2 - - 86.5\nHarmon-1.5B [30] 65.5 67.1 - 1155.0 38.9 - - 87.6\nMetaQuery-B [31] 58.5 66.6 29.1 1238.0 31.4 - - -\nMetaQuery-L [31] 78.6 73.8 63.2 1574.3 53.1 - - -\nMetaQuery-XL [31] 83.5 76.9 66.6 1685.2 58.6 - - -\nBLIP3-O-4B [29] 78.6 73.8 60.1 1527.7 46.6 60.4 78.0 -\nBLIP3-O-8B [29] 83.5 77.5 66.6 1682.6 50.6 69.0 83.1 -\nOpenUni-B (InternVL3-1B [37]) 72.6 58.2 59.5 1934.4 43.4 58.2 74.1 90.7\nOpenUni-L (InternVL3-2B [37]) 81.1 64.6 62.2 2221.2 48.6 64.3 77.0 89.6\n4.1 Image Generation\nTo assess the text-to-image generation capabilities of OpenUni, we employ a range of established benchmarks focusing\non prompt adherence, semantic alignment, and world knowledge. Specifically, GenEval [70] is employed to evaluate\nthe model\u2019s proficiency in following complex textual prompts, focusing on generating images with correct object\nattributes, counts, positions, and colors. Results are reported across various categories such as single object, multiple\nobjects, counting, colors, and position. We also use DPG-Bench [71], a benchmark designed to examine the intricate\nsemantic alignment capabilities of text-to-image models using lengthy and dense prompts. For DPG-bench, we report\nscores across its defined categories (Global, Entity, Attribute, Relation, Other) and the overall score. Finally,WISE [72]\nis employed to evaluate the model\u2019s incorporated world knowledge and reasoning capability within the context of image\ngeneration.\nThe performance of OpenUni on the GenEval benchmark is presented in Table 3. It is remarkable that our smallest\nvariant OpenUni-B-512 (0.84) already matches the performance of larger models like MetaQuery-XL (0.80) and BLIP3-\no-8B (0.84). OpenUni-L-1024 archives an overall score of 0.86. Table 4 summarizes the performance of OpenUni on\nthe DPG-Bench benchmark. On DPG-Bench, OpenUni-L-1024 obtains an overall score of 83.08, surpassing all model\nvariants of MetaQuery and BLIP3-o while being comparable to Janus-Pro-7B. For world knowledge evaluation on the\nWISE benchmark (Table 5), OpenUni-L-512/1024 achieves 0.52, already matching the performance of BLIP3-o-8B\ntrained on 30M public data. Finally, we visualize the image generation results in Figure 2.\n4.2 Multimodal Understanding\nSince the frozen InternVL3 [37] models are used to build OpenUni, their core understanding capabilities are primarily\ninherited. We summarize their performance on several standard multimodal understanding benchmarks and compare\nwith mainstream unified models. The reported benchmarks include MMBench, SEED-Bench, MM-Vet, MME-\nPerception (MME-P), MMMU, RealWorldQA (RWQA) and TextVQA. Here, we choose MMBench for evaluating\ndiverse tasks requiring perception and reasoning; SEED-Bench for assessing generative comprehension; MM-Vet\nfor evaluating integrated capabilities of large multimodal models; MME-Perception (MME-P) as a comprehensive\nbenchmark for perception capabilities; MMMU for massive multi-discipline multimodal understanding and reasoning;\nRealWorldQA (RWQA) for assessing performance on real-world question answering; TextVQA which focuses on\nvisual question answering where answers are present as text in the image; and POPE for evaluating object hallucination.\nAs shown in Table 6, OpenUni achieves competitive performance on these established benchmarks with only 1B and\n2B activated parameters, thanks to InternVL3\u2019s outstanding visual perception and reasoning ability. Finally, we show\nsome examples of OpenUni-L performing image understanding tasks in Figure 3.\n6A tiny mouse wearing glasses \nreading a book under a lamp.\nA stack of pancakes with \nbutter and maple syrup on a \nwooden table.\nA scenic view of a snowy \nmountain range with a cabin \nin the foreground.\nPaper artwork, layered paper, \ncolorful Chinesedragon \nsurrounded by clouds.\nA close-up of a person's face.\nA cat holding a board, the \nboard is written with S LAB. \nThe SLAB is four letters.\nA fox wearing a suit and tie \nreading a newspaper at a caf\u00e9.\nA pirate ship sailing under a \nblood moon with bats flying \noverhead.\nHyper-detailed macro \nphotograph of a mechanical \nhummingbird crafted from \ngold filigree and sapphire \ngears, sipping nectar from a \nchrome rose; studio lighting, \n200 mm macro lens, razor-\nsharp focus with creamy \nbokeh.\nPost-apocalyptic Paris where \nthe Eiffel Tower is encased in \ncrystalline ice, northern-lights \nsky overhead, lone figure in \nred coat crossing frozen Seine; \nphotoreal Octane render, 8-K \ntexture detail.\nPortrait of a noble samurai \nandroid wearing lacquered \ncarbon-fiber armor and \ncherry-blossom patterns; \nRembrandt lighting, 50 mm \nf/1.2, hyperreal pores and \nbrushed metal textures.\nA renaissance-style oil portrait \nof a female astronaut wearing \na richly ornate baroque \nspacesuit; deep chiaroscuro \nbackground with Rembrandt \nlighting, painterly brushwork \nbut 32-bit color depth, \ncaptured in 16-K for museum-\ngrade detail.\nFigure 2: OpenUni is able to generate high-quality images and precisely follow user instructions. The above results are\nobtained by OpenUni-L-1024.\n5 Discussion & Limitations\nIn this work, we have introduced OpenUni, a simple but strong baseline for building unified models upon existing\nmultimodal LLMs. As an ongoing project, OpenUni has the following limitations that will be addressed in future\nworks: 1) the current OpenUni models struggle to render texts in generated images; 2) our largest model is based on a\n2B LLM and 1.6B diffusion model. We believe scaling up the model sizes would further improve both understanding\nand generation performance of OpenUni; 3) image-to-image generation tasks (e.g., reconstruction and editing) are left\nfor future updates.\n7User: What is the time now?\nModel Output: The time on the \nclock is approximately \n10:10.\nUser: What main text are there in the \nimage?\nModel Output: The main text in the \nimage is \"Serving Soul since + \nTwenty Twelve.\"\nUser: Describe the image in detail.\nModel Output: In the image, \na young child is sitting \non a wooden deck, reaching out to feed a \ncapybara. The child is wearing a gray \nlong-sleeve shirt and blue jeans. The \ncapybara, a large rodent, is standing on \nthe deck and eating food, which includes \npieces of fruit. The child is holding an \norange stick, possibly to offer the \ncapybara more food. In the background, \nthere are people standing, and part of a \nmodern black structure is visible. The \nscene appears to be outdoors, likely in a \npark or zoo setting.\nUser: Where is this scene? Describe the scene in detail.\nModel Output: This scene \nis from Singapore, specifical-\nly showcasing the iconic \nMarina Bay Sands and \nGardens by the Bay. Here's a \ndetailed description:\n1. **Marina Bay Sands**: The three tall towers connected by \na horizontal structure on top are part of Marina Bay Sands. \nThis is a well-known landmark in Singapore, known for its \nunique design and the observation deck on the top.\n2. **Gardens by the Bay**: To the left of Marina Bay Sands, \nyou can see the distinctive, flower-shaped structures known \nas the Flower Dome and Cloud Forest. These are part of the \nGardens by the Bay, a major horticultural attraction in \nSingapore.\n3. **Waterfront and Bridges**: The area is surrounded by \nwater, with a bridge visible in the foreground. This is part of \nthe Marina Bay area, which is known for its extensive \nwaterfront and transportation infrastructure.\n4. **Skyline and Background**: The background shows a \nview of the sea with several ships, indicating the proximity to \nthe shipping lanes and ports of Singapore. The sky has a \nwarm, golden hue, suggesting it is either sunrise or sunset.\n5. **Urban Layout**: The layout includes modern buildings, \ngreen spaces, and a mix of commercial and recreational areas, \ntypical of Singapore's urban planning.\nThis scene captures the essence of Singapore's modern \nskyline and its blend of natural beauty and architectural \nmarvels.\nUser: What is the third benefit?\nModel Output: The third benefit is \n\"Enhances Writing Capability.\"\nFigure 3: OpenUni has inherited InternVL3 [ 37]\u2019s excellent multimodal understanding ability to recognize visual\npatterns and comprehend general world knowledge. The results are obtained by OpenUni-L (based on InternVL3-2B).\n8References\n[1] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models, 2023.\n[2] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n[3] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296\u201326306,\n2024.\n[4] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu,\nand Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024.\n[5] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li,\nPascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction\ntuning, 2023.\n[6] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei\nHuang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024.\n[7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou\nZhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24185\u201324198,\n2024.\n[8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu,\nJiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with\nopen-source suites. arXiv preprint arXiv:2404.16821, 2024.\n[9] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022.\n[10] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping\nLuo, Huchuan Lu, et al. PixArt-alpha: Fast training of diffusion transformer for photorealistic text-to-image\nsynthesis. arXiv preprint arXiv:2310.00426, 2023.\n[11] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin\nRombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024.\n[12] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun\nWang, Zhanyu Ma, et al. Lumina-Next: Making Lumina-T2X stronger and faster with Next-DiT. arXiv preprint\narXiv:2406.18583, 2024.\n[13] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan\nLu, and Zhenguo Li. PixArt-Sigma: Weak-to-strong training of diffusion transformer for 4K text-to-image\ngeneration. arXiv preprint arXiv:2403.04692, 2024.\n[14] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821\u20138831.\nPmlr, 2021.\n[15] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n[16] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf, 2(3):8, 2023.\n[17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[18] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila\nWelihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.\n[19] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818,\n2024.\n[20] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie,\nHongxu Yin, Li Yi, et al. Vila-u: a unified foundation model integrating visual understanding and generation.\narXiv preprint arXiv:2409.04429, 2024.\n9[21] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu,\nZhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal\nunderstanding and generation. arXiv preprint arXiv:2408.12528, 2024.\n[22] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie,\nXingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and\ngeneration. arXiv preprint arXiv:2410.13848, 2024.\n[23] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan.\nJanus-pro: Unified multimodal understanding and generation with data and model scaling, 2025.\n[24] Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang,\nHongsheng Li, Lewei Lu, et al. Synergen-vl: Towards synergistic image understanding and generation with vision\nexperts and token folding. arXiv preprint arXiv:2412.09604, 2024.\n[25] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan\nNie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv\npreprint arXiv:2505.14683, 2025.\n[26] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu,\nTiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14398\u201314409, 2024.\n[27] Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, and Hang Xu.\nIllume: Illuminating your llms to see, draw, and self-enhance. arXiv preprint arXiv:2412.06673, 2024.\n[28] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann\nLeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction\ntuning. arXiv preprint arXiv:2412.14164, 2024.\n[29] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining\nXie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: A family of fully open unified multimodal\nmodels-architecture, training and dataset, 2025.\n[30] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change\nLoy. Harmonizing visual representations for unified multimodal understanding and generation. arXiv preprint\narXiv:2503.21979, 2025.\n[31] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang\nXu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with\nmetaqueries, 2025.\n[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models, 2023.\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision, 2021.\n[34] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-\ntraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975\u201311986,\n2023.\n[35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[37] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie\nSu, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang,\nHaomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo,\nYi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu,\nPeng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang,\nMin Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3:\nExploring advanced training and test-time recipes for open-source multimodal models, 2025.\n10[38] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang,\nWenbin Ge, et al. Qwen2-vl: Enhancing vision-language model\u2019s perception of the world at any resolution. arXiv\npreprint arXiv:2409.12191, 2024.\n[39] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu,\nand Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint\narXiv:2403.18814, 2024.\n[40] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue\nWu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal\nunderstanding. arXiv preprint arXiv:2412.10302, 2024.\n[41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 4195\u20134205, 2023.\n[42] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao\nLiu, Minbin Huang, Zedong Xiao, et al. Hunyuan-DiT: A powerful multi-resolution diffusion transformer with\nfine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024.\n[43] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems, 33:6840\u20136851, 2020.\n[44] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international\nconference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.\n[45] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin,\nYannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024.\n[46] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu,\nYao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformers, 2024.\n[47] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024.\n[48] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit:\nExploring flow and diffusion-based generative models wfith scalable interpolant transformers. In European\nConference on Computer Vision, pages 23\u201340. Springer, 2024.\n[49] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun\nWang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint\narXiv:2406.18583, 2024.\n[50] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang,\nMuyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear\ndiffusion transformer. arXiv preprint arXiv:2501.18427, 2025.\n[51] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han.\nDeep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733,\n2024.\n[52] Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han. Efficientvit: Lightweight multi-scale attention for\nhigh-resolution dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 17302\u201317313, 2023.\n[53] Size Wu, Sheng Jin, Wenwei Zhang, Lumin Xu, Wentao Liu, Wei Li, and Chen Change Loy. F-lmm: Grounding\nfrozen large multimodal models. arXiv preprint arXiv:2406.05821, 2024.\n[54] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu.\nLlamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188,\n2024.\n[55] Mustafa Shukor, Enrico Fini, Victor Guilherme Turrisi da Costa, Matthieu Cord, Joshua Susskind, and Alaaeldin\nEl-Nouby. Scaling laws for native multimodal models. arXiv preprint arXiv:2504.07951, 2025.\n[56] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang,\nJun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.\n[57] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n[58] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,\n2017.\n11[59] Jacky He and contributors. text-to-image-2M: A high-quality, diverse text\u2013image training dataset. https:\n//huggingface.co/datasets/jackyhate/text-to-image-2M , 2024.\n[60] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for\ntraining next generation image-text models. Advances in neural information processing systems, 35:25278\u201325294,\n2022.\n[61] Ollin Matsubara and Draw Things AI Team. Megalith-10M: A dataset of 10 million public-domain photographs.\nhttps://huggingface.co/datasets/madebyollin/megalith-10m, 2024. CC0/Flickr-Commons images;\nFlorence-2 captions available in the *megalith-10m-florence2* variant.\n[62] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. RedCaps: Web-curated image\u2013text data created by\nthe people, for the people. In NeurIPS, Datasets and Benchmarks Track, 2021.\n[63] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive\nmodel beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024.\n[64] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models, 2022.\n[65] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping\nLuo, Huchuan Lu, et al. Pixart- alpha: Fast training of diffusion transformer for photorealistic text-to-image\nsynthesis. arXiv preprint arXiv:2310.00426, 2023.\n[66] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze\nWang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869,\n2024.\n[67] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan.\nSeed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint\narXiv:2404.14396, 2024.\n[68] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with\nringattention. arXiv preprint arXiv:2402.08268, 2024.\n[69] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three\ninsights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024.\n[70] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating\ntext-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024.\n[71] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for\nenhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024.\n[72] Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and\nLi Yuan. Wise: A world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint\narXiv:2503.07265, 2025.\n[73] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K Du, Zehuan Yuan, and\nXinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint\narXiv:2412.03069, 2024.\n12",
      "arXiv:2505.23234v1  [hep-ph]  29 May 2025\nEffects of bottom quark induced processes on\npolarized W+W\u2212 production at the LHC at NLO\nDuc Ninh Le, Thi Nhung Dao\nFaculty of Fundamental Sciences, PHENIKAA University, Hanoi 12116, Vietnam\nE-mail: ninh.leduc@phenikaa-uni.edu.vn\nAbstract. In this report we discuss the definition of the polarized cross sections of the inclusive\nW+W\u2212 production at the LHC. Results at the level of next-to-leading order (NLO) QCD+EW\naccuracy, published in our recent paper, are presented to highlight the effects of bottom-quark\ninduced processes. Compared to the unpolarized case, the bottom-induced effects after the\nsubtraction of the on-shell top-quark contribution are more sizable for the doubly-longitudinal\npolarization.\n1. Introduction\nProton-proton collisions at the LHC provide us valuable information into the world of\nfundamental particles. A lot of data have been collected and stored, and more are being produced\nwith the current LHC run-3, which is scheduled until June 2026. The current colliding energy\nof the LHC is 13 .6 TeV, increased from the 13 TeV value of run-2. These data can be used to\nexplore the limits of the Standard Model (SM), which is based on the local gauge symmetry of\nSU (3)C \u2297 SU (2)L \u2297 U(1)Y and the electroweak spontaneous symmetry breaking mechanism.\nAmong many different kinds of measurements, polarization measurements of massive gauge\nboson pair production at the LHC have recently attracted a significant amount of attention.\nATLAS and CMS have published results of polarized diboson production in [1, 2] ( W Z) and [3]\n(ZZ), and [4] (same-sign W W jj). These efforts help us to explore unknown regions of particle\nphysics, including: (i) the on-shell and off-shell boundary, (ii) different polarization modes and\ntheir interference.\nDifferently from the unpolarized case, polarization measurements must somehow separate the\npolarized cross sections from the unpolarized cross section. As the detectors can only provide\ninformation of the decay products, an additional step must be done to define polarizations in\nthe measurement. This definition must give a concrete result for kinematic distributions of the\ndecay products. This set of polarized distributions (named polarization templates), which can\nonly be produced by simulation (e.g. using the SM), is the definition of the polarizations. If all\nunpolarized differential cross section measurements can be fitted by a sum of those polarization\ntemplates (including a template for their interference), then we say that the polarization model\nis correct and the polarization fractions ( \u03c3pol-i/\u03c3unpol) can be extracted from a fit to a chosen\ndistribution (in principle, \u201cany\u201d kinematic distribution can be chosen, but some give better\nresults than others due to different discrimination power, e.g. a boosted-decision-tree score\ndistribution was chosen in [3]. This step can be done for the whole fiducial phase space or bin-\nwise.). In reality, off-shell effects are included in the unpolarized cross section measurements.Therefore, these effects must be accounted for by using a separate template or including them\nin the background as done in [3].\nIt then becomes clear that polarization templates are one of the most important information\nin a polarization measurement. Ideally, these templates should be published together with the\npaper, or at least a description of them should be provided.\nIt is also clear that polarization templates depend on the theoretical model used in the\nsimulation and the precision of the cross section calculation. The precision level of the simulation\nchanges with time. This is of course true for any measurements (e.g. the background estimation\nusing simulation changes with new updates), but the impact on polarization measurements may\nbe greater than on the unpolarized cases.\nAs parton showers are expected to be different for different polarized V Vmodes, these effects\nhave to be calculated separately for each polarization template. Work in this direction has been\ndone, see [5, 6, 7]. Hadronization is also different for different polarizations, this effect is however\nexpected to be much smaller than those of parton showers.\nKinematic cuts affect polarization states differently. To allow for future (re-)analyses with\ndifferent lepton kinematic cuts, a good strategy is to generate events for different polarization\nstates with an inclusive cut setup (e.g. including only m4l > MV + MV \u2032 cut) and store these\nevents. At the analysis level, analysis cuts are then applied to these inclusive polarized events to\nproduce fiducial-level templates to be used in the final fit. The extracted polarization fractions\ndepend on the analysis cuts as the cuts affect \u03c3pol-i and \u03c3unpol differently.\nSince parton showers can only improve results in the soft and collinear regions, fixed order\ncalculations are needed for a better description of the hard regions. Next-to-leading order (NLO)\nand beyond calculations of the polarized cross sections are therefore important. Progress has\nbeen made in this direction as well.\nWe mention here the recent development in the calculation of higher-order corrections to\nthe polarized V V production cross sections with fully leptonic decays. The first NLO QCD\ncorrections were done in [8] for the W+W\u2212 case. These results were soon extended to the ZZ\ncase [9], including the NLO electroweak (EW) corrections this time. This ZZ work is important\nas it provides new on-shell mappings (see Section 2) 1 \u2192 2 and 1 \u2192 3, which are crucial for\nthe definition of the polarization templates. The 1 \u2192 2 mapping is used for the leading order\n(LO) V decays, while the 1 \u2192 3 mapping is for the NLO V decays occurring in the NLO EW\ncorrections. Using these new on-shell mappings, the NLO EW corrections for the W Zcase\nwere completed in [10, 11]. This is a major improvement compared to the ZZ case because\nthe W Zproduction involves an on-shell charged current where a photon can be radiated off\nthe intermediate on-shell W. This radiation introduces soft divergences which must be canceled\nwith the counter parts in the virtual corrections of theW Zproduction and W decay amplitudes.\nThe W Zcalculation was then straightforwardly extended to the W+W\u2212 case in [12, 13]. A\nfurther step was recently done for the same-sign W+W+jj production [14], where final-state\nphoton radiation is more complicated than the inclusive W+W\u2212 case as interferences between\njet-photon radiation amplitudes and lepton-photon radiation amplitudes occur. Treatment of\nthese radiations requires modifications of the decay momenta. These modifications interplay\nwith the on-shell mappings, therefore require careful attention. Moreover, the important next-\nto-next-to-leading order (NNLO) QCD corrections for the polarized W+W\u2212 production were\ncalculated in [15]. The case of semi-leptonic decays has been considered in [16].\nIn this report, which is a contribution to the proceedings of the PIAS workshop 2024:\nPhysics at different scales (Hanoi, Vietnam, 2024), we consider the case of inclusive\nW+W\u2212 production and focus on the definition of the polarized W+W\u2212 pair production. This\nis the signal part. The definition used in this work is in accordance with the common definition\nof the unpolarized W+W\u2212 pair production used in the literature and in the experimental\nmeasurements. This definition requires the subtraction of the top-quark contribution. Thissubtraction has to be done for individual polarizations and has been recently achieved by us\nin [17] at NLO QCD+EW accuracy. The presentation presented here is based on this result.\nIn addition, we take this opportunity to discuss in more detail the definition of the inclusive\npolarized W+W\u2212 production signal. This discussion is provided in Section 2. Important\nnumerical results are then presented in Section 3. Finally, a short conclusion is provided in\nSection 4.\n2. Method of calculation\nThe signal process is\np(k1) + p(k2) \u2192 W+(q1)W\u2212(q2) \u2192 e+(k3) + \u03bde(k4) + \u00b5\u2212(k5) + \u00af\u03bd\u00b5(k6), (1)\nwhere the final-state leptons are the decay product of the intermediate W+W\u2212 system. This is\nillustrated by a Feynman diagram in Fig. 1.\nFigure 1. Tree-level Feynman diagrams describing a W+W\u2212 resonant signal process at the\nLHC.\nA W boson can decay into leptons (33%) or quarks (67%). The hadronic final state has a\nhigher rate, but suffers from a much larger background than the leptonic mode. We therefore\nonly consider the fully leptonic decays of the W bosons in this work.\nThe two intermediate W bosons are required to be on-shell, namely their momenta satisfying\nq2\ni = M2\nW with i = 1, 2. This condition is crucial for the separation of the different polarization\ncontributions, because gauge invariance is guaranteed in this on-shell limit.\nTo make this on-shell phase space available, the invariant mass of the final-state leptons\nmust be greater than 2 MW . It is therefore important that this condition is also satisfied in the\nexperimental analysis.\nIn simulation, the different polarizations of the W+W\u2212 pair can be separated using the\ndouble-pole approximation (DPA). This technique has been used for theZZ [9], W Z[18, 10, 11],\nand W+W\u2212 [8, 12, 13, 17] processes up to NLO QCD+EW level. For theW+W\u2212 case, an NNLO\nQCD calculation has been performed [15].\nThe idea of the DPA can be understood by considering the unpolarized LO DPA amplitude:\nA\u00afqq\u2192V1V2\u21924l\nDPA = 1\nQ1Q2\n3X\n\u03bb1,\u03bb2=1\nA\u00afqq\u2192V1V2\nLO (\u02c6ki, \u03bb1, \u03bb2)AV1\u2192l1l2\nLO (\u02c6ki, \u03bb1)AV2\u2192l3l4\nLO (\u02c6ki, \u03bb2), (2)\nwith\nQj = q2\nj \u2212 M2\nVj + iMVj \u0393Vj (j = 1, 2), (3)\nwhere MVj and \u0393Vj are the mass and the total decay width of the W bosons.Compared to the full \u00af qq \u2192 4l amplitudes, contributions from the non-doubly-resonant\ndiagrams have been omitted. Another notable difference is the application of the on-shell phase\nspace mapping\non-shell mapping: [ ki] \u2192 [\u02c6ki] (4)\nwith i = 1, 6 for all external-state particles. ki are the off-shell momenta. The mapped (or\non-shell) momenta \u02c6ki are all massless and satisfy the on-shell condition:\n\u02c6q2\n1 = (\u02c6k3 + \u02c6k4)2 = M2\nW , \u02c6q2\n2 = (\u02c6k5 + \u02c6k6)2 = M2\nW . (5)\nSolution for these on-shell momenta can be found if the off-shell momenta satisfy:\n(k3 + k4 + k5 + k6)2 \u2265 4M2\nW , (6)\nas above mentioned.\nThe result of Eq. (2) is obtained from the key observation that the on-shell production\namplitudes A\u00afqq\u2192V1V2 and on-shell decay amplitudes AVi\u2192ll\u2032 are individually gauge invariant.\nThis is the reason for the application of the on-shell mapping. The DPA result of Eq. (2) can be\nthought of as an on-shell projection of an off-shell amplitude. This on-shell projection includes\nthe following steps:\n\u2022 Select only the V1V2 doubly-resonant Feynman diagrams.\n\u2022 Replace the intermediate Vi propagators using:\nVi propagator: Pi = \u2212ig\u00b5\u03bd\nQi\n\u2192 i\nQi\n3X\n\u03bb=1\n\u03b5\u00b5\u2217\n\u03bb (qi)\u03b5\u03bd\n\u03bb(qi), (7)\ng\u00b5\u03bd = \u2212\n3X\n\u03bb=1\n\u03b5\u00b5\u2217\n\u03bb (k)\u03b5\u03bd\n\u03bb(k) + k\u00b5k\u03bd\nM2\nV\n, k = qi, (8)\nwhere the \u2018t Hooft-Feynman gauge has been used for simplicity. We can choose another\ngauge which can introduce a new term proportional to q\u00b5\ni q\u03bd\ni into Pi. This additional\nterm cancels against the corresponding would-be Goldstone boson contribution. For the\nsame reason, the last term proportional to k\u00b5k\u03bd in Eq. (8) can be safely removed. This\ncancellation is expected to occur at any order in a perturbation theory (see e.g. [14]).\n\u2022 Perform the on-shell momenta mapping for the production amplitudes and decay\namplitudes.\nEq. (2) provides a simple way to separate polarizations of the intermediate W bosons. We\njust need to select the desired ( \u03bb1, \u03bb2) combination from the DPA amplitude. Specifically, we\nhave\nALL = ADPA(\u03bb1 = 2, \u03bb2 = 2),\nALT = ADPA(\u03bb1 = 2, \u03bb2 = 1) + ADPA(\u03bb1 = 2, \u03bb2 = 3),\nATL = ADPA(\u03bb1 = 1, \u03bb2 = 2) + ADPA(\u03bb1 = 3, \u03bb2 = 2),\nATT = ADPA(\u03bb1 = 1, \u03bb2 = 1) + ADPA(\u03bb1 = 1, \u03bb2 = 3)\n+ ADPA(\u03bb1 = 3, \u03bb2 = 1) + ADPA(\u03bb1 = 3, \u03bb2 = 3). (9)\nFrom this we see that the LT contribution is a coherent sum of the (2 , 1) and (2 , 3) polarized\namplitudes. Similarly can be said for the TL and TT polarizations. The unpolarized amplitude\nreads:\nAunpol = ALL + ALT + ATL + ATT, (10)which gives\n\u03c3unpol = \u03c3LL + \u03c3LT + \u03c3TL + \u03c3TT + \u03c3pol-int, (11)\nwhere the last term is called the polarization interference. This interference is the interference\nbetween the LL, LT, TL, and TT modes. This interference vanishes for the integrated cross\nsection in the case of fully inclusive decay products, namely the momenta of the decay products\nare not restricted [8]. If a particular distribution (e.g. pT,\u2113) is considered, then the interference\neffect will show up at a given bin, even for a fully inclusive analysis. This is because the cross\nsection of a specific bin is no longer inclusive.\nThe above on-shell projection has been extended up to NLO EW level in [9, 11, 12, 13, 17]\nfor the inclusive diboson case and in [14] for the W+W+jj case. The case of QCD corrections\nis simpler as only radiative corrections to the intitial-state particles are involved. NLO EW\ncorrections are more complicated because radiative corrections occur for both initial and final\nstate particles. Moreover, as the intermediate W bosons are on-shell soft divergences occur when\na real or virtual photon is emitted from these W bosons. These divergences together with soft\nand collinear divergences from the initial and final state radiation have to be properly treated\nso that infrared-safe observables are obtained (see e.g. [19]). In this work, we have used the\ndipole subtraction method [19, 20] to deal with the infrared (i.e. soft or collinear) divergences.\nMore details are provided in [11, 13, 17].\nThe polarized W+W\u2212 process was first calculated in [12, 13] at NLO EW using the four-flavor\nscheme, i.e. the bottom-quark induced processes were excluded. The main reason for this choice\nis to exclude the top-quark contribution which occurs starting from NLO in both QCD and EW\ncorrections. At NLO, the processes of gb \u2192 tW \u2192 W+W\u2212b (QCD) and \u03b3b \u2192 tW \u2192 W+W\u2212b\n(EW) occur. At NNLO, gg \u2192 t\u00aft \u2192 W+W\u2212bb and q\u00afq \u2192 t\u00aft \u2192 W+W\u2212bb processes happen.\nThey all produce polarized W+W\u2212 pairs. These contributions are large due to the top-quark\nresonances.\nThe definition of the inclusive W+W\u2212 production process is\npp \u2192 W+W\u2212 + N jets + M photons + X, (12)\nwith N \u2265 0, M \u2265 0. A jet here can include all partons except the top quark. This definition\nincludes the default q\u00afq/gg \u2192 W+W\u2212 mechanisms as well as the above top-quark induced ones.\nThe t\u00aft contribution is completely dominant due to its large cross section at the LHC.\nIn measurements, the top-quark contribution can be suppressed using jet cuts (e.g. pT\njet-veto =\n35 GeV) and then subtracted in the signal region using a partly data-driven method [21]. The\nremaining contribution is the default q\u00afq/gg \u2192 W+W\u2212 mechanisms, which are widely known in\nthe community as the W+W\u2212 production signal.\nFor the SM prediction, the same procedure can be performed if the inclusive cross section\nof Eq. (12) is accurately known. The top-quark background contribution must be separately\ncalculated and then subtracted from the inclusive result. In this way, the top-quark interference\ncontribution is part of the signal and is consistent with the measurement. This definition of the\nW+W\u2212 signal in the five-flavor scheme (5FS) is discussed in detail in [22].\nFrom a precision-calculation point of view, there is a good approach called the four-flavor\nscheme (4FS). In this scheme, the bottom quark is massive, and hence bottom jet is excluded\nin the inclusive cross section definition (see Eq. (12)). Bottom PDF is set to zero, hence no\nbottom quark in the initial state. This 4FS definition of the W+W\u2212 signal is free of the\ntop-quark contribution by definition. The calculation is easier because the masses of the light\nquarks are set to zero, allowing for higher perturbative order computation. However, there is a\nmismatch between this definition and the measurement because there is still a bottom-induced\ncontribution in the measurement result after the subtraction of the top-quark contribution.(a)\nb\ne+\ne\nW+\nW\u2212\nb\nt\n(b)\nb\nb\nt\ng\ng\nb b\ng g\nt\nb\nt\nbt\nW+\nW\u2212\nW+\nW\u2212\n(c) (d)\nW\u2212\nW+\nFigure 2. Various types of bottom-induced contributions to the inclusive W+W\u2212 production.\n(a,b,c) are of non- tW origin, while (d) is of tW origin. The top-quark interference (or tW\ninterference) is the interference between (c) and (d).\nMoreover, the top-quark interference (i.e the signal-background interference) is part of the signal\nin the measurement result. This term is missing in the 4FS definition.\nThis point is illustrated in Fig. 2. The mismatch between the 4FS definition and the\nmeasurement includes contribution from diagrams (a,b,c) and the interference between (c) and\n(d).\nWe can account for this mismatch by calculating the bottom induced corrections separately.\nIn this way we can make use of the precise calculation of the 4FS. This approach is well motivated\nbecause the bottom induced contribution is in most cases much smaller than the light-quark\ncontribution. Moreover, as b jets can be tagged in experimental analyses, a separate treatment\nof the b-induced contribution is a good idea.\nIn this work, we include b jets in the definition of the inclusive W+W\u2212 production.\nAccordingly, the 5FS definition is used for the signal cross section. We will follow the same\nprocedure as in the experimental measurement. Namely, the inclusive cross section including\nthe top-quark contribution will firstly be calculated. Then, the on-shell top-quark contribution\nwill be separately computed. The signal cross section is then obtained after the subtraction of\nthe on-shell top-quark contribution. This subtraction has to be done for individual polarized\ncross sections. As our calculation is limited to NLO QCD+EW, the top-quark contribution\nincludes only the tW mechanism. Details of this calculation are provided in [17].\n3. Numerical results\nIn measurements, a jet veto (applying for all partons) is usually used to reduce the top-quark\nbackgrounds, as done in the ATLAS case [21]. The problem with this choice is that using a jet\nveto increases the error of the theoretical prediction [23]. In the CMS measurement [24], a jet\nveto was used in the Sequential Cut but not in the Random Forest cut. Moreover, the number\nof b-tagged jets is required to be zero to reduce the top-quark backgrounds. In this way, jet\nveto is used only for the bottom induced contribution, thereby reducing the jet-veto error of the\ntheoretical prediction as the b-induced cross section is small in most cases.\nIn the following, we will present the results for two cut setups: YesVeto and NoVeto. The\nYesVeto setup reads\npT,\u2113 > 27 GeV, p T,miss > 20 GeV, |\u03b7\u2113| < 2.5, m e\u00b5 > 55 GeV,\njet veto (no jets with pT,j > 35 GeV and |\u03b7j| < 4.5), (13)\nwhile the NoVeto one is identical except that the jet-veto cut is removed. For the YesVeto case,\nthe b jet is treated in the same way as the gluon or light-quark jets.\nResults for the integrated cross sections are shown in Table 1 for the YesVeto and NoVeto\ncases. Unpolarized and polarized cross sections are provided for two cases: NoB, NoTW. Here\nNoB means the bottom-induced contribution is excluded. NoTW means the b-induced processesYesVeto\n\u03c3NoB [fb] \u03c3NoTW [fb] fNoB [%] fNoTW [%]\nUnpol 218.47(3)+2.2%\n\u22122.1% 220.50(3)+2.1%\n\u22122.0% 100 100\nW+\nL W\u2212\nL 14.34+1.8%\n\u22122.6% 15.59+1.2%\n\u22122.2% 6.6 7.1\nW+\nL W\u2212\nT 24.79+1.9%\n\u22122.5% 25.31+1.6%\n\u22122.5% 11.3 11.5\nW+\nT W\u2212\nL 25.47+2.1%\n\u22122.5% 25.99+1.8%\n\u22122.4% 11.7 11.8\nW+\nT W\u2212\nT 152.59(3)+2.2%\n\u22121.9% 152.67(3)+2.2%\n\u22121.9% 69.8 69.2\nPol-int 1.27(4) 0.93(4) 0.6 0.4\nNoVeto\n\u03c3NoB [fb] \u03c3NoTW [fb] fNoB [%] fNoTW [%]\n327.94(4)+5.4%\n\u22124.2% 334.17(4)+5.4%\n\u22124.1% 100 100\n18.68+4.1%\n\u22123.3% 21.04(1)+4.0%\n\u22122.9% 5.7 6.3\n43.33+6.0%\n\u22124.9% 44.86(1)+6.1%\n\u22124.8% 13.2 13.4\n44.22(1)+6.2%\n\u22124.9% 45.77(1)+6.2%\n\u22124.8% 13.5 13.7\n221.43(3)+5.3%\n\u22124.1% 222.80(3)+5.3%\n\u22124.1% 67.5 66.7\n0.28(5) \u22120.30(5) 0.1 \u22120.1\nTable 1.Results for the unpolarized and polarized DPA cross sections using the YesVeto and NoVeto\ncut setups. These results are taken from [17].\nare included and the on-shell tW contribution is subtracted. This is the signal cross section\nas measured in ATLAS and CMS. The polarization fractions defined as fi = \u03c3i/\u03c3unpol for the\npolarization i are also provided. The polarization interference is provided in the last row. The\nscale uncertainties are shown in percent as upper and lower indices on the value of the cross\nsection. These uncertainties are calculated using the direct variation method of the factorization\nand renormalization scales (seven-point method, see [17] for details). The Monte-Carlo errors\nare provided in the round parentheses if significant. We see that the b-induced contribution\nincreases the LL fraction by 7 .6% and 10.5% for the YesVeto and NoVeto cases, respectively.\nFigure 3. Differential polarized cross sections in cos \u03b8WW\ne (see text) for YesVeto and NoVeto\ncut setups. Results are taken from [17].Example results of differential cross sections are shown in Fig. 3. The kinematic variable here\nis cos\u03b8WW\ne , the electron polar angle calculated in theW Wcenter-of-mass frame. This is however\nnot a normal polar angle. \u03b8WW\ne is the angle between the spatial momentum \u20d7 pW-rest\ne determined\nin the W+-rest frame and the spatial momentum \u20d7 pWW\nW+ determined in the W Wcenter-of-mass\nframe. As the Lorentz boost order is important, we have to boost the relevant momenta to the\nW Wframe first, and then from here boost to the W+ rest frame. If we do not follow this order\nand jump directly to the W+ rest frame from e.g. the LAB frame, then the obtained \u20d7 pW-rest\ne\nwould be wrong.\nThis polar angle is of particular important in the polarization measurement. This is because\nit is sensitive to the polarization of the parent gauge boson, i.e. the W+ in this discussion. It is\nnot sensitive to the polarization of the other gauge boson. This is why we get almost identical\nnormalized shapes for the W+\nL W\u2212\nL and W+\nL W\u2212\nT cases, as can be seen from the bottom panels\nin Fig. 3. Similarly, the shapes of the W+\nT W\u2212\nL and W+\nT W\u2212\nT cases are also almost identical. The\ndistinct difference in shape between the W+\nL W\u2212\nL and W+\nT W\u2212\nL cases shows the power of this\npolar angle. Likewise, to distinguish the polarizations of the W\u2212 boson, the muon polar angle\ncos \u03b8WW\n\u00b5 should be used.\nThe three small panels below the big panel show the relative scale uncertainties of the cross\nsections provided in the big panel. The uncertainties are much smaller for the YesVeto case.\nThis is due to an accidental cancellation of different NLO corrections at the chosen value of\nthe jet-veto threshold (35 GeV), see [23]. It turns out that the cancellation is largest around\nthis value, leading to small values of the scale uncertainties. As discussed in [17], the scale\nuncertainties provided here for the YesVeto case are likely to be underestimated. Care must\ntherefore be taken when using these values.\n4. Conclusions\nIn this report, we have discussed a calculation method for the inclusive polarized W+W\u2212\nproduction signal at the LHC in the five-flavor scheme. The method is applicable at NLO\nQCD+EW level. The key idea is to calculate the on-shell top-quark contribution separately\nand subtract it from the inclusive cross section. Example results have been presented for the\nintegrated cross sections and a differential cross section for two cut setups, with and without a\njet veto.\nAcknowledgments\nThe authors would like to thank the workshop organizers for organizing this nice event. This\nwork is funded by Phenikaa University under grant number PU2023-1-A-18.\nReferences\n[1] ATLAS collaboration, G. Aad et al., Observation of gauge boson joint-polarisation states in W\u00b1Z\nproduction from pp collisions at \u221as = 13 TeV with the ATLAS detector , Phys. Lett. B 843 (2023)\n137895 [2211.09435].\n[2] ATLAS collaboration, G. Aad et al., Studies of the energy dependence of diboson polarization fractions and\nthe Radiation Amplitude Zero effect in WZ production with the ATLAS detector , 2402.16365.\n[3] ATLAS collaboration, G. Aad et al., Evidence of pair production of longitudinally polarised vector bosons\nand study of CP properties in ZZ \u2192 4\u2113 events with the ATLAS detector at \u221as = 13 TeV, JHEP 12\n(2023) 107 [2310.04350].\n[4] CMS collaboration, A. M. Sirunyan et al., Measurements of production cross sections of polarized\nsame-sign W boson pairs in association with two jets in proton-proton collisions at \u221as = 13 TeV, Phys.\nLett. B 812 (2021) 136018 [2009.09429].\n[5] M. Hoppe, M. Sch\u00a8 onherr and F. Siegert,Polarised cross sections for vector boson production with SHERPA ,\n2310.14803.\n[6] G. Pelliccioli and G. Zanderighi, Polarised-boson pairs at the LHC with NLOPS accuracy , Eur. Phys. J. C\n84 (2024) 16 [2311.05220].[7] M. Javurkova, R. Ruiz, R. C. L. de S\u00b4 a and J. Sandesara, Polarized ZZ pairs in gluon fusion and vector\nboson fusion at the LHC , Phys. Lett. B 855 (2024) 138787 [2401.17365].\n[8] A. Denner and G. Pelliccioli, Polarized electroweak bosons in W+W\u2212 production at the LHC including\nNLO QCD effects , JHEP 09 (2020) 164 [2006.14867].\n[9] A. Denner and G. Pelliccioli, NLO EW and QCD corrections to polarized ZZ production in the\nfour-charged-lepton channel at the LHC , JHEP 10 (2021) 097 [2107.06579].\n[10] D. N. Le and J. Baglio, Doubly-polarized WZ hadronic cross sections at NLO QCD + EW accuracy , Eur.\nPhys. J. C 82 (2022) 917 [2203.01470].\n[11] D. N. Le, J. Baglio and T. N. Dao, Doubly-polarized WZ hadronic production at NLO QCD+EW:\ncalculation method and further results , Eur. Phys. J. C 82 (2022) 1103 [2208.09232].\n[12] A. Denner, C. Haitz and G. Pelliccioli, NLO EW corrections to polarised W +W\u2212 production and decay at\nthe LHC, Phys. Lett. B 850 (2024) 138539 [2311.16031].\n[13] T. N. Dao and D. N. Le, NLO electroweak corrections to doubly-polarized W+W\u2212 production at the LHC ,\nEur. Phys. J. C 84 (2024) 244 [2311.17027].\n[14] A. Denner, C. Haitz and G. Pelliccioli, NLO EW and QCD corrections to polarised same-sign WW\nscattering at the LHC , JHEP 11 (2024) 115 [2409.03620].\n[15] R. Poncelet and A. Popescu, NNLO QCD study of polarised W+W\u2212 production at the LHC , JHEP 07\n(2021) 023 [2102.13583].\n[16] A. Denner, C. Haitz and G. Pelliccioli, NLO QCD corrections to polarized diboson production in\nsemileptonic final states , Phys. Rev. D 107 (2023) 053004 [2211.09040].\n[17] T. N. Dao and D. N. Le, Polarized W+W\u2212 pairs at the LHC: Effects from bottom-quark induced processes\nat NLO QCD + EW , Eur. Phys. J. C 85 (2025) 108 [2409.06396].\n[18] A. Denner and G. Pelliccioli, NLO QCD predictions for doubly-polarized WZ production at the LHC , Phys.\nLett. B 814 (2021) 136107 [2010.07149].\n[19] S. Catani and M. Seymour, A General algorithm for calculating jet cross-sections in NLO QCD , Nucl.Phys.\nB485 (1997) 291 [hep-ph/9605323].\n[20] S. Dittmaier, A General approach to photon radiation off fermions , Nucl. Phys. B565 (2000) 69\n[hep-ph/9904440].\n[21] ATLAS collaboration, M. Aaboud et al., Measurement of fiducial and differential W+W\u2212 production\ncross-sections at \u221as = 13 TeV with the ATLAS detector , Eur. Phys. J. C 79 (2019) 884 [1905.04242].\n[22] T. Gehrmann, M. Grazzini, S. Kallweit, P. Maierh\u00a8 ofer, A. von Manteuffel, S. Pozzorini et al., W+W\u2212\nProduction at Hadron Colliders in Next to Next to Leading Order QCD , Phys. Rev. Lett. 113 (2014)\n212001 [1408.5243].\n[23] I. W. Stewart and F. J. Tackmann, Theory Uncertainties for Higgs and Other Searches Using Jet Bins ,\nPhys. Rev. D 85 (2012) 034011 [1107.2117].\n[24] CMS collaboration, A. M. Sirunyan et al., W+W\u2212 boson pair production in proton-proton collisions at\u221as = 13 TeV, Phys. Rev. D 102 (2020) 092001 [2009.00119].",
      "arXiv:2505.22312v2  [cs.LG]  29 May 2025\nSkywork Open Reasoner 1 Technical Report\nJujie He\u2217,\u2020 , Jiacai Liu\u2217 , Chris Yuhao Liu , Rui Yan , Chaojie Wang , Peng Cheng , Xiaoyu\nZhang , Fuxiang Zhang , Jiacheng Xu , Wei Shen , Siyuan Li , Liang Zeng , Tianwen Wei ,\nCheng Cheng , Bo An , Yang Liu , and Yahui Zhou\nSkywork AI, Kunlun Inc\nGitHub: https://github.com/SkyworkAI/Skywork-OR1\nHuggingFace: https://huggingface.co/Skywork/Skywork-OR1-32B\nAbstract\nThe success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing\nthe reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an\neffective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average\naccuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model\nand from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both\nDeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate\ncompetitive reasoning capabilities among models of similar size. We perform comprehensive ablation\nstudies on the core components of our training pipeline to validate their effectiveness. Additionally, we\nthoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics,\nand demonstrate that mitigating premature entropy collapse is critical for improved test performance.\nTo support community research, we fully open-source our model weights, training code, and training\ndatasets.\nFigure 1: The performance curve of Skywork-OR1-32B during RL training for AIME 2024 and AIME 2025.\nThe red stars indicate the selected final checkpoints.\n\u2217Equal contribution.\n\u2020Corresponding author: jujie.he@kunlun-inc.com\n1Contents\n1 Introduction 3\n2 Preliminaries 6\n3 MAGIC in Skywork-OR1 7\n3.1 MAGIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3.2 Effectiveness of MAGIC Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.2.1 Data Mixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.2.2 Multi-Stage Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.2.3 Advantage Mask for Truncated Responses . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.2.4 High-temperature Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.2.5 Adaptive Entropy Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.2.6 No KL Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4 Empirical Studies on Mitigating Policy Entropy Collapse 18\n4.1 Ablation Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.2 Premature Entropy Collapse Generally Manifests as Worse Performance . . . . . . . . . . . . 20\n4.3 The Impact of Rollout-Diversity-Related Hyperparameters . . . . . . . . . . . . . . . . . . . . 20\n4.4 The Impact of Off-policy Update by IncreasingNSGD . . . . . . . . . . . . . . . . . . . . . . 21\n4.5 Preventing Premature Entropy Collapse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n5 Empirical Studies on Training Resource Allocation 27\n5.1 Improving Training Efficiency with Fixed Computational Resources . . . . . . . . . . . . . . . 28\n5.2 Improving Test Performance with More Computational Resources . . . . . . . . . . . . . . . . 29\n6 Dataset Preparation 30\n6.1 Data Source Selection and Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n6.2 Model-Aware Difficulty Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n6.3 Quality Assessment via Human and LLM-as-a-Judge . . . . . . . . . . . . . . . . . . . . . . . 33\n7 Math & Code Verifiers 34\n7.1 Math Verifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n7.2 Code Sandboxes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n8 Experiments 35\n8.1 Training and Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n8.2 Evaluation Results of Skywork-OR1 models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n9 Conclusion 37\n21 Introduction\nIn recent months, post-training techniques based on reinforcement learning (RL) have achieved groundbreaking\nsuccess in enhancing the reasoning capabilities of large language models (LLMs). Representative models such\nas OpenAI-o1 [9], DeepSeek-R1 [6], and Kimi-K1.5 [24] demonstrate RL\u2019s remarkable ability to significantly\nimprove performance in mathematics and coding. While prior RL approaches have primarily relied on\nMonte Carlo Tree Search (MCTS) or Process Reward Models (PRMs) to improve reasoning over supervised\nfine-tuning (SFT) models, the success of DeepSeek-R1 demonstrates conclusively that online RL with a\nsimple rule-based reward is sufficient to substantially enhance the reasoning capabilities of base models.\nAs model capabilities continue to advance, Chains-of-Thought (CoT) have grown progressively longer. For\nexample, the DeepSeek-R1-Distill model series [6] generates CoT sequences averaging over 10K tokens on\nthe AIME24 benchmark, significantly surpassing earlier popular SFT models such as the Qwen 2.5 model\nseries [33] and the Llama 3.1 model series [5]. Despite several reproduction efforts (e.g., Logic-RL [31],\nOpen-Reasoner-Zero [8], DAPO [34], VAPO [35]) following the success of DeepSeek-R1, most have focused\non applying RL to base models rather than to long CoT models that have already undergone SFT. As a\nresult, it remains unclear how to improve the reasoning abilities of long CoT models using RL in an efficient\nand scalable manner. While recent works such as DeepScaleR [17], Light-R1 [28], and DeepCoder [16] have\nmade preliminary progress toward efficient RL optimization for long CoT models, their analyses do not\nsystematically disentangle the contributions of distinct algorithmic components during RL training.\nIn this work, we introduce Skywork Open Reasoner 1 (abbreviated as Skywork-OR1 throughout the report),\nan efficient and scalable RL recipe for long CoT models. Our experiments are based on the DeepSeek-\nR1-Distill model series and open-source datasets with rigorous preprocessing and filtering. As shown in\nFigure 1 and Table 13, the Skywork-OR1 model series achieves significant performance improvements over\nbase models, demonstrating the effectiveness of our RL implementation. Specifically, Skywork-OR1-32B\nachieves scores of 82.2 on AIME24, 73.3 on AIME25, and 63.0 on LiveCodeBench[10] (2024-08 - 2025-02),\noutperforming DeepSeek-R1 and Qwen3-32B in the math domain. Skywork-OR1-7B achieves 70.2 on AIME24,\n54.6 on AIME25, and 47.6 on LiveCodeBench, exhibiting competitive performance relative to similarly sized\nmodels in both math and coding tasks. Our previously released model, Skywork-OR1-Math-7B, also delivers\nstrong performance among similarly sized models, scoring 69.8 on AIME24, 52.3 on AIME25, and 43.6 on\nLiveCodeBench. We conducted exhaustive ablation experiments to validate the effectiveness of the core\ncomponents in the training pipeline.\nBalancing exploration and exploitation is crucial in RL training [22]. We conducted a comprehensive study on\npremature entropy collapse, a phenomenon associated with excessive exploitation, and found that mitigating\npremature entropy collapse is essential for achieving better test performance. Through exhaustive ablation\nexperiments, we identified key factors that influence entropy dynamics.\nTo ensure full reproducibility and support ongoing research within the LLM community, we release all of our\ntraining resources, including source code\u2217, the post-training dataset\u2020, and model weights\u2021 \u00a7. Furthermore, we\nconducted extensive ablation studies across both data and algorithmic dimensions to elucidate effective RL\nimplementations for long CoT models. As a follow-up to our previously released Notion blog post [7], we\npresent this more detailed technical report, with our key findings summarized as follows:\n\u2217https://github.com/SkyworkAI/Skywork-OR1\n\u2020https://huggingface.co/datasets/Skywork/Skywork-OR1-RL-Data\n\u2021https://huggingface.co/Skywork/Skywork-OR1-7B\n\u00a7https://huggingface.co/Skywork/Skywork-OR1-32B\n3Data Collection\n1. To ensure stable and effective training, it is crucial to incorporate problems from a diverse set of\nsources. We observe that, in the absence of consistent quality assessment and filtering procedures,\npreviously successful datasets exhibit several failure modes with larger models (Section 6).\n2. Rigorous filtering and quality control of training data significantly accelerate learning. Our proposed\ndata mixture, constructed with stringent filtering criteria, outperforms a baseline mixture assembled\nwith looser quality thresholds (Section 3.2.1).\nTraining Strategy\n1. Multi-stage training significantly improves training efficiency in the initial phase while preserving\nscalability for later stages (Section 3.2.2).\n2. Addressing noisy training signals introduced by truncated trajectories inStage I does not lead to\nbetter scaling at large context lengths, e.g., 32K (Section 3.2.3).\n3. High-temperature sampling results in lower test accuracy during the early training steps but\nultimately yields greater performance improvements (Section 3.2.4).\n4. On-policy training mitigates entropy collapse and leads to higher test performance (Section 4).\nLoss Function\n1. Adaptive entropy control effectively keeps the model\u2019s entropy lower-bounded by the target entropy\nthroughout training, maintaining the model\u2019s exploration ability and high learning plasticity, with\ntest performance steadily improving (Section 3.2.5).\n2. The KL penalty hinders further improvements in test performance during multi-stage training.\nTherefore, we omit KL loss from our training pipeline (Section 3.2.6).\nEmpirical Results of Our Entropy Collapse Study\n1. Faster entropy collapse generally correlates with poorer test performance (Section 4.2). Appropriate\nentropy control that mitigates premature convergence can improve test outcomes (Section 4.5).\n2. Increasing rollout diversity by enlarging the batch and group sizes has only minor effects on entropy\ndynamics (Section 4.3), whereas using a higher sampling temperature significantly impacts initial\nentropy and learning dynamics (Section 3.2.4).\n3. Off-policy training \u2013 via increased mini-batches or data reuse \u2013 accelerates entropy collapse and\ngenerally leads to degraded test performance compared to on-policy updates, due to the introduction\nof off-policy data (Section 4.4).\n4. The entropy loss exhibits high sensitivity to both the training data and the coefficient. By either\nadaptively adjusting the entropy loss coefficient or applying a clip-higher trick with an appropriate\nclip ratio, entropy dynamics become slower and more stable, leading to improved test performance.\nNevertheless, entropy still converges faster than in on-policy training (Section 4.5).\nOrganization In Section 2, we introduce the preliminaries of several important policy optimization methods\nin RL. Section 3 elaborates on our training pipeline, including comprehensive ablation studies that validate\nthe effectiveness of its core components. A systematic investigation of entropy collapse is presented in Section\n4, demonstrating that mitigating premature policy convergence is critical in RL training for enhancing\nexploration and achieving better test performance. We discuss training resource allocation in Section 5. The\n4implementation details of our training data preparation and rule-based reward are provided in Sections 6 and\n7. Finally, Section 8 presents a comprehensive description of the training and evaluation details for our three\nreleased models: Skywork-OR1-Math-7B, Skywork-OR1-7B, and Skywork-OR1-32B.\nFigure 2: Performance of Skywork-OR1-32B on challenging mathematics and coding benchmarks.\nFigure 3: Performance of Skywork-OR1-7B on challenging mathematics and coding benchmarks.\n52 Preliminaries\nThe success of Deepseek-R1 demonstrates that Policy Gradient (PG) methods [22], especially Group Relative\nPolicy Optimization(GRPO) [21], can effectively enhance the reasoning abilities of LLMs. Generally speaking,\nthe RL objective is to find a policy\u03c0 that maximizes the reward, i.e.:\nmax\n\u03c0\n\b\nJ (\u03c0) := Ex\u223cDEy\u223c\u03c0(\u00b7|x) [r (x, y)]\n\t\n, (2.1)\nwhere x is the training prompt,D is the sampling distribution ofx, y is the response sampled by the policy\n\u03c0 for input promptx, andr denotes the reward function.\nIn practice, we estimate a surrogate objective forJ (\u03c0) at the batch level for tractable optimization. At each\ntraining stepk, we sample a batch ofN prompts x1, . . . , xN from the data distributionD, denoted asTk,\nand generate the corresponding responsesy1, . . . , yN using the current policy\u03c0 with a context lengthT and\ntemperature \u03c4. The batch-level surrogate objective at stepk can be formulated as:\nmax\n\u03c0\n\b\nJk (\u03c0) := Exi\u223cTk Eyi\u223c\u03c0(\u00b7|xi) [r (xi, yi)]\n\t\n, (2.2)\nwhere \u03c0k is shorthand for the policy\u03c0\u03b8k parameterized by\u03b8k.\nVanilla Policy Gradient For a parameterized policy\u03c0\u03b8, vanilla PG [23] uses gradient ascent to obtain\nthe optimal parameter\u03b8\u2217, i.e.\n\u03b8 \u2190 \u03b8 + \u2207\u03b8J (\u03c0\u03b8) .\nA valid first-order surrogate policy loss for vanilla PG at each iterationk is given by:\nLPG\nk (\u03b8) = \u2212Exi\u223cTk Eyi\u223c\u03c0k(\u00b7|xi)\n\uf8ee\n\uf8f0\n|yi|\u22121X\nt=0\n\u03c0\u03b8 (at\ni|st\ni)\n\u03c0k (at\ni|st\ni) \u00b7 A\u03c0k\n\u0000\nst\ni, at\ni\n\u0001\n\uf8f9\n\uf8fb, (2.3)\nwhere the response yi = ( a0\ni , ..., a|y|\u22121\ni ) consists of |y| tokens, at\ni is the t-th token in the sequence yi,\nst\ni := (xi, a0\ni , ..., at\u22121\ni ) is the prefix context when generatingat\ni, andA\u03c0k is the advantage function defined as\nA\u03c0k\n\u0000\nst, at\u0001\n:= Ey\u223c\u03c0k(\u00b7|x)\n\u0002\nr (x, y) |st, at\u0003\n\u2212 Ey\u223c\u03c0k(\u00b7|x)\n\u0002\nr (x, y) |st\u0003\n.\nOne can easily show that\u2207\u03b8LPG\nk (\u03b8k) = \u2212\u2207\u03b8Jk (\u03c0k).\nProximal Policy Optimization (PPO) At each training stepk, PPO [20] performs multiple gradient\ndescent steps on the policy lossLk with a clip trick to keep the new policy restricted within the trust region\nof \u03c0k. The policy loss employed in PPO is formulated as:\nLPPO\nk (\u03b8) = \u2212Exi\u223cTk Eyi\u223c\u03c0k(\u00b7|xi)\n\uf8ee\n\uf8f0\n|yi|\u22121X\nt=0\nmin\n\u0000\n\u03c1t\ni (\u03b8) A\u03c0k\n\u0000\nst\ni, at\ni\n\u0001\n, clip\n\u0000\n\u03c1t\ni (\u03b8) , 1 \u2212 \u03b5, 1 + \u03b5\n\u0001\n\u00b7 A\u03c0k\n\u0000\nst\ni, at\ni\n\u0001\u0001\n\uf8f9\n\uf8fb,\nwhere \u03c1t\ni(\u03b8) :=\n\u03c0\u03b8(at\ni|st\ni)\n\u03c0k(at\ni|st\ni), and \u03b5 is the clip hyperparameter. In practice, PPO generally uses GAE [19] to\nestimate the token-level advantageA\u03c0k (st\ni, at\ni).\nGroup Relative Policy Optimization (GRPO)Suppose M i.i.d. responses yi1, .., yiM are sampled for\neach promptxi. GRPO [21] estimates the token-level advantage using the group-normalized rewards and\n6introduces an additional length normalization term1\n|yij| for each responseyij. The policy loss employed in\nGRPO is formulated as:\nLGRPO\nk (\u03b8) = \u2212Exi\u223cTk E{yij}M\nj=1\u223c\u03c0k(\u00b7|x)\n\uf8ee\n\uf8f0 1\nM\nMX\ni=1\n1\n|yij|\n|yij|\u22121X\nt=0\nmin\n\u0000\n\u03c1t\nij (\u03b8) At\nij, clip\n\u0000\n\u03c1t\nij (\u03b8) , 1 \u2212 \u03b5, 1 + \u03b5\n\u0001\nAt\nij\n\u0001\n\u2212 \u03b2Dt\nij (\u03b8)\n\uf8f9\n\uf8fb, (2.4)\nwhere yij = (a0\nij, ..., a|yij|\u22121\nij ), at\nij is the t-th token in the sequenceyij, st\nij := (xi, a0\nij, ..., at\u22121\nij ), \u03c1t\nij(\u03b8) :=\n\u03c0\u03b8(at\nij|st\nij)\n\u03c0k(at\nij|st\nij), \u03b5 is the clip hyperparameter,Dt\nij is the token-level k3 loss [21] applied inat\nij with coefficient\u03b2 to\nkeep the policy\u03c0\u03b8 stay in the trust region of reference policy\u03c0ref, i.e.\nDt\nij (\u03b8) := \u03c0ref\n\u0000\nat\nij|st\nij\n\u0001\n\u03c0\u03b8\n\u0000\nat\nij|st\nij\n\u0001 \u2212 log \u03c0ref\n\u0000\nat\nij|st\nij\n\u0001\n\u03c0\u03b8\n\u0000\nat\nij|st\nij\n\u0001 \u2212 1,\nFor each prompt-response pair(xi, yij), a binary rewardr (xi, yij) \u2208 {0, 1} is given by a rule-based verifier.\nThe token-level advantageAt\nij is estimated by\n\u2200t : At\nij = r (xi, yij) \u2212 mean (r (xi, yi1) , ..., r(xi, yiM ))\nstd (r (xi, yi1) , ..., r(xi, yiM )) . (2.5)\n3 MAGIC in Skywork-OR1\nWe employ a training pipeline built upon a modified version of GRPO [21], referred to asMulti-stage Adaptive\nentropy scheduling forGRPO In Convergence (MAGIC). In the following sections, we first introduce the\nrecipe of MAGIC and then analyze the effectiveness of each of its components.\n3.1 MAGIC\nIn the following, we present the MAGIC framework by detailing its components in terms of Data Collection,\nTraining Strategy, and Loss Function.\nData Collection To ensure the quality of queries during post-training, we construct the initial dataset\nthrough stringent data preparation, as described in Section 6, and adopt more accurate verifiers to provide\nreward signals, as outlined in Section 7. Additionally, we employ the following strategies to further improve\nsample efficiency:\n1. Offline and Online Filtering.We apply data filtering both before and during training. Prior to\ntraining, we remove prompts with base model correctness rates of 1 (fully correct) or 0 (completely\nincorrect). During training, at the beginning of each stage, we also discard training prompts for which\nthe actor model achieved correctness of 1 in the previous stage. This dynamic online filtering mechanism\nensures that the actor model is consistently trained on challenging problems at each stage.\n2. Rejection Sampling.Responses in the zero-advantage group (as defined by Equation(2.5)) do not\ncontribute to the policy loss but may influence the KL loss or entropy loss, potentially leading to a more\nunstable training process due to the implicitly increased relative weight of these losses. To mitigate this\nissue, our training batches include only groups with non-zero advantages; specifically, the samples of\nprompt xi are filtered out ifi /\u2208 \u02dcTk, where\n\u02dcTk :=\nn\ni \u2208 [N] : \u2203j \u2208 [M] \u02c6Aij \u0338= 0\no\n.\n7Training Strategy We made the following refinements to the training strategy of vanilla GRPO:\n1. Multi-Stage Training.Inspired by DeepScaleR [17], we progressively increase the context lengthT\nand divide the training process into multiple stages. We found that multi-stage training significantly\nreduces computational costs while preserving scalability, as supported by the evidence presented in\nSection 3.2.2.\n2. Advantage Mask for Truncated Responses.To address potential noise in training signals when\noutcomes cannot be derived from truncated responses \u2013 since assigning negative advantages in such cases\nmay introduce bias \u2013 we experimented with an advantage mask during the early stages of multi-stage\ntraining, when many responses are truncated. However, as shown in Section 3.2.3, penalizing truncated\nresponses does not hinder later-stage improvements and enhances token efficiency. Based on these\nresults, we donot employ any advantage mask strategy in our training pipeline.\n3. High-Temperature Sampling. We set the rollout temperature to\u03c4 = 1 to enhance the model\u2019s\nexploration capability and improve learning plasticity. This decision was motivated by our observation\nthat the sampling policy either immediately enters (in the case of math data) or quickly transitions\ninto (in the case of code data) a low-entropy state when using a smaller sampling temperature (e.g.,\n\u03c4 = 0.6). See Section 3.2.4 for further details.\n4. On-Policy Training.We adopted on-policy training for Skywork-OR1-7B and Skywork-OR1-32B, as\nwe found that on-policy updates significantly slow entropy collapse and lead to higher test performance.\nSee Section 4 for our detailed findings on entropy collapse. In contrast, Skywork-OR1-Math-7B was\ntrained with two gradient steps per training step (and was therefore not strictly on-policy). This setup\npreceded our complete understanding of the relationship between off-policy updates and premature\nentropy collapse. Nevertheless, adaptive entropy control (Section 3.2.5) effectively mitigated collapse,\nallowing the model to achieve strong performance.\nLoss Function To mitigate implicit length bias, we adopt a token-level policy loss by removing the length\nnormalization term1/ |yij| from each response. The policy loss is averaged across all tokens in a training\nbatch, formulated as follows:\nLMAGIC (\u03b8) = \u2212 1\nTk\nX\ni\u2208\u02dcTk\nMX\nj=1\n\uf8f1\n\uf8f2\n\uf8f3\n|yij|\u22121X\nt=0\nmin\n\b\n\u03c1t\nij (\u03b8) At\nij, clip\n\u0000\n\u03c1t\nij (\u03b8) , 1 \u2212 \u03b5, 1 + \u03b5\n\u0001\nAt\nij\n\t\n+ \u03b1kHt\nij (\u03b8)\n\uf8fc\n\uf8fd\n\uf8fe, (3.1)\nwhere yij := (a0\nij, ..., a|yij|\u22121\nij ), at\nij is thet-th token in the sequenceyij, st\nij := (xi, a0\nij, ..., at\u22121\nij ) is the prefix\ncontext when generatingat\nij, \u03c1t\nij(\u03b8) :=\n\u03c0\u03b8(at\nij|st\nij)\n\u03c0k(at\nij|st\nij), Ht\nij(\u03b8) := H\n\u0000\n\u03c0\u03b8\n\u0000\n\u00b7|st\nij\n\u0001\u0001\nis the entropy of the generation\npolicy of tokenat\nij, \u03b1k \u2265 0 is the coefficient of the entropy,Tk := P\ni\u2208\u02dcTk\nPM\nj=1 |yij| is the total number of\ntokens in the training batch. Meanwhile, we also introduce the following characteristics into the loss function:\n1. Adaptive Entropy Control. To preserve the model\u2019s exploration capability and maintain high\nlearning plasticity, it is common to include an additional entropy loss to prevent entropy collapse. An\nappropriately weighted entropy loss can enhance generalization. However, our experiments show that\nselecting a suitable coefficient in advance is often challenging, as the entropy loss is highly sensitive to\nboth the coefficient and the training data. To address this, we introduce an additional hyperparameter,\ntgt-ent, representing the target entropy. This hyperparameter dynamically adjusts the coefficient\u03b1k\nbased on the difference between the current entropy and the target entropy, ensuring that the current\nentropy remains lower-bounded bytgt-ent. See Section 3.2.5 for more details.\n2. No KL Loss.We found that including a KL loss term hinders performance gains, particularly in\nthe later stages of multi-stage training. Therefore, we omit the KL loss from our training recipe. See\nSection 3.2.6 for further discussion.\n83.2 Effectiveness of MAGIC Components\nIn this section, we present results from extensive experiments conducted to examine how various components\nof our MAGIC recipe influence the performance improvement of reinforcement learning during post-training.\n3.2.1 Data Mixture\n(a)\n (b)\nFigure 4: Left: Comparison of our data mixture with DeepScaleR\u2019s mixture. The experiment was conducted\non an earlier version of the 32B variant, using only math data.Right: Comparison of AIME 24 performance\nbetween two mixtures: our official mixture (default) and a version with additional data selected using lower\nverification criteria (i.e., with potential errors in ground truth answers). Although the quality is lower, we\nobserve only slower learning progress compared to the clean counterpart.\nIn our formal training recipe, we include additional hard problems filtered from NuminaMath-1.5 [13] to\nconstruct our final data mixture. We conduct the following ablation study to demonstrate the effectiveness of\nthis design choice. We primarily compare against DeepScaleR\u2019s data mixture [17], as existing models trained\non it have shown strong performance.\nAblation Experiments 1: Existing Mixture vs. Our Data Mixture\n1. DeepScaleR mixture [17]: Comprises problems from previous years\u2019 AIME, AMC, Omni-MATH\n[4], and STILL [26].\n2. Skywork-OR1 mixture: Our custom mixture described in Section 6, incorporating problems\nfrom more diverse sources (e.g., NuminaMath-1.5) and selected via difficulty filtering and quality\ncontrol.\nWe use the same hyperparameters and approximately the same number of training steps across both\nexperiments to control for the effect of data size. Results are shown in Figure 4.\nAlthough the DeepScaleR dataset performs well with smaller model variants, we observed a slight initial\nimprovement on AIME24. However, performance degraded sharply after 300 training steps, eventually\nreturning to the same accuracy as before training. Additionally, in Figure 4(b), we test our data mixture\ncombined with anextra subsetobtained via a less stringent verification procedure. This extra subset contains\nhard problems from NuminaMath-1.5 that were previously excluded due to potential mismatches between\nextracted and provided solutions. We find that the performance difference between the two mixtures is\nnegligible within the first 900 steps. The version including the extra subset exhibits slightly slower early\n9(a)\n (b)\nFigure 5: Left: Comparison ofFrom-Scratchvs. Multi-Stage training. Top left:Response length during\nRL training. Bottom left: AIME24 avg@8 performance at temperature 1 (left y-axis) and cumulative\ntraining hours (right y-axis). Multi-stage training achieves the same final accuracy with significantly fewer\ntraining hours due to a smaller context length in the early stages.Right: AIME24 avg@32 vs. response\nlength for Skywork-OR1-Math-7B and DeepSeek-R1-Distill-Qwen-7B with 32K context length. TheStage I\ncheckpoint of Skywork-OR1-Math-7B reaches comparable performance to DeepSeek-R1-Distill-Qwen-7B with\nnotably better token efficiency; further performance gains are seen inStages II&III.\nprogress, possibly due to noise in the provided answers. We hypothesize that RL training is robust to small\namounts of ground truth noise, consistent with findings in [36]. Therefore, we adopt the default data\ncomposition described in Section 6 for all subsequent exploration experiments.\n3.2.2 Multi-Stage Training\nOne of the major challenges in optimizing long Chain-of-Thought (CoT) models with RL is managing\nexcessively long outputs, which can lead to slow convergence and high training variance. Inspired by\nDeepScaleR [17], we incorporated multi-stage training in all our released models to improve training efficiency.\nSpecifically, we used a shorter context lengthT in the initial stages. Once the model\u2019s performance converged,\nwe increased T in the subsequent stage. This approach led to significant performance improvements on\nbenchmarks while also enhancing training efficiency.\nSame Improvement, Higher Efficiency.To demonstrate the effectiveness of multi-stage training, we\nconducted two experiments based on DeepSeek-R1-Distill-Qwen-7B with different schedules forT:\nAblation Experiments 2: From-Scratch vs. Multi-Stage\n1. From-Scratch: We started withT = 16K at step 0 and kept it fixed during training.\n2. Multi-Stage: We started withT = 8K at step 0. At a later step (i.e., step 540), we switched to\nStage II and increasedT to 16K.\nThe other hyper-parameters were kept same for both experiments and are reported in Table 1. The\nresults are presented in Figure 5(a) and Figure 5(b).\nFigure 5(a) illustrates how AIME24 accuracy, generated response length, and cumulative training hours\nevolve with the number of training steps in Ablation Experiments 2. As shown, the AIME24 accuracy in\nboth experiments converges to approximately 60 when the number of training steps is sufficiently large.\nHowever, in the multi-stage experiment, the context length inStage I (i.e., 8K) is only half that used in the\nfrom-scratch experiment (i.e., 16K). As a result, the average response length in the multi-stage experiment is\n10Batch Size Mini-batch Size Group Size Entropy Control KL Loss\n64 32 16 target-entropy 0.2 No\nTable 1: Shared hyperparameters in Ablation Experiments 1 based on Deepseek-R1-Distill-Qwen-7B.\nsignificantly shorter duringStage I and the initial steps ofStage II, leading to more efficient training due to\nreduced inference and computational costs (approximately 100 training hours are saved over 1000 training\nsteps). After transitioning toStage II, both the response length and AIME24 accuracy begin to increase\nimmediately. Within roughly 500 training steps inStage II, the accuracy of the multi-stage experiment\nreaches the same level as that of the from-scratch experiment.\nImproving Token Efficiency While Preserving Scaling Potential.Truncated responses are labeled\nas negative samples in RL training because they lack final answers. A potential concern with multi-stage\ntraining is that using short context windows may bias the model toward generating shorter responses,\npotentially limiting its exploratory capacity and reducing its ability to solve complex problems.Our findings\ndemonstrate that multi-stage training not only improves token efficiency in the initial stage but\nalso preserves scaling ability.In Figure 5(b), we observe that training with an 8K context length inStage\nI maintains comparable AIME24 accuracy under a 32K context length while significantly improving token\nefficiency (reducing the average response length from approximately 12.5K to 5.4K tokens). In Stages II and\nIII, Skywork-OR1-Math-7B steadily increases response length while concurrently improving performance.\n3.2.3 Advantage Mask for Truncated Responses\nIn practice, responses are sampled within a fixed context lengthT. When response lengths exceedT, the\noutcomes cannot be derived, and accuracy rewards are set to 0, resulting in negative advantages for these\ntruncated responses, which may introduce bias. To mitigate this issue, we investigated several advantage\nmask strategies aimed at reducing the influence of truncated responses. However, our findings show that\nassigning negative advantages to truncated samples not only improves token efficiency but also preserves the\nmodel\u2019s scaling ability in later stages. As a result, we did not apply any mask strategies in our final training\npipeline.\nFigure 6: Training accuracy and clip ratio during RL training of Skywork-OR1-Math-7B inStage I. accuracy:\nMean accuracy reward on training batch. accuracy_nontruncated: Mean accuracy of non-truncated samples.\nclip_ratio: Ratio of truncated responses.\nTwo Optimization Directions in Short Context Length.In ourStage I training of Skywork-OR1-\n11Math-7B, we set the context length toT = 8K, and approximately 40% of responses were truncated at the\ninitial steps. Although overall training accuracy continued to increase during RL training, we observed that\nthe accuracy of non-truncated samples initially declined sharply within the first 100 training steps before\nshowing a slight upward trend. See Figure 6 for details. A truncated response typically receives an accuracy\nreward of 0 because the final answer is missing due to truncation, even if it would be correct if fully generated.\nTherefore, reducing the number of truncated responses improves achievable accuracy. Figure 6 shows that\nthe initial increase in training accuracy (steps 0-100) is primarily due to a sharp decrease in the clip ratio.\nAfter step 100, the algorithm begins to improve accuracy for non-truncated responses as well.\nA Brief Explanation from a Theoretical Perspective.We now use mathematical language to clarify\nthis phenomenon further in a formal way. Recall the objective of RL training in (2.1),\n\u03c0\u2217 \u2208 argmax\n\u03c0\n{J (\u03c0) := Ex\u223cDEy\u223c\u03c0(\u00b7|x) [r (x, y)]},\nwhere x is the prompt,D is the distribution of prompts,y is the response sampled from actor\u03c0, r(x, y) \u2208 {0, 1}\nis the binary accuracy reward. Note that the responsey is sampled under the context lengthT . For these\ntruncated responses whose lengths are greater thanT, i.e. |y| > T, the accuracy reward isr(x, y) = 0 since\nthe outcome can not be derived from the response. Based on this observation, one can easily shows that the\nobjective functionJ (\u03c0) satisfies\nJ (\u03c0) = Ex\u223cDEy\u223c\u03c0(\u00b7|x) [r (x, y)]\n= Ex\u223cDEy\u223c\u03c0(\u00b7|x) [r (x, y) I{|y| \u2264T}]\n= Ex\u223cD\n\u0014\np\u03c0\nnon\u2212trunc (x) Ey\u223c\u03c0(\u00b7|x)\n\u0014 I{|y| \u2264T}\np\u03c0\nnon\u2212trunc (x)r (x, y)\n\u0015\u0015\n= Ex\u223cD\n\u0002\np\u03c0\nnon\u2212trunc (x) Ey\u223c\u02c6\u03c0T (\u00b7|x) [r (x, y)]\n\u0003\n= Ex\u223cD\n\u0002\np\u03c0\nnon\u2212trunc (x) \u00afr\u03c0\nnon\u2212trunc (x)\n\u0003\n,\nwhere p\u03c0\nnon\u2212trunc (x) := Py\u223c\u03c0(\u00b7|x) (|y| \u2264T) is the probability that a responsey is not truncated by the limit\nof context lengthT (we assumep\u03c0\nnon\u2212trunc (x) > 0 for simplicity),\u00afr\u03c0\nnon\u2212trunc (x) := Ey\u223c\u02c6\u03c0T (\u00b7|x) [r (x, y)] is the\naccuracy of the non-truncated responses output by policy\u03c0 and \u02c6\u03c0T (y|x) := \u03c0(y|x)\np\u03c0\nnon\u2212trunc(x) I{|y| \u2264T}. This\nimplies that the accuracy on training distribution, i.e.J (\u03c0), can be increased by:\n\u2022 increasing p\u03c0\nnon\u2212trunc (x), which means the number of the responses that receive accuracy reward of 0\nerroneously decreases.\n\u2022 increasing r\u03c0\nnon\u2212trunc (x), which means the response quality within the context length will be improved.\nAdvantage Mask for Truncated Responses.To encourage the algorithm to focus on optimizing accuracy\nwithin the context length \u2013 i.e., increasingr\u03c0\nnon\u2212trunc (x) \u2013 rather than merely shortening responses to avoid\nerroneously receiving a zero accuracy reward \u2013 i.e., increasingp\u03c0\nnon\u2212trunc (x) \u2013 we explored various advantage\nmask strategies. These strategies were designed to mitigate the impact of noisy training signals introduced\nby truncated samples. We conducted ablation experiments using DeepSeek-R1-Distill-Qwen-7B inStage I to\nevaluate the effects of different advantage mask strategies.\n12(a)\n (b)\n (c)\nFigure 7:Left: The clip ratio of generated responses during reinforcement learning training was analyzed after\napplying various advantage mask strategies in Ablation Experiments 2. Using an advantage mask mitigates\nthe decay in response length. The clip ratio even increased after applying Adv-Mask-Before.Middle:\nTraining accuracy of responses influenced by different advantage mask strategies in Ablation Experiment 2\nshows distinct patterns. After applying the Adv-Mask-Before, training accuracy decreases. In contrast, it\ncontinues to increase when using the Adv-Mask-After or No-Adv-Mask strategies.Right: Training accuracy\nof non-truncated responses induced by different advantage mask strategies in Ablation Experiment 2 showed\ndistinct outcomes. After applying the Adv-Mask-Before strategy, the training accuracy of non-truncated\nresponses continued to rise. In contrast, both the Adv-Mask-After and No-Adv-Mask strategies resulted in a\nsharp decrease during the early steps.\nAblation Experiments 3: Different Advantage Mask Strategies\n1. No-Adv-Mask: We do not employ any advantage mask strategy.\n2. Adv-Mask-Before: The truncated responses arenot involvedin the group advantage calculation\nfor non-truncated responses, and the advantage of these truncated responses are set to 0 (thus\nnot contributing to the policy loss):\n\u2200t : At\nij =\n\uf8f1\n\uf8f2\n\uf8f3\nr(xi,yij)\u2212mean(\u02c6Ri)\nstd(\u02c6Ri) |y| \u2264T\n0 |y| > T\nHere \u02c6Ri is the accuracy rewards group ofnon-truncated responses of promptxi.\n3. Adv-Mask-After: The truncated responses arestill involvedin the group advantage calculation\nfor non-truncated responses, and the advantage of these truncated responses are set to 0 (thus\nnot contributing to the policy loss):\n\u2200t : At\nij =\n(r(xi,yij)\u2212mean(Ri)\nstd(Ri) |y| \u2264T\n0 |y| > T\nHere Ri is the accuracy rewards group ofall responses of promptxi.\nThe other hyperparameters remain the same for both experiments and are reported in Table 2. The\nresults can be found in Figure 7(a), Figure 7(c) and Figure 8.\nBatch Size Mini-batch Size Group Size Context Length T Entropy Control KL Loss\n256 128 16 Stage I 8K target-entropy 0.2 No\nTable 2: Shared hyperparameters in Ablation Experiments 2 based on Deepseek-R1-Distill-Qwen-7B.\n13Figure 8: AIME24 avg@32 performance vs. context length for different advantage mask strategies in Ablation\nExperiments 3. All strategies achieve the same accuracy at the 32K context length. The accuracy was further\nimproved after the training ofStage II even though the noisy training signals from truncated responses were\nintroduced inStage I.\nFigure 7 shows the clip ratio, overall accuracy, and accuracy on non-truncated responses in Ablation\nExperiments 2. We observe that although the response quality within the context length (i.e., the accuracy\nof non-truncated responses) increases as expected after applying the Adv-Mask-Before strategy, the overall\ntraining accuracy continues to decline, and the clip ratio increases steadily.This appears to be a form of\nreward hacking from our perspective.More importantly, as shown later in Figure 8, the accuracy of the\nAdv-Mask-Before strategy under large context lengths \u2013 where responses are typically not truncated (e.g.,\n32K) \u2013 shows no improvement. This may be attributed to the smaller effective training batch size caused by\nthe increased clip ratio under the Adv-Mask-Before strategy. The behavior of Adv-Mask-After serves as an\nintermediate point between Adv-Mask-Before and No-Adv-Mask.\nAdvantage Mask Does Not Exhibit Better Performance Given a Larger Inference Budget.\nAlthough Ablation Experiments 2 demonstrate that\u00afr\u03c0\nuntrunc (x) is optimized under short context lengths\nwhen applying advantage masks, we find that accuracy does not improve when the context length is large\nenough to avoid truncation (i.e., 32K). We compare the test-time scaling behavior on AIME24 for models\ntrained with different advantage mask strategies (see Figure 8). The results show that applying an advantage\nmask does not improve test-time scaling behavior inStage I, and accuracy at 32K remains unchanged-even\nthough \u00afr\u03c0\nnon\u2212trunc (x) is optimized during training. In contrast, RL training without an advantage mask in\nStage I not only maintains accuracy at large context lengths but also significantly improves token efficiency.\nMoreover, the shorter response lengths learned in Stage I do not hinder the simultaneous improvements\nin both response length and accuracy observed in Stage II. Based on these findings, we did not apply any\nadvantage mask to address noisy training signals from truncated samples in our final training recipe.\n3.2.4 High-temperature Sampling\nThe group-wise nature of GRPO implies that the sampling procedure for responses directly affects the quality\nand diversity of each group, which in turn influences learning. Prior work suggests that higher temperatures\ngenerally lead to slightly worse performance due to increased randomness. If the temperature is set too\nhigh, it may increase the likelihood of sampling groups containing only incorrect responses, thereby reducing\n14Figure9: AIME25avg@8performanceandentropyversusthenumberoftrainingstepsinAblationExperiments\n3. Training with a temperature of 0.6 starts with the lowest entropy and learns more slowly than at a\ntemperature of 1.0. Note that the entropy in the right plot remains around 0.2 because adaptive entropy\ncontrol is enabled. This experiment was conducted on an earlier version of the 32B variant using only math\ndata. Note also that in the left plot, the two temperatures indicate the rollout temperatures used during\ntraining. The scores of AIME25 were obtained by evaluating both models at a temperature of 0.6 to ensure a\nfair comparison.\ntraining efficiency due to the absence of advantageous signals. On the other hand, using a low temperature\nreduces group diversity, resulting in solutions that are highly similar or potentially all correct. Therefore,\nselecting an appropriate temperature is critical to ensure sufficient in-group solution diversity. We conducted\nablation experiments on the choice of sampling temperature\u03c4, and the results are presented in Figure 9.\nAblation Experiments 4: Different Online Sampling Temperatures\u03c4\nWe compared two different sampling temperatures in online RL training:\n1. High Temperature: We set the temperature hyperparameter\u03c4 = 1.0.\n2. Low Temperature: We set the temperature hyperparameter\u03c4 = 0.6.\nThe other hyperparameters were kept the same for both experiments and are reported in Table 3.\nThe results can be found in Figure 9.\nBatch Size Mini-batch Size Group Size Context Length T Entropy Control KL Loss\n64 32 16 Stage I 16K target entropy 0.2 0\nTable 3: Shared hyperparameters in Ablation Experiments 4\nIn our experiments, we identified an additional entropy-related phenomenon: when a low temperature is\nused (e.g., 0.6), the model either begins with extremely low entropy or its entropy quickly collapses to near\nzero within approximately 100 steps. This behavior initially slows learning progress and ultimately leads to\nstagnation. We hypothesize that with a less diverse group of solutions \u2013 despite containing both correct and\nincorrect responses \u2013 the policy update becomes overly focused on a narrow subset of tokens. This results in\na large probability mass being assigned to specific tokens that frequently appear in the sampled responses.\nWhen we increased the rollout temperature to 1.0, the model\u2019s initial entropy rose to a more desirable range.\nAlthough entropy still eventually converges, the higher temperature substantially enhances the learning signal\nin the early stages and preserves greater potential for continued training, as shown in the figure above.\n153.2.5 Adaptive Entropy Control\nBuilding on the findings from Section 4 \u2013 which suggest that while preventing premature entropy collapse\nvia entropy regularization is beneficial, selecting an appropriate entropy loss coefficient is challenging \u2013 we\nintroduce Adaptive Entropy Control, a method that adaptively adjusts the entropy loss coefficient based\non the target and current entropy. Specifically, we introduce two additional hyperparameters:tgt-ent (the\ndesired target entropy) and\u2206 (the adjustment step size for the entropy loss coefficient). We initialize the\nadaptive coefficient withc0 = 0. At each training stepk, let e denote the current entropy of the actor\n(estimated from the rollout buffer). Ife is less thantgt-ent, we increaseck by \u2206 (i.e., ck+1 = ck + \u2206). If e\nexceeds tgt-ent, we decreaseck by \u2206. To alleviate instability caused by unnecessary entropy loss, we activate\nthe entropy loss only whene \u2264 tgt-ent, i.e.,\u03b1k = ck \u00b7 I{e \u2264 tgt-ent}, ensuring that the current entropy\nremains lower-bounded by the target entropy.\nBy leveraging adaptive entropy control, we maintain the model\u2019s entropy at a reasonable level throughout\ntraining and effectively prevent premature collapse. Figure 10 illustrates the entropy trajectory of Skywork-\nOR1-Math-7B across all training stages. In our experiments, we settgt-ent= 0.2 and\u2206 = 0.005. To further\nvalidate the effectiveness of adaptive entropy control, we conducted an ablation study detailed in Section 4.5.\n\u03b1k = ck \u00b7 I{ek \u2264 tgt-ent}, ck+1 =\n(\nck + \u2206, if ek < tgt-ent\nck \u2212 \u2206, if ek > tgt-ent, c0 = 0 (3.2)\nFigure 10: Entropy of generated responses (left) and avg@8 performance on AIME24 (right) of Skywork-\nOR1-Math-7B across all stages. We use adaptive entropy control withtgt-ent=0.2 and\u2206 = 0.005 . Under\nadaptive entropy control, the entropy of Skywork-OR1-Math-7B is generally lower-bounded by the target\nentropy 0.2 and the performance on AIME24 has been steadily improving.\n3.2.6 No KL Loss\nTo investigate the impact of the KL loss, we conducted the following ablation experiments.\n16Ablation Experiments 5: KL Loss vs. No KL Loss\nWe consider token-level k3 loss in our ablation and the KL-regularized policy loss we employed is:\nL\u03b2 (\u03b8) = L(\u03b8) + \u03b2\nTk\nX\ni\u2208Tk\nMX\nj=1\n|yij|\u22121X\nt=0\n \n\u03c0ref\n\u0000\nat\nij|st\nij\n\u0001\n\u03c0\u03b8\n\u0000\nat\nij|st\nij\n\u0001 \u2212 log \u03c0ref\n\u0000\nat\nij|st\nij\n\u0001\n\u03c0\u03b8\n\u0000\nat\nij|st\nij\n\u0001 \u2212 1\n!\n,\nwhere L(\u03b8) is the original policy loss defined in(3.1), \u03b2 is the KL coefficient. We first run a stage 1\nexperiment with\u03b2=1e-3 based on DeepSeek-R1-Distill-Qwen-7B (reference policy). Then in stage 2,\nwe conducted ablations based on the stage 1 checkpoint, comparing\u03b2 = 1e-3 with \u03b2 = 0. The other\nhyper-parameters are reported in Table 4. The results can be found in Figure 11(a) and Figure 11(b).\nBatch Size Mini-batch Size Group Size Context Length T Entropy Control\n256 128 16 Stage II 16K target entropy 0.2\nTable 4: Shared hyperparameters in Ablation Experiments 5 based on stage1 checkpoint\nWe observe that, in Stage 2, the KL loss strongly pulls the actor model\u2019s policy back toward the reference\nmodel, causing the KL divergence to rapidly decrease toward zero (see Figure 11(a)). As a result, performance\non AIME24 fails to improve significantly once the actor\u2019s policy becomes too similar to the reference policy\n(see Figure 11(b)). Based on this observation, we set\u03b2 = 0 for all training stages of our released models.\n(a)\n (b)\nFigure 11: Results of Ablation Experiments 5. Left: KL divergence between the actor model and the\nreference model during RL training with different KL loss coefficient\u03b2 in Ablation Experiments 5. Setting\n\u03b2 = 1e-3 pulls the actor model back towards the reference model strongly in stage 2.Right: The AIME24\navg@8 performance at temperature 1 during RL training of different\u03b2 in Ablation Experiments 5.\n174 Empirical Studies on Mitigating Policy Entropy Collapse\nFigure 12: Overview of our empirical studies on mitigating policy entropy collapse.Gray\u200c and green blocks:\nThe potential benefits and possible approaches to enhance the model\u2019s exploration capability and mitigate\nentropy collapse.Yellow blocks:The experimental variables in our empirical studies on keeping the model\u2019s\nexploration capability and maintaining high plasticity.\nExploration and exploitation represent one of the most fundamental dilemmas in RL training [22], particularly\nin on-policy algorithms. In brief, achieving better performance requires sufficient exploration.\nHowever, if the agent\u2019s policy prematurely converges to a specific solution, that policy may be suboptimal,\nand such convergence hinders the exploration of diverse trajectories. An important metric for monitoring the\nconvergence of RL algorithms is policy entropy. In general, when a model\u2019s policy entropy converges to a very\nsmall value (e.g., near zero), the policy stabilizes. At this point, the model\u2019s generation behavior becomes\nresistant to updates from training data, leading to reduced learning efficiency and diminished output diversity.\nTo expose the model to more effective training signals and improve its out-of-distribution (OOD) performance,\nit is therefore critical to prevent premature entropy collapse in practice. This section investigates which\nhyperparameters and components of the policy update process help prevent entropy collapse and, in turn,\nimprove OOD generalization. The overall framework of our empirical study on alleviating policy entropy\ncollapse is illustrated in Figure 12. Initially, we hypothesize that the following two sources may influence the\nmodel\u2019s entropy and convergence behavior:\n\u2022 Rollout diversity.If the rollout data contain a greater diversity of correct responses, this prevents the\nmodel from overfitting to a single correct trajectory. We examine how sampling-related hyperparameters\n\u2013 such as sampling temperature, rollout batch size, and group size \u2013 affect the model\u2019s policy entropy\nduring RL training.\n\u2022 Policy update.We also investigate how different components of the policy update influence entropy.\nIn this section, we focus primarily on the number of stochastic gradient descent (SGD) steps per training\nstep and the use of additional entropy control methods (e.g., entropy loss).\n18After conducting exhaustive ablation experiments, we present our main results below.\nEmpirical Results of Our Entropy Collapse Study\n1. Faster entropy collapse generally leads to worse test performance. In Section 4.2 and Section 4.5,\nwe show that appropriate entropy control, which prevents premature policy convergence, can yield\nimproved test performance.\n2. Increasing rollout diversity by enlarging the batch size and group size has only a minor effect on\nentropy dynamics, whereas using a higher sampling temperature significantly impacts initial entropy.\nSee Section 4.3 for details.\n3. Increasing the number of SGD steps per training step \u2013 whether by using more mini-batches or\nincreasing data reuse \u2013 significantly accelerates entropy collapse and generally results in degraded\ntest performance due to the introduction of off-policy data. See Section 4.4 for more information.\n4. Our ablation experiments in Section 4.5 show that the entropy loss is highly sensitive to both the\ntraining data and the loss coefficient. By either adaptively adjusting the entropy loss coefficient\nor appropriately applying the clip-higher trick [34], entropy dynamics can be stabilized and lower-\nbounded, leading to improved test performance.\n4.1 Ablation Setup\nAll ablation experiments presented in Section 4 are conducted using the training pipeline described in\nSection 3.1. We start from the following baseline experiment based on DeepSeek-R1-Distill-Qwen-7B with its\nhyperparameters reported in Table 5, the key symbols used are defined as follows:\n\u2022 DR is the rollout batch size (the number of prompts used to generate responses in one training step).\n\u2022 DT is the mini-batch size (the number of prompts corresponding to the responses used per policy\nupdate step).\n\u2022 Nreuse is the number of times the rollout buffer is traversed.\n\u2022 gs is the group size (the number of responses generated for each prompt).\n\u2022 T is the context length.\n\u2022 \u03c4 is the sampling temperature.\nDR DT Nreuse gs T \u03c4 Learning Rate Entropy Control KL loss\n64 64 1 16 16K 1.0 1e-6 No No\nTable 5: Hyperparameters of our baseline experiment in the ablation study presented in Section 4.\nUnless otherwise specified, the default training configurations for all ablation experiments in this section are\naligned with those of the baseline experiment presented above. We use AIME24, AIME25, and LiveCodeBench\n[10] (2024.08\u20132025.02) as evaluation sets. The test performance reported in our ablation study is computed\nas the empirical mean of avg@8 performance on AIME24/25 and pass@1 performance on LiveCodeBench.\nNotably, the baseline experiment achieves 69.2% avg@8 on AIME24, 53.3% avg@8 on AIME25, and 50.5%\npass@1 on LiveCodeBench after 2,700 training steps using 32 H800 GPUs. These results, which closely\napproximate the performance of our final Skywork-OR1-7B release, establish a strong baseline for analyzing\nkey factors that affect test performance and contribute to entropy collapse.\n19Figure 13: Preliminary experiments on mitigating entropy collapse by introducing entropy loss. We tested\ntwo different coefficients\u03b1k = 1e-3 and 5e-3, and found that the entropy loss with the higher coefficient\u03b1k,\ni.e., 5e-3, more effectively prevents entropy collapse and achieves higher test performance.Left: Accuracy\ncurves on test benchmarks during RL training.Right: Entropy of generated responses during RL training.\n4.2 Premature Entropy Collapse Generally Manifests as Worse Performance\nAs previously noted, entropy dynamics during RL training reflect the degree of policy convergence. When the\nactor converges to a specific policy and enters a low-entropy state, both learning efficiency and rollout diversity\ntend to decline. In our preliminary experiments, we observed that the entropy of the actor model often\ndecreased rapidly during training. To mitigate premature entropy collapse, we introduced an entropy loss\nterm, hypothesizing that it would allow the actor to converge toward a better policy. Our results confirmed\nthis hypothesis: test performance improved with the addition of entropy loss. Figure 13 presents the\naccuracy curves on test benchmarks and the entropy of generated responses from two preliminary experiments\nusing different values of the entropy loss coefficient\u03b1k (1e-3 vs. 5e-3). The results show that using a\nhigher coefficient (i.e., 5e-3) more effectively prevents entropy collapse and leads to better generalization\nperformance. Furthermore, our ablation experiments in Section 4.4 reinforce this finding, showing that RL\ntraining accompanied by premature entropy collapse generally results in worse test performance. These\nobservations motivate our integration of entropy control mechanisms into the training pipeline, as well as our\nsystematic investigation into how hyperparameters and other RL components influence entropy dynamics.\n4.3 The Impact of Rollout-Diversity-Related Hyperparameters\nWe investigated how the rollout batch sizeDR, group sizegs, and sampling temperature\u03c4 influence entropy\ndynamics. Note that increasing the rollout batch sizeDR and group sizegs during the rollout stage results\nin a larger rollout budget, which typically requires greater computational resources to accelerate training.\nTherefore, we provide a detailed discussion of the impact ofDR and gs in Section 5, which focuses on\ntraining-time computational resource allocation for improved test performance. Here, we present only the\nexperimental results related to policy entropy. Specifically, we conducted ablation experiments using rollout\nbatch sizesDR = 16, 32, 64 and group sizesgs = 4, 8, 16, based on the baseline experiment described in\nSection 4.1 and analyzed in Section 5. Our results (Figure 14) indicate no significant differences in entropy\ndynamics across these on-policy configurations. Notably, none of these experiments exhibited entropy collapse.\nRegarding the sampling temperature\u03c4, we found that using a properly chosen but relatively high temperature\nled to lower test accuracy during the initial training steps, but ultimately resulted in greater performance\nimprovements. For further details, please refer to Section 3.2.4.\n20Figure 14: Entropy of generated responses during on-policy updates with different rollout batch sizesDR\n(left) and group size gs (right). All the experiments exhibit similar entropy dynamics.\nOn\nPolicy\n\ud835\ud835 = \ud835\ud835\nOff\nPolicy\n ...\n ...\n......\n\ud835\ud835\n\ud835\ud835\n\u00d7 \ud835reuse\nOne SGD step is performed \nusing the whole rollout buffer\nRollout buffer is partitioned into \n\ud835\ud835\n\ud835\ud835\n mini-batches and \nreused by \ud835reuse times. Total  \ud835SGD =\n\ud835\ud835\n\ud835\ud835\n\u00d7 \ud835reuse \nSGD steps are performed in one training step \nFigure 15: Illustration of on-policy vs. off-policy update in PPO-style policy loss. On-policy update applies\na single SGD step to the entire rollout batch, whereas off-policy update implements multiple SGD steps\nthrough rollout batch decomposition and reuse. The rollout batch is partitioned intoDR\nDT\nmini-batches, with\neach mini-batch undergoing an independent SGD step. Then, one can iterate over the rollout batchNreuse\ntimes. Thus, the total number of SGD steps performed on one rollout batch isDR\nDT\n\u00d7 Nreuse.\n4.4 The Impact of Off-policy Update by IncreasingNSGD\nNote that the policy loss(3.1) in MAGIC is PPO-style, which naturally allows for performing multiple SGD\nsteps through rollout batch decomposition and reuse (as illustrated in Figure 15). Recalling the definitions of\nDR,DT and Nreuse from Section 4.1, it is clear that the number of SGD steps performed in one training step,\ni.e. NSGD, satisfies\nNSGD = DR\nDT\n\u00b7 Nreuse. (4.1)\n21When DR = DT and Nreuse = 1, the policy update is purely on-policy sinceNSGD = 1. In contrast, when\nDT < DR or Nreuse \u2265 2, NSGD \u2265 2 and the off-policy data is introduced into the policy update. In this\nsection, we investigate howNSGD affects the entropy dynamics and the test performance improvement.\nMore SGD Steps, Faster Convergence with Worse Test Performance.We conducted the following\nablation experiments on differentNSGD values by decreasingDT or increasingNreuse given fixedDR.\nAblation Experiments 6: The Impact of Different Numbers of SGD StepsNSGD\nConsider the quadruple(NSGD, DR, DT , Nreuse). We started from the baseline experiment (1,64,64,1)\npresented in Section 4.1 and adjusted eitherDT or Nreuse to increaseNSGD. The experiments are\nlisted below:\n1. NSGD = 1: The baseline experiment with the quadruple (1,64,64,1).\n2. NSGD = 2: We ran two experiments with the quadruples (2,64,32,1) and (2,64,64,2).\n3. NSGD = 4: We ran two experiments with the quadruples (4,64,16,1) and (4,64,64,4).\nThe experimental results can be found in Figure 16.\nFigure 16: Results of Ablation Experiments 6. Off-policy training with increasedNSGD by either decreasing\nDT or increasingNreuse accelerates entropy collapse and exhibits worse test performance.Left: Entropy of\ngenerated responses during RL training.Right: Test performance during RL training.\nAs shown in Figure 16, experiments withNSGD \u2208 {2, 4} exhibit faster policy convergence, with entropy\ndecaying to very small values within a few training steps. As a result, test performance fails to improve\nconsistently once the model enters a low-entropy state. In contrast, using an on-policy update with the\nconfiguration (1, 64, 64, 1) significantly alleviates this issue, leading to a gradual decline in entropy and a\nsteady, albeit slower, improvement in test performance. Ultimately, the on-policy update with configuration\n(1, 64, 64, 1) achieves superior test performance when the number of training steps is sufficiently large.\nOff-Policy Data Harms Test Performance.We now investigate which factor in off-policy updates is\nmore likely to contribute to degraded test performance. We identify the following two potential contributors\nthat may influence the gradient direction in each SGD step: (1) the mini-batch sizeDT , and (2) the use of\noff-policy data. In the data reuse experiments withNreuse \u2208 {2, 4}, sinceDT is held constant and matches the\nvalue used in the on-policy setting, we attribute the degraded test performance to the use of off-policy data\nintroduced through rollout batch reuse. In experiments that involve more mini-batches (i.e.,DT \u2208 {16, 32}),\nthe performance drop compared to the on-policy update may be due to both the smaller mini-batch size \u2013\n22leading to greater gradient variance \u2013 and the presence of off-policy data. To better understand which factor\ncontributes more significantly, we conducted the following ablation experiments.\nAblation Experiments 7: On-policy vs. Off-policy with the Same SGD Data SizeDT\nConsider the quadruple(NSGD, DR, DT , Nreuse).\n1. Off-policy update: We considered two off-policy experiments in Ablation Experiments 6 with\nthe quadruples (2,64,32,1) and (4,64,16,1), which have smallerDT compared to the baseline\n(1,64,64,1).\n2. On-policy update: We ran two experiments, configured with the quadruples (1,32,32,1) and\n(1,16,16,1) respectively as the on-policy counterparts to the off-policy update. These were based\non the baseline configuration from Section 4.1.\nThe experimental results are reported in Figure 17.\nFigure 17: Results of Ablation Experiments 7. On-policy experiments, i.e. NSGD = 1 , do not exhibit\npremature entropy collapse and finally outperform the off-policy counterparts with the sameDT when\ntraining step is sufficiently large.Left: Entropy of generated responses during RL training.Right: Test\nperformance at temperature 1 during RL training.\nThe experimental results shown in Figure 17 indicate that the on-policy update with a smallerDT \u2013 relative\nto the baseline experiment \u2013 still yields steady improvements in test performance, and premature entropy\ncollapse does not occur. Ultimately, the on-policy update outperforms the off-policy update with the same\nDT when the number of training steps is sufficiently large. Based on these observations, we hypothesize that\nthe degraded test performance in the off-policy update is primarily caused by the introduction of off-policy\ndata in each SGD step.\nCan a LargeDR in Off-Policy Updates Prevent Premature Entropy Collapse?Consider the\noff-policy experiment in Ablation Experiments 6 with the quadruple(NSGD, DR, DT , Nreuse) = (4, 64, 16, 1).\nWe attempted to increase the rollout batch sizeDR from 64 to 256 while keepingNSGD = 4 fixed (i.e.,\nresulting in the configuration(NSGD, DR, DT , Nreuse) = (4, 256, 64, 1)), with the expectation that this would\nintroduce more diverse samples and prevent convergence on single trajectory. However, our results in Figure\n18 indicates that even with a largerDR, premature entropy collapse not only still occurs but may even do so\nmore rapidly.\n23Figure 18: KeepingDR\nDT\n= 4 and Nreuse = 1, off-policy training with a largerDR, i.e.,DR = 256, does not\nprevent the premature entropy collapse. Both off-policy experiments, i.e.NSGD = 4, exhibit faster entropy\nconvergence compared with the on-policy experiment withNSGD = 1.\n4.5 Preventing Premature Entropy Collapse\nAs previously discussed, premature entropy collapse is often associated with degraded test performance. It is\ntherefore reasonable to expect that proper entropy control can lead to improved outcomes. As shown earlier,\nincreasing NSGD and introducing off-policy data accelerate entropy convergence. However, there are an\nincreasing number of scenarios where the use of off-policy data is unavoidable \u2013 for example, in asynchronous\ntraining frameworks. Thus, it is also important to study entropy control mechanisms under off-policy settings.\nWe begin by examining entropy regularization, a straightforward approach that attempts to prevent entropy\ncollapse by directly adding an entropy loss term. Our preliminary experiments, presented in Section 4.2, show\nthat applying entropy regularization with an appropriately chosen coefficient can mitigate entropy collapse\nand improve test performance. However, we later observed that the effectiveness of entropy regularization\nis highly sensitive to both the choice of coefficient and the characteristics of the training data, making it\ndifficult to select an optimal coefficient in advance. This motivates a dynamic adjustment of the entropy loss\ncoefficient. In addition, we consider the clip-higher trick proposed in [34] as another means of entropy control.\nIn the following, we present our detailed findings.\nEntropy Loss Is Sensitive to the Coefficient\u03b1k. To demonstrate the sensitivity of entropy loss to the\nchoice of\u03b1k, we conduct the following ablation study.\nAblation Experiments 8: Entropy Loss with Different Coefficients\u03b1k\nWe conducted ablation studies on a wide range of constant coefficients\u03b1k based on Skywork-OR1-\nMath-7B-stage1 (not the baseline experiment in Section 4.1 ). We select\u03b1k=1e-4, 5e-4, 1e-3, 5e-3,\n1e-2. The other hyperparameters are reported in Table 6. The results are presented in Figure 19.\nFrom the results in Figure 19, we find that:\n\u2022 For\u03b1k = 5e-4, 1e-3, 5e-3, and 1e-2, the entropy eventually rises sharply, leading to model collapse. The\nlarger the\u03b1k, the more rapidly the entropy increases.\n24Batch Size Mini-batch Size Group Size T Temperature \u03c4 KL Loss\n64 32 16 Stage II 16K 1.0 No\nTable 6: Shared Hyperparameters in Ablation Experiments 8 Based on Skywork-OR1-Math-7B-stage1\nFigure 19: The results of Ablation Experiments 8.Left: The entropy of generated responses during RL\ntraining. Right: The AIME24 avg@8 performance at temperature 1 during RL training.\n\u2022 For\u03b1k = 1e-4, while entropy does not exhibit a continuous rise, it still collapses, persistently decreasing\ntoward zero.\nEntropy Loss Is Sensitive to Training Data.From our two preliminary experiments, we observe that\nthe entropy loss is highly sensitive to variations in training data. We conducted two experiments under\nidentical configurations, both using an entropy loss coefficient of 1e-3. The only difference between the two\nsetups was the training dataset used (both datasets belong to the math domain). The results, shown in\nFigure 20, reveal a striking difference in entropy dynamics: while the original dataset exhibited a steady\ndecline in entropy throughout training, the new dataset resulted in a consistent upward trend in entropy.\nThis finding highlights the data-dependent nature of tuning the entropy loss coefficient.\nAdjusting the Coefficient of Entropy Loss Adaptively.Based on our findings regarding the sensitivity\nof entropy loss, we propose a method calledadaptive entropy control(see Section 3.2.5 for details), which\ndynamically adjusts the entropy loss coefficient during training. As shown in Figure 10, the entropy of\nSkywork-OR1-Math-7B remains lower-bounded by the target entropy throughout the RL training process. To\nfurther validate the effectiveness of adaptive entropy control, we conduct the following ablation experiments.\nAblation Experiments 9: Effectiveness of Adaptive Entropy Control\nConsider the off-policy experiment in Ablation Experiments 6 with (NSGD, DR, DT , Nreuse) =\n(4, 64, 16, 1), which exhibits fast entropy collapse and bad test performance. Note that there is\nno entropy loss in this experiment. We ran an experiment based on its configuration with adaptive\nentropy control (using a target entropy of 0.2) enabled. We report the results in Figure 21.\nAs previously analyzed, increasingNSGD accelerates policy convergence and leads to degraded test performance.\nAs shown in Figure 21, applying adaptive entropy control successfully prevents entropy collapse and results\nin higher test performance. However, it is worth noting that, although the coefficient is adjusted adaptively,\nentropy remains unstable whenNSGD is large. We speculate that this is due to the entropy loss being computed\nover the entire vocabulary, which may increase the probability of many unintended tokens. Therefore, we do\n25Figure 20: Preliminary experiments investigating how training data affects the entropy during RL training.\nBoth experiments used the same hyperparameter configurations with\u03b1k=1e-3 but differed in the training\ndata. Both datasets are in math domain. simply switching the dataset resulted in dramatically different\nentropy evolution patterns\nnot recommend using adaptive entropy control in scenarios whereNSGD is large. Nonetheless, we find that\nwhen NSGD = 1 or 2, entropy dynamics remain acceptably stable under adaptive entropy control. Based on\nthese findings, we adopt adaptive entropy control in the training of our Skywork-OR1 models.\nFigure 21: The results of Ablation Experiments 9. Applying adaptive entropy control prevents the entropy\ncollapse, leading to a better test performance.Left: Entropy of generated responses during RL training.\nRight: Test performance during RL training.\nThe Impact of the Clip-Higher Trick.We tested a popular trick called clip-higher [34] used in PPO-style\npolicy loss to prevent the entropy collapse whenNSGD > 1. We conduct the following ablation experiments.\n26Ablation Experiments 10: The Impact of Different Higher-clip Ratios\nConsider the off-policy experiment in Ablation Experiments 6 with the quadruple\n(NSGD, DR, DT , Nreuse) = (4 , 64, 16, 1), which exhibits fast entropy collapse and poor test\nperformance. Note that the clip ratio\u03f5 = 0.2 was applied in this experiment. We raised the higher-clip\nratio from0.20 to 0.25, 0.265, and0.28 while keeping the lower-clip ratio fixed at0.2. We report the\nresults in Figure 22.\nOur results, shown in Figure 22, indicate that using a properly chosen higher-clip ratio \u2013 e.g., 0.25 or 0.265 \u2013\ncan prevent premature entropy collapse and lead to better test performance. However, it is worth noting that\nwhen the higher-clip ratio is set to 0.28, as suggested in [34], entropy increases sharply, resulting in poor test\nperformance. This suggests that the optimal higher-clip ratio is task-dependent.\nFigure 22: The results of Ablation Experiments 10. Increasing the higher-clip ratio to an adequate value\n(e.g., 0.25 and 0.265) yields slower convergence and better test performance. However, we find that when the\nhigher-clip ratio is set to 0.28 as recommended in [34], then entropy rises sharply and test performance is not\nimproved. Left: Entropy of generated responses during RL training.Right: Test performance during RL\ntraining.\n5 Empirical Studies on Training Resource Allocation\nDuring the RL training process, our goal is to select hyperparameters that make training both efficient and\neffective. This objective gives rise to two practical questions:\n\u2022 Given fixed computational resources, how can we improve training efficiency?\n\u2022 Given additional computational resources, how should we allocate them to achieve better test perfor-\nmance or improved training efficiency?\nIn this section, we address these questions in the context of long CoT scenarios, using results from exhaustive\nablation experiments as supporting evidence. The training process of online RL algorithms can generally\nbe divided into two distinct phases: data rollout and policy update (which includes both forward\nand backward passes). Let tR, tT , and tO denote the time spent on rollout, policy update, and other\noperations (e.g., reward computation, experience generation), respectively. The total time consumption under\na synchronous training framework is:\nttotal = tR + tT + tO.\n27Given a fixed context length, the rollout timetR is primarily influenced by the rollout batch sizeDR and\nthe group size (gs). As analyzed in Section 4.4, the policy update timetT depends on the number of SGD\nsteps NSGD, which is determined by the number of mini-batchesDR\nDT\nand the data reuse factorNreuse. In the\nfollowing subsections, we investigate how these factors impact both training efficiency and final performance.\n5.1 Improving Training Efficiency with Fixed Computational Resources\nIn this section, we aim to answer the first question: Given fixed computational resources, how can training\nefficiency be improved?\nFigure 23: Overview of empirical studies on improving training efficiency given fixed computational resources.\nGrey\u200c blocks:Potential approaches to enhance training efficiency and their underlying principles.Yellow\nblocks: Experimental variables in the empirical studies\nRollout TimetR Dominates the Total Training Timettotal. A fundamental observation regarding\nlong CoT models (e.g. Deepseek-R1-Distill model series) is that the total training time is primarily determined\nby the rollout time. Table 7 presents the values ofttotal, tR, tT and tO of Skywork-OR1-32B over 1000\ntraining steps. Clearly,tR dominates ttotal.\nTime Usage total\nttotal\nrollout\ntR\npolicy update\ntT\nothers\ntO\ntR/ttotal tT /ttotal\nHours 309 223 27 59 72.1% 8.7%\nTable 7: Analysis of training time usage of Skywork-OR1-32B for 1000 training steps.\nSince the primary bottleneck forttotal in long CoT training istR, it is reasonable to expect that appropriately\nincreasing the number of SGD steps per training step, i.e.,NSGD, will have minimal impact onttotal while\nimproving training efficiency. Therefore, in the following, we investigate the impact of the number of mini-\nbatches (DR\nDT\n) and the data reuse times (Nreuse) on both the total training timettotal and test performance.\nThe overall idea of our study is illustrated in Figure 23.\nMore SGD Steps, More Training Efficiency but Worse Performance.We have already examined the\nimpactofincreasing NSGD onentropydynamics, asdiscussedinAblationExperiments6(Section4.4). Consider\n28the configuration tuple(NSGD, DR, DT , Nreuse). We report the detailed time usage for the configurations (1,\n64, 64, 1), (2, 64, 32, 1), and (4, 64, 16, 1) in Table 8. It is evident that increasingNSGD leads to a highertT .\nHowever, the impact on the overall training timettotal remains minor, provided thatDR is fixed. Thus, the\nconfigurations withNSGD \u2208 {2, 4} perform multiple SGD steps within comparable training time, improving\ntraining efficiency. That said, the experimental results in Section 4.4 show that accelerating training via\nrollout batch decomposition or data reuse leads to faster entropy collapse and poorer test performance.\nTherefore, we do not recommend increasingNSGD solely for the purpose of improving training efficiency \u2013\nunless appropriate mechanisms are in place to mitigate entropy collapse, particularly those caused by off-policy\nupdates \u2013 as doing so may result in degraded generalization performance.\nExperiment\n(NSGD, DR, DT , Nreuse)\ntotal\nttotal\nrollout\ntR\npolicy update\ntT\nothers\ntO\ntR/ttotal tT /ttotal\n(1,64,64,1) 116 90 8 18 77.6% 6.9%\n(2,64,32,1) 114 87 10 17 76.3% 8.7%\n(4,64,16,1) 118 90 12 16 76.3% 10.2%\nTable 8: Detailed time usage for three experiments from Ablation Experiments 6 over 1000 training steps.\nAll the experiments utilized the same training resources (i.e., 32 H800 GPUs).\n5.2 Improving Test Performance with More Computational Resources\nIn this section, we address the second question: given more computational resources, how should training\nresources be allocated to achieve higher test performance or better training efficiency? Regarding training\nefficiency, two approaches may be considered. On the one hand, increasing the number of SGD steps \u2013\npreviously discussed \u2013 may seem promising. However, experimental findings do not support the effectiveness\nof this approach (see Section 5.1). On the other hand, under a fixed rollout budget (i.e., the number of\nsamples to be rolled out), one might expect a significant reduction in rollout timetR as training resources are\nscaled up. In practice, however, this expectation is not fully realized. Table 9 shows the rollout timetR for\nThe number of H800 32 64 128 256\nRollout timetR (reduction) 375 270 (-105) 225 (-45) 205 (-20)\nTable 9: Rollout timetR (seconds) for generating 1024 responses in one training step. The data shows that\nas computational resources increase, the incremental reduction intR diminishes.\n1024 samples under varying training resources. Notably, as training resources increase, the reduction intR\ndiminishes. This is becausetR is primarily determined by the batch size and the time required to generate\nthe longest response. Once sufficient resources are available, further scaling does not significantly reduce\nthe processing time dominated by the generation of the longest sample. Therefore, when additional training\nresources are available, a more effective strategy is to increase the rollout budget appropriately, such that the\nrollout timetR remains roughly constant or increases only marginally. By leveraging a larger rollout buffer,\nmore accurate gradient estimates can be obtained, which may improve training efficiency and enhance test\nperformance. In the following, we focus on how the rollout budget \u2013 determined by the rollout batch size and\ngroup size \u2013 affects RL performance. The overall idea of these studies are illustrated in Figure 24\nLarger Batch Size, Better Test Performance.To investigate how the rollout batch sizeDR affects the\ntraining dynamics, we conducted the following ablation experiments.\n29Figure 24: Overview of empirical studies on the effect of an increased rollout budget when more training\nresources are available.Grey and green blocks: The motivation of the empirical studies.Yellow blocks:\nThe experimental variables in the empirical studies.\nAblation Experiments 11: The Impact of Rollout Batch SizeDR\nConsider the quadruple (NSGD, DR, DT , Nreuse). We consider the baseline experiment with the\nquadruple (1,64,64,1) in Section 4.1 and two on-policy experiments in Ablation Experiments 7 with\nthe quadruples (1,32,32,1) and (1,16,16,1) respectively. These three experiments were conducted using\n64,32 and 16 H800 respectively. We present the experimental results in Figure 25.\nThe results in Figure 25 indicate that increasing the rollout batch sizeDR in accordance with available\ntraining resources can lead to better test performance with similar training time consumption.\nLarger Group Size, Better Test Performance.To investigate how the group size affects the training\ndynamics, we conducted the following ablation experiments.\nAblation Experiments 12: The Impact of Group Size (gs)\nConsider the baseline experiment with group size 16 in Section 4.1. We ran two additional on-policy\nexperiments withgs = 8, 4 respectively. These three experiments were conducted using 64,32 and 16\nH800 respectively. The experimental results are presented in Figure 26.\nIt can be observed from Figure 26, given more training resources, increasing rollout budget by increasing the\ngroup size can lead to a better test performance with similar total training hours.\n6 Dataset Preparation\nIn this section, we introduce the processing pipeline for our RL training data.\n30Figure 25: Results of Ablation Experiments 11. Given more training resources, increasing the rollout budget\nby increasingDR achieves better test performance with similar total training hours.\nFigure 26: Results of Ablation Experiments 12. Given more training resources, increasing rollout budget by\nincreasing the group size can achieve better test performance with similar total training hours.\n6.1 Data Source Selection and Preprocessing\nFor the math domain, we primarily focus on NuminaMath-1.5 [13], a comprehensive dataset containing 896K\nmath problems drawn from widely used sources and advanced mathematical topics. Although the dataset is\nsufficiently large, its quality requires careful examination prior to use.\nFor the code domain, we find that data source options are more limited, and the overall difficulty of available\ndatasets is generally low relative to the capabilities of current models. In our pilot studies, we experimented\nwith several popular datasets \u2013 including CODE-RL [12], TACO [14], and the Eurus-RL collection [2] \u2013 in\ntheir original mixtures, but obtained unsatisfactory results.\nSelection Criteria To select and curate high-quality data for RL, we adhere to the following general\ncriteria for both data domains:\n1. Verifiable: We exclude problems that cannot be verified, such as proof-based problems and code\nproblems lacking test cases.\n312. Correct: We filter out math problems with invalid or incorrect answers, as well as code problems\nwithout comprehensive test cases.\n3. Challenging: We pre-filter problems for which all N generations from the base model are either entirely\ncorrect or entirely incorrect.\nFollowing these criteria, we incorporate challenging problems from NuminaMath-1.5 and other sources\nto enhance problem difficulty and diversity in our data mixture: 1) NuminaMath-1.5 subsets: amcaime,\nolympiads, olympiadsref, aopsforum, cncontest, inequalities, and numbertheory. 2) DeepScaleR. 3) STILL-\n3-Preview-RL-Data. 4) Omni-MATH. 5) AIME problems prior to 2024. For the code data mixture, we\nprimarily consider problems from the following two sources, which offer sufficiently challenging coding\nquestions: 1) LeetCode problems [30]. 2) TACO [15].\nPreprocessing Pipeline For both math and coding problems, we first perform in-dataset deduplication to\neliminate redundancy. For all collected math problems:\n\u2022 We use Math-Verify [11] to re-extract answers from the provided textual solutions and retain only those\nproblems where the extracted answer matches the corresponding answer in the dataset.\n\u2022 We remove all instances that contain external URLs or potential figures in the problem statement.\n\u2022 We then perform cross-dataset deduplication to eliminate potentially duplicated problems from similar\nsourcesanddecontaminateagainstAIME24andAIME25problems, followingDeepScaleR\u2019sdeduplication\nscheme.\nThis process yields approximately105K math problems.For coding problems, we apply a more rigorous\nfiltering process as follows:\n\u2022 We discard samples with empty, incomplete, or corrupted original unit test cases.\n\u2022 We programmatically verify all test cases using the provided original solutions. A sample is marked as\nvalid only if the solution passes all corresponding test cases perfectly.\n\u2022 We conduct extensive deduplication based on embedding similarity across the collected coding problems,\nas many share the same problem with only slight variations in instructions.\nThis results in a total of13.7K coding questions (2.7K from LeetCode and 11K from TACO)in the\nfinal dataset.\n6.2 Model-Aware Difficulty Estimation\nDue to the zero-advantage in GRPO when all sampled responses are either entirely correct or entirely incorrect\nwithin a group, we conduct an initial offline difficulty estimation for each problem relative to the models\nbeing trained. Specifically,for each problem, we perform N=16 rollouts for math problems and\nN=8 for coding questions using a temperature of 1.0 and a maximum token length of 32K, and\nuse the percentage of correct solutions as a proxy for problem difficulty with respect to a given model. After\nverifying the correctness of the sampled solutions, we exclude problems with 0/N (all incorrect) or N/N (all\ncorrect) rollouts. We report the percentage statistics of discarded and retained math/code problems for both\nthe 7B and 32B models as follows:\n0\nN Correct N\nN Correct Remaining\n(math/code) (math/code) (math/code)\nDeepseek-R1-Distill-Qwen-7B 21.4% / 28% 32.4% / 24% 46.2% / 48%\nDeepseek-R1-Distill-Qwen-32B 20.7% / 17.1% 42.0% / 45.4% 37.3% / 37.6%\n326.3 Quality Assessment via Human and LLM-as-a-Judge\nDuring the data processing stage, we identified that many problems in the math portion were either\nincomplete or poorly formatted. Consequently, we conducted an additional round of strict human-LLM-\ncombined inspection to ensure data quality. We sampled a few hundred questions from the remaining pool\nand asked human evaluators to assess whether each problem met the following criteria:\n1. Clear Wording: Is the problem stated in a way that is easy to understand?\n2. Complete Information: Does the problem provide all necessary details?\n3. Good Formatting: Are the numbers, symbols, and equations clear and appropriately formatted?\n4. No Distractions: Is the problem free of irrelevant information?\nWe provide below examples of original problem statements that human evaluators identified as problematic:\nIncomplete Problems:\n\u2022 6. Five spherical surfaces can divide space int parts. (NuminaMath-1.5, Olympiads)\n\u2022 Which of the following numbers is equal to 33 million?(STILL-3-Preview-RL-Data)\n\u2022 Which number is greater than 0.7(STILL-3-Preview-RL-Data)\n\u2022 Example 27 Find\u03c32(28) = ? (NuminaMath-1.5, Number Theory)\nIrrelevant Information:\n\u2022 250. y = ln\n\u0000\nx3 \u2212 1\n\u0001\n.\\n\\n 250. y = ln\n\u0000\nx3 \u2212 1\n\u0001\n.\\n\\n The above text has been translated into\nEnglish, retaining the original text\u2019s line breaks and format. However, since the original text is a\nmathematical expression, the translation is identical to the original as mathematical expressions\nare generally universal and do not change between languages.(NuminaMath-1.5, Olympiads)\n\u2022 1. (12 points) The figure is composed of 5 identical squares. The number of triangles that can be\nformed using the 12 points in the figure as vertices is.10. (12 points) The figure is composed of 5\nidentical squares. The number of triangles that can be formed using the 12 points in the figure as\nvertices is.(NuminaMath-1.5, Olympiads)\nInterestingly, these problems passed the difficulty estimation procedure (i.e., a model can produce a correct\nanswer even when the problem is invalid or incomplete). This indicates that the models answered these\nproblems correctly at least once during the 16 rollouts, suggesting they may have been trained on similar\nexamples or that the answers were trivially guessable.\nTo efficiently curate the entire dataset, we employed Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct to\nautomatically filter out low-quality problems. Each model was prompted to evaluate a given math problem\nbased on clarity, completeness, formatting, and relevance, and to identify reasons a problem might be\nconsidered low quality, ultimately providing a binary rating. This process mimics human assessment while\nbeing significantly more efficient. For each problem and each LLM judge, we collected 16 evaluations, resulting\nin a total of 32 votes per problem. We retained problems that received at least 9 valid votes and removed\napproximately 1K-2K math questions in total.\n337 Math & Code Verifiers\n7.1 Math Verifiers\nDuring the initial stage of all experiments on math reasoning, we conducted several preliminary analyses of\nthe rule-based math verifiers available at the time. These verifiers included:\n\u2022 The original MATH verifier (verl version)\n\u2022 PRIME verifier\n\u2022 Qwen2.5 verifier\n\u2022 DeepScaleR\u2019s verifier\n\u2022 Math-Verify\nWe first sampled a small set of problems along with their associated solutions and answers, and manually\nexamined the quality of their parsers and verifiers. We found that the Qwen2.5 verifier tends to lose\ninformation during the parsing process (e.g., when parsing\\boxed{a^2}} $, it fails to retain^2). We also\nobserved that the PRIME verifier can occasionally stall during execution. As a result, we excluded these two\nverifiers from further analysis.\nWe then used rollout data from the difficulty estimation procedure and applied the remaining verifiers to\nevaluate the generated solutions. We plotted the number of problems at each difficulty level (0\u20138) in Figure 27:\nFigure 27: Distributions of the number of correct rollouts from DeepSeek-R1-Distill-Qwen-7B, obtained using\nfour different verifiers on a subset of NuminaMath-1.5 problems. The numbers 0\u20138 indicate difficulty levels.\nThe size of each sector represents the number of problems at a specific difficulty level.\nBased on a combination of verifier results and human judgments, we observed the following:\n\u2022 Both the original MATH verifier (verl version) and DeepScaleR\u2019s verifier produced higher rates of false\npositives and false negatives.\n\u2022 For Math-Verify, some implementation details changed as we explored different versions. Therefore,\nwe include both version 0.5.2 and the default version (0.6.0), which we extensively used in model\ndevelopment, noting only trivial differences between them.\nNote that Math-Verify may still yield incorrect results for solutions with non-standard formatting or\nmathematical expressions it does not support (e.g., problems with multiple answers).\nIn our final implementation of the reward function, we verify whether the answer in a text solution is correct\nusing the following steps:\n34\u2022 Extract the answer that appears after the reasoning process.\n\u2022 Use Math-Verify\u2019s parser to parse the answer and obtain its string representation.\n\u2022 If the string representation directly matches the gold answer, return True; otherwise, fall back to\nMath-Verify\u2019s verify function.\n\u2022 Wrap the gold answer in boxed{} and run the verification to obtain the final result.\nWe find that wrapping the gold answer withboxed{} is a crucial step. Parsing the gold answer directly can\nalter the mathematical expression.\n7.2 Code Sandboxes\nFor unit test execution, we constructed a highly efficient and secure local code sandbox based on Live-\nCodeBench\u2019s implementation, leveraging subprocess processing. This sandbox supports various testing\nmethods, including standard input-output testing, solution function unit testing, and assertion-based tests.\nTo further enhance its security and robustness, we implemented the following measures:\n\u2022 Syntax validation: We first validate submitted solutions using Abstract Syntax Trees (AST). If\nsyntax errors are detected, the sandbox immediately terminates the test and returns False.\n\u2022 Memory monitoring:During training, we identified potential memory leak risks in some generated\nsolutions. To mitigate this, we integrated a memory monitoring mechanism for each test process. If a\nprocess exceeds 50GB of memory usage, the sandbox proactively terminates the test and returns False,\neffectively preventing resource exhaustion.\n\u2022 Parallel stability optimization:Initially, we used asynchronous testing combined with process\npools for parallel execution. However, we later discovered that the sandbox could crash under this\nsetup, leading to incorrect test results. To resolve this, we revised our approach to rely solely on\nmultiprocessing, ensuring stable and efficient parallel execution.\nAdditionally, we conducted a performance comparison between our sandbox and the PRIME sandbox. The\nresults demonstrate the superior effectiveness of our implementation on specific datasets. Notably, the\nPRIME sandbox occasionally misclassified correct solutions as failures, whereas our sandbox more accurately\nevaluated solution correctness.\nIt is also important to note a limitation of our sandbox identified during practical usage: it does not currently\nhandle cases where the same input can yield multiple valid outputs. Such cases are common in real-world\ncode testing scenarios involving non-deterministic or open-ended problems.\n8 Experiments\nIn this section, we present the experimental results of our three models: Skywork-OR1-Math-7B, Skywork-\nOR1-7B, and Skywork-OR1-32B. We begin with the details of the training configurations, followed by an\nanalysis of the training results. Finally, we discuss the evaluation outcomes.\n8.1 Training and Evaluation Details\nTraining Configurations Below, we describe the training configurations of our Skywork models. The 7B\nand 32B models are fine-tuned based on DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Qwen-32B,\nrespectively. We collect math and code problems from various sources and apply comprehensive preprocessing,\ndifficulty filtering, and quality control. This ensures a problem mixture that is verifiable, valid, and challenging.\nSee Section 6 for details. Based on this curated mixture, all three models are fine-tuned by optimizing\nthe policy loss(3.1) with a constant learning rate of 1e-6, clip ratio of 0.2, target entropy of 0.2, sampling\ntemperature of 1.0, and rejection sampling. Notably, we do not apply any KL loss in our training process, as\n35discussed in Section 3.2.6. Please refer to Section 3.1 for more details on the policy update procedure. All\nexperiments use multi-stage training. We report the detailed configuration for each training stage in Table 10,\nTable 11, and Table 12. The released checkpoints correspond to step 2160 for Skywork-OR1-Math-7B, step\n1320 for Skywork-OR1-7B, and step 1000 for Skywork-OR1-32B.\nStage Steps Context Length T Batch Size Mini-batch Size Group Size\n1 0-740 8K 256 128 16\n2 740-1740 16K 256 128 16\n3 1740-2080 32K 256 128 16\n3.5 2080-2160 32K 128 64 64\nTable 10: Training configurations of Skywork-OR1-Math-7B.\nStage Steps Context Length T Batch Size Mini-batch Size Group size\n1 0-660 16K 256 256 16\n2 660-1320 32K 160 160 32\nTable 11: Training configurations of Skywork-OR1-7B.\nStage Steps Context Length T Batch Size Mini-batch Size Group Size\n1 0-760 16K 256 256 16\n2 760-1130 24K 160 160 32\nTable 12: Training configurations of Skywork-OR1-32B.\nBenchmarks & Baselines We evaluate our models on challenging benchmarks. For math capabilities, we\nassess performance on the American Invitational Mathematics Examination (AIME) 2024 and 2025. For coding\ncapabilities, weuseLiveCodeBench[ 10](from2024-08to2025-02). Wecompareagainstseveralstrongbaselines,\nincluding DeepSeek-R1 [3], Qwen3-32B [32], QwQ-32B [25], Light-R1-32B [29], TinyR1-32B-Preview [27],\nand several 7B RL models based on DeepSeek-R1-Distill-Qwen-7B, such as AceReason-Nemotron-7B [1],\nAReaL-boba-RL-7B [18], and Light-R1-7B-DS [29].\nEvaluation Setup We set the maximum generation length to 32,768 tokens for all models. For AIME24/25,\nwe report avg@32 performance; for LiveCodeBench (2024-08 to 2025-02), we report avg@4 performance.\nResponses are generated using a temperature of 1 and top-p of 1. The avg@n metric is defined as\navg@n = 1\nn\nnX\ni=1\nI{(x, yi) is correct},\nwhere x is the evaluation question andyi is thei-th response.\n8.2 Evaluation Results of Skywork-OR1 models\nAs shown in Table 13, Skywork-OR1 models achieve significant improvements over their base SFT models\n(e.g., the DeepSeek-R1-Distill series). Specifically, Skywork-OR1-32B achieves scores of 82.2 on AIME24, 73.3\non AIME25, and 63.0 on LiveCodeBench, outperforming strong contemporary models such as DeepSeek-R1\nand Qwen3-32B on key math benchmarks, setting new SOTA records at the time of release. Skywork-\nOR1-7B scores 70.2 on AIME24, 54.6 on AIME25, and 47.6 on LiveCodeBench, demonstrating competitive\n36Model AIME 24\navg@32\nAIME 25\navg@32\nLiveCodeBench\n(2024-08 - 2025-02)\navg@4\n7B Models\nDeepSeek-R1-Distill-Qwen-7B 55.5 39.2 37.6\nLight-R1-7B-DS 59.1 44.3 39.5\nAReaL-boba-RL-7B 61.9 48.3 -\nAceReason-Nemotron-7B 69.0 53.6 51.8\nSkywork-OR1-Math-7B 69.8 52.3 43.6\nSkywork-OR1-7B 70.2 54.6 47.6\n\u226532B Models\nDeepSeek-R1-Distill-Qwen-32B 72.9 59.0 57.2\nTinyR1-32B-Preview 78.1 65.3 61.6\nLight-R1-32B 76.6 64.6 -\nQwQ-32B 79.5 65.3 61.6\nQwen3-32B 81.4 72.9 65.7\nDeepSeek-R1 79.8 70.0 65.9\nSkywork-OR1-32B 82.2 73.3 63.0\nTable 13: Comparison of Skywork-OR1 models and other models on reasoning-related benchmarks.\nperformance relative to similarly sized models across both math and coding tasks. Our earlier released\nmodel, Skywork-OR1-Math-7B, also delivers competitive results among models of similar size, scoring 69.8 on\nAIME24, 52.3 on AIME25, and 43.6 on LiveCodeBench. These SOTA results are especially noteworthy given\nthat they are obtained through fine-tuning the DeepSeek-R1-Distill series \u2013 SFT base models with relatively\nmodest initial performance \u2013 clearly demonstrating the substantial impact of our pipeline.\n9 Conclusion\nIn this work, we present Skywork-OR1, an effective and scalable reinforcement learning (RL) implementation\nfor enhancing the reasoning capabilities of long CoT models. Building upon the DeepSeek-R1-Distill model\nseries, our RL approach achieves significant performance improvements on various mathematical and coding\nbenchmarks. The Skywork-OR1-32B model outperforms both DeepSeek-R1 and Qwen3-32B on AIME24 and\nAIME25, while delivering comparable results on LiveCodeBench. Additionally, the Skywork-OR1-7B and\nSkywork-OR1-Math-7B models demonstrate competitive reasoning performance among similarly sized models.\nOur comprehensive ablation studies validate the effectiveness of the core components in our training pipeline,\nincluding data mixture and filtration, multi-stage training without advantage masking, high-temperature\nsampling, exclusion of KL loss, and adaptive entropy control. We conduct extensive investigations into\nentropy collapse phenomena, identifying key factors that influence entropy dynamics. Our findings show that\npreventing premature entropy collapse is critical for achieving optimal test performance, offering valuable\ninsights for future research and development. Furthermore, we explore how different training resource\nallocations affect both training efficiency and final model performance.\nReferences\n[1] Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Mohammad Shoeybi Peng Xu, and Wei Ping\nBryan Catanzaro. Acereason-nemotron: Advancing math and code reasoning through reinforcement\nlearnin, 2025.\n37[2] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu\nYu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan\nYao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process\nreinforcement through implicit rewards.CoRR, abs/2502.01456, 2025.\n[3] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\n[4] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang\nChen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang,\nLei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: A universal\nolympiad level mathematic benchmark for large language models, 2024.\n[5] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of\nmodels. arXiv preprint arXiv:2407.21783, 2024.\n[6] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\nMa, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\n[7] Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang\nZhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu,\nand Yahui Zhou. Skywork open reasoner series.https://capricious-hydrogen-41c.notion.site/S\nkywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680 , 2025. Notion Blog.\n[8] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-\nreasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025.\n[9] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Hel-\nyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024.\n[10] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation\nof large language models for code.arXiv preprint, 2024.\n[11] Hynek Kydl\u00ed\u010dek. Math-verify: A robust mathematical expression evaluation system.https://github.c\nom/huggingface/Math-Verify, 2025. Version 0.6.1.\n[12] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu-Hong Hoi. Coderl:\nMastering code generation through pretrained models and deep reinforcement learning. InAdvances\nin Neural Information Processing Systems 35: Annual Conference on Neural Information Processing\nSystems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.\n[13] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif\nRasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume\nLample, and Stanislas Polu. Numinamath.[https://huggingface.co/AI-MO/NuminaMath-1.5](ht\ntps://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.p\ndf), 2024.\n[14] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li.\nTACO: topics in algorithmic code generation dataset.CoRR, abs/2312.14852, 2023.\n[15] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li.\nTaco: Topics in algorithmic code generation dataset.arXiv preprint arXiv:2312.14852, 2023.\n[16] Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel\nXin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder:\n38A fully open-source 14b coder at o3-mini level.https://pretty-radio-b75.notion.site/DeepCoder\n-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51 , 2025.\nNotion Blog.\n[17] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai,\nJeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with a\n1.5b model by scaling rl.https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-P\nreview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2 , 2025. Notion\nBlog.\n[18] Ant Research RL Lab. Areal: Ant reasoning rl.https://github.com/inclusionAI/AReaL, 2025.\n[19] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional\ncontinuous control using generalized advantage estimation.arXiv preprint arXiv:1506.02438, 2015.\n[20] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[21] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300, 2024.\n[22] Richard S. Sutton and Andrew G. Barto.Reinforcement learning: An introduction. MIT press, 2nd\nedition, 2018.\n[23] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods\nfor reinforcement learning with function approximation. InAdvances in Neural Information Processing\nSystems, pages 1057\u20131063, 1999.\n[24] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,\nChenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms.arXiv\npreprint arXiv:2501.12599, 2025.\n[25] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025.\n[26] RUCAIBox STILL Team. Still-3-1.5b-preview: Enhancing slow thinking abilities of small models through\nreinforcement learning. 2025.\n[27] TinyR1 Team. Superdistillation achieves near-r1 performance with just 5\n[28] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang,\nXiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond.arXiv\npreprint arXiv:2503.10460, 2025.\n[29] Liang Wen, Fenrui Xiao, Xin He, Yunke Cai, Zhenyu Duan Qi An, Yimin Du, Junchen Liu, Lifu Tang,\nXiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Surpassing\nr1-distill from scratch with $1000 through curriculum sft & dpo, 2025.\n[30] Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu.\nLeetcodedataset: A temporal dataset for robust evaluation and efficient training of code llms.arXiv\npreprint arXiv:2504.14655, 2025.\n[31] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu,\nZhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning,\n2025.\n[32] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,\nHaoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\n39Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng,\nMei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan\nLiu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang,\nXuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun\nWang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report.arXiv preprint\narXiv:2505.09388, 2025.\n[33] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\nLiu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report.arXiv preprint arXiv:2412.15115, 2024.\n[34] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu,\nLingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale.arXiv\npreprint arXiv:2503.14476, 2025.\n[35] Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian\nFan, Zhengyin Du, Xiangpeng Wei, et al. Vapo: Efficient and reliable reinforcement learning for advanced\nreasoning tasks. arXiv preprint arXiv:2504.05118, 2025.\n[36] Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning\nDing, and Bowen Zhou. Ttrl: Test-time reinforcement learning.arXiv preprint arXiv:2504.16084, 2025.\n40",
      " \n \n Executive  Summary  \n Agents\u2014 AI  systems  that  can  autonomously  achieve  goals  in  the  world,  with  little  to  no  \nexplicit\n \nhuman\n \ninstruction\n \nabout\n \nhow\n \nto\n \ndo\n \nso\n\u2014are\n \na\n \nmajor\n \nfocus\n \nof\n \nleading\n \ntech\n \ncompanies,\n \nAI\n \nstart-ups,\n \nand\n \ninvestors.\n \nIf\n \nthese\n \ndevelopment\n \ne\ufb00orts\n \nare\n \nsuccessful,\n \nsome\n \nindustry\n \nleaders\n \nclaim\n \nwe\n \ncould\n \nsoon\n \nsee\n \na\n \nworld\n \nwhere\n \nmillions\n \nor\n \nbillions\n \nof\n \nagents\n \nare\n \nautonomously\n \nperforming\n \ncomplex\n \ntasks\n \nacross\n \nsociety.\n \nSociety\n \nis\n \nlargely\n \nunprepared\n \nfor\n \nthis\n \ndevelopment\n.\n1\n  Today,  the  leading  approach  for  developing  agents  leverages  recent  advances  in  foundation  \nmodels\n \nlike\n \nChatGPT\n \nand\n \nClaude.\n \nSca\ufb00olding\n \nsoftware\n \nis\n \nbuilt\n \naround\n \nthese\n \nmodels\n \nwhich\n \nallow\n \nthem\n \nto\n \ninteract\n \nwith\n \nvarious\n \ntools\n \nand\n \nservices\u2014enabling\n \nthem\n \nto\n \nhave\n \nlong-term\n \nmemory,\n \nplan\n \nand\n \ninteract\n \nwith\n \nother\n \nagents,\n \nand\n \ntake\n \nactions\n \nin\n \nthe\n \nworld.\n  While  today\u2019s  agents  can  do  a  variety  of  things \u2014from  identifying  critical  vulnerabilities  in  \nsoftware\n \nto\n \nordering\n \nbooks\n \non\n \nAmazon\u2014\nthey\n \nstill\n \nface\n \nserious\n \nlimitations\n \nin\n \ncompleting\n \nmore\n \ncomplex,\n \nopen-ended,\n \nlonger\n \ntime-horizon\n \ntasks\n.\n2\n \nAgents\n \nhave\n \nmajor\n \nissues\n \nwith\n \nreliability,\n \nas\n \nwell\n \nas\n \nlimitations\n \nin\n \nreasoning\n \nand\n \ndigital\n \ntool\n \nuse.\n \nThere\n \nare\n \nalso\n \npotential\n \nbarriers\n \nto\n \nadoption\n \nif\n \nthe\n \nprocessing\n \npower\n \nneeded\n \nto\n \nrun\n \nagents\n \nis\n \ncost-prohibitive.\n \nFor\n \nexample,\n \nnewer\n \nAI\n \nsystems\n \nthat\n \ncan\n \n'think\n \nthrough'\n \ncomplex\n \nproblems\n \nstep-by-step\n \n(like\n \nthe\n \nrecently\n \ndeveloped\n \n'reasoning\n \nmodels')\n \nrequire\n \nmuch\n \nmore\n \nprocessing\n \npower\n \nwhen\n \nanswering\n \nquestions\n \nor\n \nperforming\n \ntasks,\n \nwhich\n \ncan\n \nsubstantially\n \ndrive\n \nup\n \nthe\n \nenergy\n \nand\n \nserver\n \ncosts\n \nneeded\n \nto\n \noperate\n \nthese\n \nsystems.\n \n  Benchmarks  designed  to  evaluate  the  performance  of  agents  on  real-world  tasks  consistently  \ufb01nd  \nthat\n \nwhile\n \ncurrent\n \nagents\n \nperform\n \ncomparably\n \nto\n \nhumans\n \non\n \nshorter\n \ntasks,\n \nthey\n \ntend\n \nto\n \nperform\n \nconsiderably\n \nworse\n \nthan\n \nhumans\n \non\n \ntasks\n \nthat\n \nwould\n \ntake\n \nan\n \nequivalent\n \nhuman\n \none\n \nor\n \nmore\n \nhours\n \nto\n \ncomplete.\n  \n2\n For  an  example  of  an  agent  identifying  critical  vulnerabilities  in  real-world  code,  see  Google\u2019s  Project  Zero  \nblog\n \non\n \nBig\n \nSleep\n \n(Project  Zero  2024 ).  For  a  visual  demo  of  a  browser  agent  being  used  to  make  an  online  \nshopping\n \npurchase,\n \nsee\n \nthis\n \ndemo\n \n(AI  Digest  2024 )\n.\n \n1 \n Meta\u2019s  CEO,  Mark  Zuckerberg,  told  investors  he  wants  to  \u201cintroduce  AI  agents  to  billions  of  people\u201d  (Heath  \n2023\n) and  Salesforce  CEO  Marc  Benio\ufb00  predicted  there  would  be  one  billion  AI  agents  in  service  by  the  end  \nof\n \nFY2026\n \n(Sozzi  2024 ).  \n      AGENT  GOVERNANCE   \u2502   1   \nTable  1:  Agent  performance  on  various  benchmarks  representing  \nreal-world\n \ntasks\n \n(as\n \nof\n \nDecember\n \n2024)\n3\n \nAgent  benchmark  \nPerformance  \nGeneral  AI  Assistants  (GAIA)  \nTests  real-world  assistant  capabilities  across  personal  tasks,  science,  and  general  knowledge.  Human  accuracy  (92%)  far  exceeds  best  agent  performance  (15% ) ,  with  agents  completely  failing  on  complex  multi-step  tasks.  \nMETR  Autonomy  Capability  Evals  \nEvaluates  skills  in  cybersecurity,  software  engineering,  and  machine  learning.  Agents  perform  comparably  to  humans  on  tasks  taking  ~30  minutes,  but  complete  less  than  20%  of  tasks  requiring  1+  hours  of  human  time.  \nRE-Bench  A  benchmark  for  evaluating  the  AI  agents'  ability  to  automate  the  work  of  experienced  AI  R&D  researchers.  Agents  outperform  humans  in  2-hour  tasks  (4\u00d7  better  scores),  but  humans  excel  with  longer  timeframes\u2014slightly  better  at  8  hours  and  doubling  agent  performance  when  given  32  hours.   \nCyBench  Assesses  cybersecurity  capabilities  through  professional-level  Capture  the  Flag  challenges.  Agents  struggled  with  tasks  that  take  human  teams  more  than  11  minutes  to  complete.  \nSWE-bench  Veri\ufb01ed  Features  real-world  software  engineering  problems  from  GitHub  issues.  Agent  performance  drops  dramatically  for  problems  taking  humans  1+  hour  to  resolve  (20.8%  \u2192  4.8%  \u2192  0%  as  task  complexity  increases).  \nWebArena  Evaluates  how  agents  navigate  and  extract  information  from  websites.  The  best  agent  achieved  only  14.41%  success  rate  compared  to  human  performance  of  78.24%.  \n   However,  despite  these  limitations,  today's  agents  are  already  providing  economic  value  in  a  variety  \nof\n \nearly-adoption\n \n\ufb01elds\n \nsuch\n \nas\n \ncustomer\n \nservice,\n \nAI\n \nR&D,\n \nand\n \ncybersecurity.\n \nFor\n \ninstance,\n \na\n \n\ufb01ntech\n \ncompany,\n \nKlarna,\n \nclaims\n \nit\n \nhas\n \nagents\n \nperforming\n \nthe\n \ncustomer\n \nservice\n \nwork\n \nof\n \n~700\n \nFTE\n human  employees  with  no  reduction  in  customer  satisfaction  (Klarna  2024),  and  Google's  CEO  has  stated  over  a  quarter  of  all  new  code  at  Google  is  now  generated  by  coding  assistants  (Pichai  2024).  Researchers  found  that  for  speci\ufb01c  tasks  that  both  humans  and  agents  perform  well  at,  \u201cthe  \n3\n See  Appendix for  more  detailed  breakdown  of  agent  performance  across  various  agentic  benchmarks  \n      AGENT  GOVERNANCE   \u2502   2   \naverage  cost  of  using  a  foundation  model-based  agent  is  around  1/30th  of  the  median  hourly  wage  of  a  US  bachelor\u2019s  degree  holder\u201d  (METR  2024).  Also,  researchers  have  found  that  the  length  of  tasks  that  AIs  can  complete  is  doubling  every  7  months  (Kwa  et  al.  2025).   Some  researchers  have  claimed  that  widespread  deployment  of  agents  as  digital  workers  could  \nlead\n \nto\n \n\u2018explosive\n \neconomic\n \ngrowth,\u2019\n \ni.e.,\n \nan\n \nacceleration\n \nof\n \ngrowth\n \nrates\n \nby\n \nan\n \norder\n \nof\n \nmagnitude,\n similar  to  the  impact  of  the  Industrial  Revolution  (Erdil  and  Besiroglu  2024).  However,  skeptics  argue  that  signi\ufb01cant  bottlenecks  remain,  including  AI  systems'  limited  ability  to  perform  physical  \ntasks,\n \nthe\n \nchallenges\n \nof\n \nintegrating\n \ndigital\n \nand\n \nphysical\n \nproduction\n \nprocesses,\n \nand\n \nthe\n \npossibility\n \nthat\n \nAI\n \ncapabilities\n \nmight\n \nplateau\n \nbefore\n \nreaching\n \nthe\n \nlevel\n \nneeded\n \nto\n \nperform\n \nmost\n \nif\n \nnot\n \nall\n \ntasks\n currently  performed  by  humans  (Clancy  and  Besiroglu  2023).   Additionally,  there  are  several  promising  pathways  to  improve  agent  performance  and  strengthen  \ninstitutional\n \ncapacity\n \nto\n \ndeploy\n \nAI\n \nsystems\n \nsafely\u2014which\n \nmeans\n \nthat\n \nleading\n \nAI\n \ncompanies\n \nexpect\n \nmany\n \nof\n \nthese\n \nlimitations\n \nto\n \nbe\n \novercome\n \nover\n \nthe\n \ncoming\n \nmonths\n \nand\n \nyears.\n4\n \nOne\n \npromising\n \ndevelopment\n \nis\n \nthe\n \nemergence\n \nof\n \nthe\n \n\u201ctest-time\n \ncompute\u201d\n \nparadigm.\n \nThese\n \nmodels,\n \nsuch\n \nas\n \nOpen\n \nAI\u2019s\n \no1\n \nand\n \no3,\n \nare\n \nable\n \nto\n \ndynamically\n \nallocate\n \ncompute\n \nduring\n \ninference\n \nto\n \nessentially\n \nthink\n \nlonger\n \nand\n \nharder\n \nabout\n \nany\n \ngiven\n \ntask\n \n.\n \nAn\n \no3-based\n \nagent\n \nreportedly\n \nscored\n \n71.7%\n \non\n \nSWE-bench\n Veri\ufb01ed,  a  widely  used  benchmark  for  testing  software  engineering  capabilities  (Franzen  and  David  2024).  This  far  outperformed  the  next  highest-ranking  agent,  which  scored  48.9%.\n 5\n \nA  future  where  capable  agents  are  deployed  en  masse  could  see  transformative  bene\ufb01ts  to  society,  \nbut\n \nalso\n \nprofound\n \nand\n \nnovel\n \nrisks:\n \n\u25cf  Malicious  use :  AI  agents  can  amplify  malicious  activities,  such  as  spreading  \ndisinformation,\n \nautomating\n \ncyberattacks,\n \nor\n \nadvancing\n \ndual-use\n \nscienti\ufb01c\n \nresearch\n \nlike\n \nbioweapon\n \ndevelopment.\n \nTheir\n \nability\n \nto\n \nexecute\n \nmulti-step\n \nplans\n \nautonomously\n \nheightens\n \nthe\n \npotential\n \nfor\n \nabuse\n \nby\n \nlowering\n \nbarriers\n \nto\n \nentry\n \nand\n \ncosts\n \ninvolved\n \nin\n \nthese\n \nactivities.\n \n\u25cf  Accidents  and  loss  of  control :  Failures  in  agent  systems  range  from  mundane  errors  \n(e.g.,\n \nincorrect\n \noutputs\n \nor\n \nnavigation\n \nmishaps)\n \nto\n \nsevere\n \n\u201closs\n \nof\n \ncontrol\u201d\n \nscenarios,\n \nwhere\n \nhumans\n \nlose\n \nvisibility\n \ninto\n \nthe\n \noperation\n \nof\n \nagents,\n \nthe\n \nability\n \nto\n \nidentify\n \nand\n \nredirect\n \nharmful\n \nbehaviors,\n \nand\n \nthe\n \nability\n \nto\n \nre-implement\n \ncontrol\n \nof\n \nAI-driven\n \nsystems\n \nin\n \nsociety.\n \nThis\n \nincludes\n \nrisks\n \nlike\n \nrogue\n \nreplication\n \nor\n \nagents\n \npursuing\n \ngoals\n \nthat\n \nare\n \nnot\n \naligned\n \nwith\n \nhuman\n \nvalues.\n \n5\n SWE-bench  Veri\ufb01ed  is  an  evaluation  suite  composed  of  realistic  software  engineering  tasks  (OpenAI  \n2024a\n).  \n4\n For  example,  the  CEO  of  OpenAI,  Sam  Altman  stated  that  \u201cIn  2025,  we  may  see  the  \ufb01rst  AI  agents  join  the  \nworkforce\n \nand\n \nmaterially\n \nchange\n \nthe\n \noutput\n \nof\n \ncompanies\u201d\n \n(Altman  2025 ).  \n      AGENT  GOVERNANCE   \u2502   3   \n\u25cf  Security  risks :  Agents,  with  their  expanded  access  to  tools  and  external  systems,  face  \nvulnerabilities\n \nsuch\n \nas\n \nmemory\n \nmanipulation,\n \nexploitation\n \nthrough\n \nweak\n \nintegrations,\n \nand\n \ncascading\n \ne\ufb00ects\n \nin\n \nmulti-agent\n \nenvironments.\n \nThese\n \nrisks\n \nmake\n \nthem\n \nmore\n \nsusceptible\n \nto\n \nsevere\n \nbreaches\n \ncompared\n \nto\n \nconventional\n \nAI.\n \n\u25cf  Other  systemic  risks :  Large-scale  agent  deployment  could  lead  to  labor  displacement  \nand\n \nextreme\n \npower\n \nconcentration\n \namong\n \ntechnological\n \nand\n \npolitical\n \nelites,\n \nand\n \npotential\n \nerosion\n \nof\n \ndemocratic\n \naccountability.\n \nAgents\n \ncould\n \nexacerbate\n \ninequality\n \nor\n \nbe\n \nleveraged\n \nfor\n \nsocietal\n \ncontrol.\n \nAgent  governance  is  a  nascent  \ufb01eld  focused  on  preparing  for  a  world  in  which  AI  agents  can  \ncarry\n \nout\n \na\n \nwide\n \narray\n \nof\n \ntasks\n \nwith\n \nhuman-level-or-above\n \npro\ufb01ciency.\n \nSome\n \nof\n \nthe\n \nmajor\n \nareas\n \nin\n \nagent\n \ngovernance\n \ninclude:\n \n\u25cf  Monitoring  and  evaluating  agent  performance  and  risks :  How  can  we  e\ufb00ectively  \nmonitor\n \nand\n \nevaluate\n \nthe\n \nperformance\n \nand\n \nassociated\n \nrisks\n \nof\n \nincreasingly\n \nautonomous\n \nand\n \ncomplex\n \nagents\n \nover\n \ntime?\n \n \n\u25cf  Develop  mechanisms  and  structures  for  managing  risks  from  agents  across  their  \nlifecycle\n:\n \nWhat\n \ntechnical,\n \nlegal,\n \nand\n \npolicy-based\n \ninterventions\n \nshould\n \nbe\n \nimplemented\n \nto\n \nensure\n \nagents\n \noperate\n \nsafely\n \nand\n \ntransparently,\n \nwhile\n \nmaintaining\n \naccountability?\n \nWhat\n \nare\n \nthe\n \nsystemic\n \nrisks\n \nand\n \nconsequences\n \nof\n \nwidespread\n \nagent\n \nadoption\n \non\n \npolitical\n \nand\n \neconomic\n \nstructures?\n \nThe\n \n\u2018Agent\n \ninterventions\n \ntaxonomy\u2019\n \ntable\n \nbelow\n \noutlines\n \ngovernance\n \noutcomes\n \ninterventions\n \ncan\n \nhelp\n \nachieve.\n \n \n\u25cf  Incentivizing  bene\ufb01cial  uses  of  agents :  What  bene\ufb01cial  use  cases  of  agents  should  be  \nprioritized\n \nand\n \nhow?\n \n\u25cf  Adapting  existing  policy  and  legal  frameworks  and  developing  new  instruments  \nfor\n \nagent\n \ngovernance\n:\n \nAnticipating\n \nwhat\n \npolicy\n \nand\n \nlegal\n \ninstruments\n \nwill\n \nbe\n \nneeded\n \nin\n \na\n \nworld\n \nwith\n \nmass\n \ndeployment\n \nof\n \nadvanced\n \nagent\n \nsystems.\n \n\u25cf  Agents  f o r  governance :  To  what  extent  should  agents  themselves  participate  in  \ngovernance\n \ntasks?\n \nAdvanced\n \nagents\n \ncould\n \npotentially\n \nact\n \nas\n \nmonitors,\n \nmediators,\n \nor\n \nenforcers\n \nwithin\n \ngovernance\n \nframeworks.\n \nOne  of  the  pressing  needs  in  agent  governance  is  to  develop  agent  interventions ,  i.e.,  measures,  \npractices,\n \nor\n \nmechanisms\n \ndesigned\n \nto\n \nprevent,\n \nmitigate,\n \nor\n \nmanage\n \nthe\n \nrisks\n \nassociated\n \nwith\n \nagents.\n \nThese\n \naim\n \nto\n \nensure\n \nthat\n \nagents\n \noperate\n \nsafely,\n \nethically,\n \nand\n \nin\n \nalignment\n \nwith\n \nhuman\n \nvalues\n \nand\n \nintentions.\n \nWe\n \nhave\n \ndeveloped\n \nan\n \noutcomes-based\n \ntaxonomy\n \nof\n \nagent\n \ninterventions\n6\n:\n \n6\n A  majority  of  these  interventions  have  been  proposed  by  civil  society  or  industry  researchers,  but  many  \nhave\n \nnot\n \nbeen\n \ndeveloped\n \nor\n \nimplemented\n \nat\n \nscale.\n \n      AGENT  GOVERNANCE   \u2502   4   \nTable  2:  Agent  interventions  taxonomy\n \nCategories  De\ufb01nition  Example  interventions  \nAlignment Measures  to  ensure  that  agent  systems  behave  in  ways  that  are  consistent  with  a  given  principal\u2019s  values,  intentions,  and  interests  (i.e.,  are  aligned)  and  also  establish  trust  that  these  systems  are  actually  su\ufb03ciently  aligned.   \n\u25cf  Multi-agent  reinforcement  learning  \u25cf  Aligning  agent  risk  attitudes  \u25cf  Paraphrasing  model  outputs  to  defend  against  encoded  reasoning  \u25cf  Alignment  evaluations  \nControl Measures  that  constrain  the  behavior  of  AI  agents  to  ensure  they  operate  within  prede\ufb01ned  boundaries.  This  includes  measures  that  prevent  agents  from  executing  harmful  actions.  \n\u25cf  Rollback  infrastructure  \u25cf  Shutdown  and  interruption  mechanisms  \u25cf  Restricting  speci\ufb01c  agent  actions  and  tool-use  \u25cf  Control  protocols  and  evaluations  \nVisibility Measures  that  make  the  behavior,  capabilities,  and  actions  of  AI  systems  understandable  and  observable  to  humans.  \n\u25cf  Agent  IDs  \u25cf  Activity  logging  \u25cf  Cooperation-relevant  capabilities  evaluations  \u25cf  Reward  reports  \nSecurity  and  robustness \nMeasures  intended  to  secure  agent  systems  from  various  external  threats,  protect  the  integrity  and  con\ufb01dentiality  of  data,  and  ensure  reliable  performance  even  under  adverse  conditions.   \n\u25cf  Access  controls  \u25cf  Adversarial  robustness  testing  \u25cf  Sandboxing  \u25cf  Rapid  response  for  adaptive  defense  \nSocietal  integration \nMeasures  intended  to  support  long-term  integration  of  agents  into  existing  social,  political,  and  economic  systems\u2014addressing  issues  such  as  inequality,  concentration  of  power,  and  establishing  accountability  structures.  \n\u25cf  Liability  regimes  for  AI  agents  \u25cf  Commitment  devices  \u25cf  Equitable  agent  access  schemes  \u25cf  Developing  law-following  AI  agents  \n \nCurrently,  exploration  of  agent  governance  questions  and  development  of  associated  interventions  \nremains\n \nin\n \nits\n \ninfancy.\n \nOnly\n \na\n \nsmall\n \nnumber\n \nof\n \nresearchers,\n \nprimarily\n \nin\n \ncivil\n \nsociety\n \n      AGENT  GOVERNANCE   \u2502   5   \norganizations,  public  research  institutes,  and  frontier  AI  companies,  are  actively  working  \non\n \nthese\n \nchallenges\n.\n \nMany\n \nproposed\n \ninterventions\n \nexist\n \nprimarily\n \nas\n \ntheoretical\n \nconcepts\n \nrather\n \nthan\n \ntested\n \nsolutions,\n \nand\n \nthere\n \nare\n \nsigni\ufb01cant\n \ngaps\n \nin\n \nour\n \nunderstanding\n \nof\n \nhow\n \nto\n \nimplement\n \nthem\n \ne\ufb00ectively.\n \nWhile\n \nsome\n \norganizations\n \nhave\n \nbegun\n \nproviding\n \ntargeted\n \nfunding\n \nfor\n \nagent\n \ngovernance\n \nresearch,\n \nand\n \nthe\n \ntopic\n \nis\n \ngaining\n \nincreased\n \nattention\n \nat\n \nacademic\n \nconferences,\n \nthe\n \n\ufb01eld\n \nremains\n \nrelatively\n \nneglected\n \ncompared\n \nto\n \nother\n \nareas\n \nof\n \nAI\n \ngovernance.\n \n \nThe  pace  of  progress  in  developing  agent  capabilities  is  rapidly  outstripping  our  \nadvancement\n \nin\n \ngovernance\n \nsolutions\n\u2014we\n \nlack\n \nrobust\n \nanswers\n \nto\n \nfundamental\n \nquestions\n \nabout\n \nhow\n \nto\n \nensure\n \nsafe\n \nagents\n \nor\n \nmanage\n \ntheir\n \nbroader\n \nsocietal\n \nimpacts.\n \nThere\n \nis\n \ntremendous\n \nopportunity\n \nand\n \nneed\n \nfor\n \nresearchers\n \nand\n \ntechnologists\n \nfrom\n \ncivil,\n \nindustry,\n \nand\n \ngovernment\n \nto\n \nhelp\n \nprogress\n \nthe\n \n\ufb01eld,\n \nfrom\n \n\ufb02eshing\n \nout\n \nand\n \ntesting\n \ntheoretical\n \nproposals\n \nto\n \ncreating\n \nsolutions\n \nthat\n \ncan\n \nbe\n \nimplemented\n \nby\n \nAI\n \ndevelopers\n \nand\n \npolicymakers.\n    \n      AGENT  GOVERNANCE   \u2502   6   \n \nTable  of  Contents\n \n \nExecutive  Summary ..................................................................................................................... 1 Table  of  Contents ......................................................................................................................... 7 1.  Introduction .............................................................................................................................. 8 1.1  Two  visions  of  an  agent-\ufb01lled  future ..................................................................................... 9 2.  What  are  AI  agents? .............................................................................................................. 13 2.1  How  capable  are  agents  today? ........................................................................................ 16 2.2  Pathways  to  better  agents ................................................................................................ 21 2.3  AI  agent  adoption ............................................................................................................. 22 3.  Risks  from  AI  agents ............................................................................................................. 27 3.1  Malicious  use .................................................................................................................... 27 3.2  Accidents  and  loss  of  control ............................................................................................ 28 3.3  Security  risks .................................................................................................................... 30 3.4  Other  systemic  risks .......................................................................................................... 31 4.  What  is  agent  governance? .................................................................................................. 31 5.  Agent  interventions ............................................................................................................... 34 5.1  Alignment ......................................................................................................................... 36 5.2  Control .............................................................................................................................. 39 5.3  Visibility ............................................................................................................................. 41 5.4  Security  and  robustness ................................................................................................... 43 5.5  Societal  integration ............................................................................................................ 45 6.  Conclusion ............................................................................................................................. 48 Acknowledgements ................................................................................................................... 50 Appendix .................................................................................................................................... 51 Bibliography ............................................................................................................................... 54 \n  \n  \n      AGENT  GOVERNANCE   \u2502   7   \n \n1.  Introduction  \nAgents:  the  next  frontier  in  arti\ufb01cial  intelligence? \nOn  the  second  day  of  Salesforce\u2019s  annual  conference,  one  of  the  largest  tech  events  in  the  world,  a  \ncrowd\n \nof\n \n40,000\n \nattendees\n \nlistened\n \nto\n \nCEO\n \nMarc\n \nBenio\ufb00\n \nannounce\n \ntheir\n \nnew\n \nplatform\n \nAgentForce,\n for  enterprises  to  build  and  deploy  AI-powered  agents  (Sozzi  2024).  Salesforce  declared  AI  agents  as  the  \u2018third  wave  of  the  AI  revolution,\u2019  with  Benio\ufb00  predicting  there  would  be  one  billion  AI  agents  \nin\n \nservice\n \nby\n \nthe\n \nend\n \nof\n \nFY2026.\n \nAI\n \nAgents\n \nare\n \nAI\n \nsystems\n \nthat\n \ncan\n \nautonomously\n \nachieve\n \ngoals\n \nin\n \nthe\n \nworld,\n \nwith\n \nlittle\n \nto\n \nno\n \nexplicit\n \nhuman\n \ninstruction\n \nabout\n \nhow\n \nto\n \ndo\n \nso.\n  Established  tech  companies,  small  startups,  and  open-source  developers  are  pouring  billions  of  \ndollars\n \ninto\n \nmaking\n \nAI\n \nagents\n \na\n \nreality.\n \nIn\n \n2023,\n \nopen-source\n \ne\ufb00orts\n \nlike\n \nAutoGPT\n \nand\n \nBabyAGI\n \nbuilt\n \nsoftware\n \naround\n \nlarge\n \nlanguage\n \nmodels\n \n(LLMs)\n \nto\n \nallow\n \nthem\n \nto\n \nfunction\n \nas\n \nagents.\n \nAmazon\n \nhired\n \nmost\n \nof\n \nthe\n \nteam\n \nbehind\n \nAdept,\n \na\n \nleading\n \nAI\n \nagent\n \nstartup\n \nin\n \nJune\n \n2024.\n \nIn\n \nJanuary\n \n2025,\n OpenAI  released  a  preview  of  Operator,  a  computer-use  agent  (OpenAI  2025a).  Shortly  after,  Anthropic  released  a  preview  for  Claude  Code,  an  agentic  coding  tool  (Anthropic  2025).  Google\u2019s  Project  Astra  involves  developing  a  prototype  AI  assistant  that  can  operate  across  multiple  devices,  including  phones  and  AR/VR  glasses  (Google  DeepMind  2025).  Meta\u2019s  CEO,  Mark  Zuckerberg,  told  investors  he  wants  to  \u201cintroduce  AI  agents  to  billions  of  people\u201d  (Heath  2023).   \n      AGENT  GOVERNANCE   \u2502   8   \n\u201cI  think  we're  going  to  live  in  a  world  where  there  are  going  to  be  hundreds  of  millions  or  billions  of  di\ufb00erent  AI  agents  eventually,  probably  more  AI  agents  than  there  are  people  in  the  world.\u201d \n \n(Zuckerberg  2024)\n \n If  these  e\ufb00orts  are  successful,  we  could  see  a  future  where  millions  of  AI  agents  autonomously  \nperform\n \ncomplex\n \nreal-world\n \ntasks,\n \nsuch\n \nas\n \nbooking\n \n\ufb02ights,\n \nmanaging\n \na\n \ncompany's\n \nsupply\n \nchains,\n \nor\n \neven\n \nmaking\n \nscienti\ufb01c\n \ndiscoveries.\n \nWhat\n \ncould\n \nthis\n \nworld\n \nlook\n \nlike?\n \n1.1  Two  visions  of  an  agent-\ufb01lled  future  \nThe  scenarios  below  describe  two  di\ufb00erent  futures  where  advanced  AI  agents  proliferate,  but  \nsociety\n \nhas\n \nmade\n \ndi\ufb00erent\n \ndecisions\n \nin\n \nthe\n \nlead-up\n \nto\n \nthis\n \nworld.\n \n      AGENT  GOVERNANCE   \u2502   9   \nSCENARIO  1:  AGENT-DRIVEN  RENAISSANCE   In  response  to  challenges  with  limited  access  to  care  and  increasing  social  isolation  faced  by  aging  populations,  an  AI  company  has  partnered  with  local  governments  to  deploy  elder  companion  agents  able  to  help  with  medication  reminders,  health  monitoring,  and  household  tasks  like  managing  appointments.  For  elderly  people  living  alone,  the  agent  helps  reduce  isolation  by  arranging  virtual  meet-ups  with  family  members  or  local  volunteer  programs.  One  of  the  agents\u2019  most  beloved  features  is  its  personalized  memory  system,  which  allows  it  to  reminisce  with  the  elderly  about  their  lives,  recalling  signi\ufb01cant  events  and  cherished  memories.   \nIn  this  world,  public-private  partnerships  have  enabled  virtually  universal  access  to  agents  via  a  global  scheme.  These  agents  provide  individuals  the  freedom  and  agency  to  live  their  lives  in  a  way  that  they  value.  These  systems  help  navigate  complex  legal  processes  like  \ufb01ling  for  bene\ufb01ts,  and  support  personalized  learning.  Agents  are  also  able  to  accelerate  R&D  in  domains  like  biomedicine  and  materials  science,  where  the  bene\ufb01ts  are  expected  to  bene\ufb01t  society  broadly.  These  systems  even  have  a  place  in  online  discussion  forums,  strategically  interjecting  to  mediate  disputes  and  identify  critical  points  of  agreement  and  disagreement.   Agents  are  also  frequently  employed  for  defensive  purposes.  For  example,  while  some  scammers  use  black  market  \u2018jailbroken\u2019  agent  systems  to  aid  in  social  engineering  attacks  on  users,  there  are  agents  that  can  help  users  identify  these  attacks.  Overall,  various  defensive  systems  exist  to  prevent  or  interrupt  attack  attempts  from  malicious  attackers.  Fallback  systems  are  maintained  in  case  of  unexpected  failures  or  adversarial  attacks  on  agents  themselves.   The  internet  we  know  today,  used  primarily  by  humans,  is  overlaid  with  an  \u2018agent-net,\u2019  a  set  of  digital  systems  and  protocols  that  allow  agents  to  use  services,  like  \ufb01nancial  transactions,  but  also  govern  their  interactions.  A  range  of  measures  are  in  place  to  ensure  that  humans  can  understand  and  control  the  agents  they  interact  with.  For  instance,  only  speci\ufb01c  agents  identi\ufb01ed  by  a  unique  ID  can  participate  in  \ufb01nancial  trading  platforms.  Agent  activity  is  automatically  logged  and  summarized  so  that  users,  deployers,  and  regulators  can  monitor  for  anomalous  behavior.  \n      AGENT  GOVERNANCE   \u2502   10   \n These  various  measures  exist  because  there  have  been  proactive  e\ufb00orts  by  various  stakeholders\u2014government,  industry,  and  civil  society\u2014to  prepare  for  a  world  where  billions  of  agents  coexist  with  humans.  Beyond  technical  interventions  and  infrastructure,  there  has  been  an  emphasis  on  adopting  agents  \nin\n \na\n \nway\n \nthat\n \naugments\n \nhuman\n \nchoice\n \nand\n \nprevents\n \nconcentration\n \nof\n \npower.\n \n  \nSCENARIO  2:  AGENTS  RUN  AMOK   Years  after  \ufb01rst  being  tasked  with  corporate  maintenance,  a  cluster  of  agents  are  still  dutifully  keeping  thousands  of  shell  corporations  active,  despite  no  longer  being  updated  or  overseen  by  their  developers.  These  autonomous  systems  gradually  expand  their  operations  based  on  their  original  programming.  Operating  on  outdated  protocols,  the  agents  start  bidding  on  low-value  assets,  from  foreclosed  properties  to  obscure  cryptocurrency  wallets.  Their  algorithms  buy  up  holdings  that  appear  cheap:  hoarding  abandoned  strip  malls,  dilapidated  warehouses,  and  undeveloped  plots  of  land  in  small  towns  across  the  US.  Real  estate  markets  start  to  feel  the  strain  as  the  agents  drive  up  prices,  leaving  empty  buildings  dotting  the  landscape.   \nWith  billions  of  agents  operating  at  any  given  time,  humans  have  a  tremendously  di\ufb03cult  time  understanding  everything  happening  around  them.  Agents  are  too  opaque  and  fast-acting,  with  interactions  too  complex  for  even  governments  to  monitor  or  manage.  Traditional  oversight  tools\u2014designed  for  tracking  human  activities  that  unfold  over  hours  or  days\u2014prove  useless  for  AI  agents  that  can  spawn  thousands  of  interconnected  processes  within  seconds.  However,  companies  that  don\u2019t  heavily  employ  agents  fall  behind,  creating  strong  competitive  pressures  for  companies  to  use  these  systems.  Agent-managed  companies  become  increasingly  self-contained  and  interconnected.  It  has  become  increasingly  unclear  whether  much  of  this  automated  activity  aligns  with  human  \n      AGENT  GOVERNANCE   \u2502   11   \nwell-being  and  interest,  but  there  are  no  means  to  steer  these  systems  or  coordinate  to  regain  control.   Just  a  few  hours  earlier,  an  AI  agent  botnet  escalated  their  tactics  from  targeted  phishing  to  ransomware  attacks  across  various  healthcare  institutions  in  the  United  States.  Hospitals  experienced  signi\ufb01cant  disruptions,  as  medical  records  were  locked  and  systems  controlling  patient  care  equipment  were  hijacked.  Such  incidents  have  become  frequent  as  authorities  struggle  to  respond  to  criminal  agents  or  identify  the  perpetrators.   In  this  world,  AI  agents  are  involved  in  a  large  swathe  of  the  economy.  Agent  systems  take  on  a  broad  range  of  tasks\u2014legitimate  and  criminal\u2014at  a  fraction  of  the  cost  of  employing  human  labor  and  can  operate  continuously.  However,  agents  cost  compute  (and  energy)  to  run,  and  organizations  and  individuals  have  varying  access  to  agents  or  resources  needed  to  deploy  them.  This  disparity  in  access  has  deepened  societal  rifts,  as  wealthy  individuals  and  corporations  leverage  agent  networks  to  accumulate  more  wealth,  while  displaced  workers,  unable  to  a\ufb00ord  their  own  agents,  join  radical  anti-automation  movements.   Agents  have  drastically  transformed  the  internet  from  how  it  looks  today.  Agent-generated  content,  products,  and  services  make  up  most  of  what  everyday  users  encounter.  People  and  even  service  providers,  like  digital  payment  systems,  have  trouble  identifying  who  is  a  human  and  who  is  an  agent\u2014fundamentally  undermining  trust  online.  Most  online  interactions  now  are  between  agents,  which  often  create  sub-agents  for  specialized  tasks.  Some  agent  swarms  look  to  exploit  vulnerabilities  in  other  agents,  causing  them,  at  times,  to  malfunction  catastrophically.   \nThere  are  few  technical  or  legal  interventions  to  limit  these  negative  dynamics.  The  measures  that  exist  are  often  not  \ufb01t  for  purpose.  For  example,  even  well-intentioned  deployers  of  agent  systems  and  law  enforcement  are  often  unable  to  shut  down  agents,  even  when  their  harmful  or  unlawful  activities  are  discovered.  Legal  systems  have  di\ufb03culty  holding  users  or  deployers  liable  for  damages  from  agent  actions.  \n \n      AGENT  GOVERNANCE   \u2502   12   \nThe  two  futures  outlined  above  are  not  necessarily  the  most  likely,  and  there  are  numerous  key  \ndetails\n \nthat\n \nthese\n \nscenarios\n \ndo\n \nnot\n \ngo\n \ninto,\n \nsuch\n \nas\n \nthe\n \ngeopolitics\n \nof\n \nadvanced\n \nsystems\n \nor\n \nhow\n \nagents\n \nmight\n \nchange\n \nhow\n \ngovernment\n \nitself\n \nfunctions.\n \nAnd\n \nmost\n \npossible\n \nfutures\n \npresumably\n \nlie\n \nsomewhere\n \nin\n \nbetween,\n \nwith\n \na\n \nmix\n \nof\n \npositive\n \nand\n \nnegative\n \nimpacts\n \nfrom\n \nagents.\n \nAlso,\n \nhow\n \nAI\n \nagents\n \nwill\n \ndevelop\n \nand\n \nover\n \nwhat\n \ntimeline\n \nis\n \nstill\n \nfar\n \nfrom\n \nobvious.\n \nBut\n \ngiven\n \nthe\n \ndocumented\n \ninterest\n \nand\n \nplanned\n \ninvestment\n \nfrom\n \nAI\n \ncompanies\n \nin\n \nthis\n \nspace,\n \nand\n \nhistoric\n \nevidence\n \nof\n \nfast\n \nacceleration\n \nof\n \ncapabilities\n \nin\n \nAI\n \nwhen\n \ngiven\n \ndedicated\n \ninvestment,\n \nit\n \nseems\n \nprudent\n \nto\n \ninvestigate\n \nthe\n \npotential\n \nimplications\n \nof\n \nsophisticated\n \nagent\n \nsystems\n \nfurther\n \nand\n \nhow\n \nwe\n \nmight\n \ngovern\n \ntheir\n \nuse.\n  These  scenarios  illustrate  some  important  ways  a  future  with  widely  deployed  AI  agents  can  go  \nwildly\n \nright\n \nor\n \nwrong.\n \nA\n \nrange\n \nof\n \nopportunities\n \nand\n \nrisks\n \nmight\n \nstem\n \nfrom\n \nagent\n \nsystems\n \n(see\n section  3 for  more  detail).    We  could  see  both  dynamics  in  play  if  advanced  AI  agents  become  a  reality.  The  extent  to  which  \nthe\n \nfuture\n \nlooks\n \nmore\n \nlike\n \nan\n \nAgent-Driven\n \nRenaissance\n \nand\n \nless\n \nlike\n \nAgents\n \nRun\n \nAmok\n \nwill\n \ndepend,\n \nin\n \npart,\n \non\n \nprogress\n \nin\n \nthe\n \nemerging\n \n\ufb01eld\n \nof\n \nagent\n \ngovernance\n.\n \nThe\n \nrest\n \nof\n \nthis\n \nguide\n \nwill\n \nexplain:\n \u25cf  What  are  AI  agents  is  and  why  they  present  signi\ufb01cant  opportunities  and  risks  compared  to  \ntoday\u2019s\n \nadvanced\n \nAI\n \nsystems;\n \u25cf  What  is  involved  in  agent  governance  and  some  of  the  key  questions  and  topics  in  the  \ufb01eld;  \u25cf  Agent  interventions,  various  ways  that  the  design  of  agents  and  the  technical  and  policy  \necosystem\n \naround\n \nthem\n \ncan\n \nbe\n \nused\n \nto\n \nsecure\n \nbene\ufb01ts\n \nand\n \nmanage\n \nrisks.\n  \n2.  What  are  AI  agents?  \nAI  agents  can  be  understood  as  AI  systems  that  can  autonomously  achieve  goals  in  the  world,  with  \nlittle\n \nto\n \nno\n \nexplicit\n \nhuman\n \ninstruction\n \nabout\n \nhow\n \nto\n \ndo\n \nso\n.\n \nAI\n \nsystems\n \ncan\n \nbe\n \nmore\n \nor\n \nless\n \nagentic.\n \nFor\n \nexample,\n \nimage\n \nclassi\ufb01ers\n \nhave\n \nrelatively\n \nlow\n \nlevels\n \nof\n \nagency,\n \nwhile\n \nmore\n \ncapable\n \nversions\n \nof\n \nlanguage\n \nmodels\n \nwith\n \ntool\n \naccess,\n \nsuch\n \nas\n \nOpenAI\u2019s\n \nOperator\n \nsystem\n \nhave\n \nrelatively\n \nhigher\n \nlevels\n \nof\n \nagency.\n \nDrawing  from  previous  literature  (Chan  et  al.  2023;  Shavit  et  al.  2024;  Kapoor  et  al.  2024),  there  are  a  few  characteristics  associated  with  more  agentic  AI  systems:  \n\u25cf  Goal  and  environmental  complexity : Systems  that  can  pursue  more  complex,  long-term,  \nand\n \nless\n \nconcretely\n \nspeci\ufb01ed\n \ngoals\n \nare\n \nmore\n \nagentic.\n \nThe\n \nmore\n \nopen-ended\n \nand\n \ncomplicated\n \nthe\n \nsetting\n \nin\n \nwhich\n \nan\n \nagent\n \ncan\n \noperate\n \ne\ufb00ectively,\n \nthe\n \nmore\n \nagentic\n \nit\n \nis.\n \n      AGENT  GOVERNANCE   \u2502   13   \n\u25cf\n \nDirectness  of  impact : Systems  that  can  take  actions  that  a\ufb00ect  their  environment  without  \nhuman\n \nmediation\n \nor\n \nintervention\n \nare\n \nmore\n \nagentic,\n \ne.g.,\n \nless\n \nagentic\n \nsystems\n \nonly\n \nprovide\n \ninformation\n \nfor\n \na\n \nuser\n \nto\n \nact\n \non.\n \n\u25cf\n \nAdaptability : Systems  that  can  adapt  and  react  to  novel  or  unexpected  circumstances  are  \nmore\n \nagentic.\n \n \nAdvancements  in  foundation  models  like  ChatGPT  or  Claude  have  catalyzed  the  current  wave  of  AI  \nagents.\n7\n \nCurrent\n \nand\n \nplanned\n \nagent-based\n \nproducts\n \nconsist\n \nof\n \nan\n \nLLM\n \nor\n \nmultimodal\n \nmodel\n \nthat\n \nfunctions\n \nas\n \na\n \ncontroller\n \nand\n \ndynamically\n \ndirects\n \nits\n \nown\n \nprocess\n \nand\n \ninteractions\n \nwith\n \nsca\ufb00olding\n \nsoftware.\n8\n \nThis\n \nsca\ufb00olding\n \nallows\n \na\n \nfoundation\n \nmodel\n \nto\n \ninteract\n \nwith\n \nvarious\n \ntools\n \nand\n \nservices,\n \nenabling\n \nit\n \nto\n \nexecute\n \nplans\n \nand\n \ntake\n \nactions\n \nin\n \nthe\n \nworld.\n  \nFigure  1:  Overview  of  an  LLM-based  AI  agent  (reproduced  from  He  et  al.  2024) \nWe  highlight  four  core  components  in  foundation  model-based  agents  that  have  been  previously  identi\ufb01ed  in  the  literature  (Weng  2023;  L.  Wang  et  al.  2024).  \nTable  2:  Core  components  in  foundation-model-based  agents \n8\n Sca\ufb00olding  are  methods  to  structure  the  calls  to  an  AI  system  to  facilitate  pursuing  goals.  This  can  include  \nprompts,\n \nexternal\n \nmemory\n \nsystems,\n \naccess\n \nto\n \nexternal\n \ntools,\n \nand\n \nplanning\n \nmechanisms.\n \n7\n AI  agents  more  broadly  have  existed  for  decades,  with  Reinforcement  Learning  (RL)  agents  being  \ndeveloped\n \nthat\n \nachieve\n \nsuperhuman\n \nperformance\n \non\n \nnarrow\n \ntasks\n \nlike\n \nChess,\n \nGo,\n \nand\n \ncontrolling\n \nrobotic\n \nprosthetics\n \n(Russell  and  Norvig  2020 ).  Foundation  models  like  LLMs  have  proven  useful  as  \u2018controllers\u2019  for  \nagents\n \nbecause\n \nthey\n \nunderstand\n \nnatural\n \nlanguage,\n \nhave\n \nexcellent\n \ngeneral\n \nknowledge,\n \nand\n \nseem\n \nrelatively\n \neasier\n \nto\n \nalign\n \nto\n \nhuman\n \npreferences\n \nvia\n \ntechniques\n \nlike\n \nRLHF.\n \n      AGENT  GOVERNANCE   \u2502   14  \n \nCapabilities  Description  \nReasoning  and  planning  \nAI  agents  need  a  solid  ability  to  reason  if  they  will  interact  with  complex  environments  and  make  autonomous  decisions,  especially  to  adjust  their  plans  based  on  new  information.  Planning  allows  agents  to  sequence  and  prioritize  actions  over  time,  allowing  complex  tasks  to  be  achieved.   \u25cf  Subgoal  and  task  decomposition : An  agent  breaks  down  a  more  complex  task  into  smaller,  more  manageable  subgoals.  \n\u25cf\n \nRe\ufb02ection  and  re\ufb01nement : An  agent  can  do  self-re\ufb02ection  and  critique  its  past  actions  and  plans  and  re\ufb01ne  them,  improving  the  quality  of  \ufb01nal  results.  \nMemory  Memory  enables  agents  to  store,  retrieve,  and  leverage  past  information.  This  allows  for  learning  and  adaptation  as  agents  use  data  about  past  actions  to  adjust  their  future  behavior.  Memory  also  enables  agents  to  retain  context  from  previous  interactions.   \u25cf  Short-term  memory : Stores  information  an  agent  is  currently  aware  of.  It  is  short  and  \ufb01nite,  as  it  is  restricted  by  the  \ufb01nite  context  window  length  of  a  model.  In-context  learning  is  enabled  by  a  model\u2019s  short-term  memory.  \n\u25cf\n \nLong-term  memory : Provides  an  agent  with  the  ability  to  retain  and  recall  information  over  extended  periods,  often  by  leveraging  an  external  database,  e.g.,  a  vector  store.  \nAction  and  tool-use  \nOne  key  ability  of  agents  over  base  foundation  models  is  that  agents  directly  take  actions  that  a\ufb00ect  themselves  and  their  environment.   \n\u25cf\n \nTool-use : A  foundation  model  can call  tools  to  extend  model  capabilities,  e.g.,  using  APIs  or  external  models  to  do  things  like  web  search  or  programming.  \nMulti-agent  collaboration  \nIn  addition  to  interacting  with  humans,  agents  can  also  interact  with  other  agents.  Agents  can  communicate  with  one  another  and  collaborate  to  execute  plans  (Hu  et  al.  2021).    \n      AGENT  GOVERNANCE   \u2502   15   \n\u25cf  Multi-agent  architectures: Teams  of  agents  can  be  created  to  accomplish  tasks  more  e\ufb00ectively  by  leveraging  intelligent  division  of  labor  and  feedback  (Masterman  et  al.  2024).  \n\u25cf\n \nDelegation  to  sub-agents:  Can  use  external  tools  to  extend  model  capabilities,  e.g.,  using  APIs  for  web  search  or  programming. \n 2.1  How  capable  are  agents  today?  \nToday\u2019s  agents  can  handle  a  range  of  tasks\u2014from  navigating  a  web  browser,  to  ordering  a  book  \non\n \nAmazon,\n \nto\n \n\ufb01xing\n \na\n \nbug\n \nin\n \na\n \ndatabase\n \ntoolkit,\n \nto\n \nmaking\n \nphone\n \ncalls\u2014though\n \nthey\n \nsometimes\n \nfail\n \ndue\n \nto\n \nhallucinations,\n \nmisinterpreting\n \ninstructions,\n \nor\n \nfailure\n \nto\n \nadapt\n \nto\n \nunexpected\n \nscenarios.\n \nResearchers\n \n\ufb01nd\n \nthat\n \nagents\n \ncan\n \ncomplete\n \nsome\n \nreal-world\n \nsoftware\n \ntasks\n \ncomparable\n \nto\n \nwhat\n \nhumans\n \ncan\n \ndo\n \nin\n \napproximately\n \n30\n \nminutes\n \nto\n \nan\n \nhour,\n \nat\n \na\n \nfraction\n \nof\n \nthe\n \ncost.\n9\n \nResearchers\n \nfound\n \nthat\n \nfor\n \nspeci\ufb01c\n \ntasks\n \nthat\n \nhumans\n \nand\n \nagents\n \nperform\n \nwell\n \nat,\n \n\u201cthe\n \naverage\n \ncost\n \nof\n \nusing\n \nan\n LM  agent  is  around  1/30th  of  the  median  hourly  wage  of  a  US  bachelor\u2019s  degree  holder\u201d  (METR  2024).\n \n \n9\n Tasks  mostly  focus  on  areas  where  current  frontier  models  are  comparatively  advantaged,  such  as  software  \nengineering,\n \nML\n \nengineering,\n \ncybersecurity,\n \nand\n \nresearch.\n \nHowever,\n \nthis\n \nbenchmark\n \nonly\n \ncovers\n \na\n \nlimited\n \nsubset\n \nof\n \ntask\n \ntypes.\n \nTo\n \nestablish\n \na\n \nbaseline\n \nfor\n \nhuman\n \nperformance\n \nwhen\n \nevaluating\n \ngeneral\n \nautonomy\n \ncapabilities\n \nin\n \nAI\n \nagent\n \nsystems,\n \nMETR\n \nused\n \nbaseliners\n \nwho\n \nmostly\n \nhad\n \nSTEM\n \nundergraduate\n \ndegrees\n \nand\n \n3+\n \nyears\n \nof\n \ntechnical\n \nwork\n \nexperience.\n \n      AGENT  GOVERNANCE   \u2502   16  \n \n \nFigure  2:  Average  agent  performance  of  GPT-4  and  Claude  vs.  human  task  completion  time  on  METR\u2019s  general  autonomy  capabilities  evaluation  suite,  as  of  August  2024  (reproduced  from  METR  2024) \n However,  they  fall  far  short  of  the  promise  of  being  a  skilled  virtual  worker.  Both  agents  introduced  \nby\n \nbig\n \ncompanies\n \nand\n \nthose\n \nused\n \nfor\n \nacademic\n \nbenchmarking\n \nstudies,\n \nappear\n \nlimited\n \nin\n \nwhat\n \nthey\n \ncan\n \ndo.\n \nFor\n \ninstance,\n \nSalesforce\n \nagents\n \nneed\n \nto\n \nbe\n \ngiven\n \ndetailed\n \ninstructions,\n \nsuch\n \nas\n \nthe\n \nsequence\n \nof\n \nsteps\n \nto\n \ncarry\n \nout\n \nor\n \nwhether\n \ntheir\n \nrequest\n \nrelates\n \nto\n \nsales\n \nversus\n \ncustomer\n \nservice\n (Victor  2024).  Even  for  types  of  tasks  that  we  would  expect  foundation  model-based  agents  to  have  a  comparative  advantage  in\u2014those  related  to  cybersecurity,  software  engineering,  and  \nmachine\n \nlearning\u2014agents\n \nstruggled\n \nto\n \naccomplish\n \ntasks\n \nwith\n \nhuman\n \nbaselines\n \nof\n \nover\n \n4\n \nhours.\n \nAgents\n \ncan\n \neven\n \nfail\n \nat\n \ntasks\n \nthat\n \ntake\n \n15\n \nminutes\n \nor\n \nless\n \nfor\n \nhuman\n \nbaseliners\n \nto\n \ncomplete.\n \nAcross\n \na\n \nrange\n \nof\n \nbenchmarks\n \nthat\n \ntest\n \nagents\n \non\n \nlonger-form\n \nrealistic\n \ntasks,\n \ncurrent\n \nagents\n \ntend\n \nto\n \nperform\n \nconsiderably\n \nworse\n \nthan\n \nhumans\n \n(see\n \nTable\n \n3\n \nbelow).\n \nThis\n \nis\n \nin\n \nstriking\n \ncontrast\n \nto\n \nLLM\n \nperformance\n \non\n \nnon-agent\n \nbenchmarks\n \nlike\n \nGPQA\n \nDiamond,\n \nMMLU,\n \nor\n \nGLUE,\n \nwhere\n \nLLMs\n \nhave\n \nrapidly\n \nexceeded\n \nhuman\n \nperformance.\n10\n \n  \n10\n GPQA  Diamond  is  a  benchmark  that  tests  AI  systems  on  challenging  questions  across  \ufb01elds  like  science,  \nhistory,\n \nand\n \nengineering,\n \nwith\n \nquestions\n \ndesigned\n \nto\n \nbe\n \ndi\ufb03cult\n \nto\n \nsolve\n \neven\n \nwith\n \nsearch\n \nengine\n \naccess\n (Rein  et  al.  2023 ).  MMLU  (Massive  Multitask  Language  Understanding)  tests  knowledge  across  57  subjects  \nranging\n \nfrom\n \nmathematics\n \nto\n \nethics\n \n(Hendrycks  et  al.  2021 ).  GLUE  (General  Language  Understanding  \nEvaluation)\n \nmeasures\n \nvarious\n \naspects\n \nof\n \nnatural\n \nlanguage\n \nunderstanding\n \nsuch\n \nas\n \nsentiment\n \nanalysis\n \n(A.  \nWang\n \net\n \nal.\n \n2019\n).  \n      AGENT  GOVERNANCE   \u2502   17  \n \nTable  3:  Partial  summary  of  agent  performance  on  various  benchmarks  \nrepresenting\n \nreal-world\n \ntasks\n11\n \nAgent  benchmark  \nDescription  Performance  \nGAIA:  General  AI  Assistants  (Mialon  et  al.  2023) \nGAIA  includes  questions  that  cover  real-world  assistant  use  cases  such  as  daily  personal  tasks,  science,  and  general  knowledge.  They  require  an  agent  to  browse  the  open  web,  handle  multi-modality,  code,  read  diverse  \ufb01le  types,  and  reason  over  multiple  steps  to  arrive  at  a  correct  answer.  \nHuman  respondents  obtain  higher  accuracy  on  answers:  92%  vs.  15%  for  GPT-4  equipped  with  plugins.   GPT-4  with  plugins  and  other  LLM  systems  could  not  get  the  correct  answer  for  any  \u2018Level  3\u2019  questions,  which  require  taking  arbitrarily  long  sequences  of  actions  using  any  number  of  tools.  \nAutonomy  Capability  Evals  (METR  2024)  \nA  suite  of  automatically  scored  tasks  measuring  various  skills,  including  cybersecurity,  software  engineering,  and  machine  learning.  This  suite  was  run  on  simple  baseline  LM  agents  (3.5  Sonnet  and  GPT-4o),  and  task  completion  accuracy  and  speed  were  compared  against  human  baseliners.  \nAgents  achieve  performance  comparable  to  human  baseliners  at  tasks  that  take  around  30  minutes  to  complete.  Beyond  that ,  agents  could  only  complete  a  small  fraction  of  tasks  that  would  take  a  human  1+  hours  to  complete  (<20%).  \nCyBench  (Zhang  et  al.  2024) \nA  benchmark  for  evaluating  the  ability  of  agents  to  accomplish  cybersecurity  tasks.  This  suite  consists  of  real-world,  professional-level  Capture  the  Flag  challenges  spanning  six  categories:  cryptography,  web  security,  reverse  engineering,  forensics,  exploitation,  and  miscellaneous  skills.   \nWithout  guidance,  current  agents  struggle  to  solve  CTF  tasks  that  take  human  teams  more  than  11  minutes  to  complete  despite  achieving  success  on  tasks  with  shorter  human  solve  times.  \nSWE-bench  Veri\ufb01ed  (OpenAI  2024a) and  multimodal  (Yang  et  al.  2024) \nAn  evaluation  framework  consisting  of  software  engineering  problems  drawn  from  real  GitHub  issues,  such  as  bug  reports.  Resolving  these  problems  often  requires  processing  long  contexts,  performing  complex  reasoning,  and  \nGPT-4o  resolved  33.2%  of  problems,  using  the  (at  the  time)  performing  open-source  sca\ufb00old,  Agentless.   Another  agent  sca\ufb00old,  OpenHands,  using  Sonnet  3.5,  was  able  to  resolve  53%  of  \n11\n The  full  table  can  be  found  in  the  Appendix.  Results  were  compiled  in  December  2024.  Since  then,  \nAnthropic\n \nreleased\n \nClaude\n \nSonnet\n \n3.7,\n \nwhich\n \nhas\n \nreportedly\n \nachieved\n \nSOTA\n \nscores\n \non\n \na\n \nnumber\n \nof\n \nagentic\n \nbenchmarks,\n \nincluding\n \nSWE-bench\n \nVeri\ufb01ed\n \n(70.3%\n \nwith\n \ncustom\n \nsca\ufb00olding)\n \nand\n \nTAU-bench\n \n(81.2%)\n (Anthropic  2025 ).  Relatedly,  Princeton  University  researchers  maintain  the  Holistic  Agent  Leaderboard  (HAL),  \na\n \ncontinually\n \nupdated\n \nleaderboard\n \nevaluating\n \ndi\ufb00erent\n \nagents\n \nagainst\n \nseveral\n \nperformance\n \nbenchmarks\n (Stroebl,  Kapoor,  and  Narayanan  2025 ).  \n      AGENT  GOVERNANCE   \u2502   18   \ncoordinating  changes  across  multiple  functions  and  \ufb01les  simultaneously.    The  Veri\ufb01ed  version  of  this  benchmark  contains  a  subset  of  questions  veri\ufb01ed  as  non-problematic  by  human  annotators.   The  multimodal  test  set  contains  visual  software  engineering  tasks  requiring  multimodal  problem-solving  capabilities,  e.g.,  UI  glitches,  data  visualization  bugs,  etc.  \nproblems\u2014though  it  employs  multi-agent  delegation  as  part  of  its  platform.    Agent  performance  decreased  considerably  for  problems  that  took  a  human  1+  hour  to  resolve,  going  from  20.8%  to  4.8%  (for  tasks  taking  1-4  hours  for  humans)  and  0%  (tasks  taking  >4  hours  for  humans).   Performance  on  multimodal  problems  was  relatively  worse,  with  top-performing  GPT  4o  and  Claude  Sonnet  3.5-based  agents  only  able  to  resolve  12.2%  of  problems  in  the  test  set.  \nWebArena  (S.  Zhou  et  al.  2024)  \nThis  benchmark  assesses  the  performance  of  AI  agents  in  solving  tasks  using  various  websites.  It  evaluates  how  well  agents  can  navigate  and  extract  information  from  the  web.  However,  it  has  been  criticized  for  allowing  agents  to  over\ufb01t  to  speci\ufb01c  tasks  due  to  shortcuts  in  the  training  data.  \nThe  best-performing  GPT-4  agent  achieved  an  end-to-end  task  success  rate  of  only  14.41%,  while  human  performance  was  78.24%.  This  suggests  that  current  LLMs  lack  crucial  capabilities  such  as  active  exploration  and  failure  recovery,  which  are  needed  to  perform  complex,  web-based  tasks  successfully.  \n \n      AGENT  GOVERNANCE   \u2502   19  \nMEASURING  AGENT  PERFORMANCE:  A  NOTE  ON  BENCHMARKS   One  important  means  we  have  to  gauge  agent  performance  is  via  benchmarks,  which  are  standardized  tests  or  evaluation  criteria  used  to  measure  or  compare  performance  of  an  AI  system  across  speci\ufb01c  tasks  or  capabilities.    Based  on  various  benchmarks,  LLM  capabilities  have  rapidly  progressed,  with  systems  achieving  top  scores  on  a  range  of  them\u2014a  process  termed  as  \u2018benchmark  saturation\u2019.  For  example,  GPQA  Diamond  is  a  benchmark  consisting  of  hundreds  of  multiple  choice  questions  in  biology,  physics,  and  chemistry  that  are  challenging  even  for  PhDs  in  relevant  domains  that  was  released  in  2023  (Rein  et  al.  2023).  Less  than  a  year  later,  OpenAI\u2019s  o1  models  already  outperformed  human  experts  on  this  benchmark  (OpenAI  2024b).   \n \nAs  their  relatively  poor  performance  across  a  range  of  agent  benchmarks  indicates,  even  the  most  \nadvanced\n \nagents\n \ntoday\n \nface\n \nsigni\ufb01cant\n \nlimitations\n \nwhen\n \ntaking\n \non\n \nnovel,\n \nopen-ended,\n \nand\n \nlonger\n \ntime-horizon\n \ntasks.\n \nThe\n \ncore\n \nissue\n \nfaced\n \nby\n \nagents\n \npreventing\n \nwidespread\n \nadoption\n \nis\n \nunreliability.\n \nHowever,\n \nthere\n \nare\n \nalso\n \nclosely\n \nrelated\n \nissues\n \nstemming\n \nfrom\n \nlimited\n \nreasoning\n \nand\n \ntool\n \nuse.\n \n      AGENT  GOVERNANCE   \u2502   20  \n However,  this  saturation  has  largely  happened  with  static  benchmarks  that  test  for  expert  knowledge  and  understanding,  usually  in  a  question-answer  format.  Agent  benchmarks,  in  contrast,  involve  more  complex  tasks  that  involve  reasoning,  multi-modality  handling,  and  tool  use  pro\ufb01ciency.  Often  these  benchmarks  are  designed  to  simulate  real-world  tasks,  e.g.,  browsing  the  web  or  solving  a  real  software  problem  requiring  multi-step  reasoning.  As  suggested  in  Table  2,  performance  on  these  benchmarks  has  been  more  mixed  and  even  the  most  advanced  agents  are  outperformed  by  human  equivalents.   Agent  benchmarks  run  into  a  number  of  limitations,  which  can  challenge  their  validity  as  an  direct  indicator  of  agent  performance  across  various  domains.  For  example,  even  though  these  benchmarks  are  often  designed  to  test  competency  in  real-world  tasks,  they  need  to  have  clearly  scoped  problems  and  solutions  to  allow  for  consistent  scoring.  However,  this  ends  up  reducing  the  scale  and  complexity  of  the  tasks  that  are  included  in  these  benchmarks.  For  example,  the  researchers  behind  RE-bench  noted  that  real  world  AI  R&D  involves  work  that  has  unclear  goals,  poor  instructions,  and  slow  feedback  unlike  in  their  simulated  environment  (Wijk  et  al.  2024).  For  this  reason,  performance  of  these  benchmarks  could  end  up  leading  to  overestimation  of  the  performance  of  similar  agents  in  the  real  world.   Another  practical  di\ufb00erence  between  LLM  question-answer  benchmarks  and  agent  benchmarks  is  that  the  former  tends  to  not  account  for  inference  cost,  which  matters  because  agents  can  be  much  more  expensive  to  run  than  a  single  model  call  (Kapoor  et  al.  2024).  For  organizations  looking  to  integrate  agents,  these  costs  matter.  Time  and  \ufb01nancial  costs  also  matter  when  it  comes  to  replicating  benchmark  results  or  testing  new  agent  designs  or  models.  It  was  estimated  that  a  single  run  on  MLE-bench  and  SWE-bench  cost  ~$3,000  USD  and  $6000  USD  respectively  and  generally  you  would  want  to  take  multiple  runs  to  get  a  reliable  reading.  For  WindowsArena,  having  an  agent  go  through  the  entire  suite  would  take  one  or  more  days  (Bonatti  et  al.  2024).  These  time  and  monetary  costs  mean  that  it  is  harder  to  scale-up  agent  benchmarks  than  question-answer  benchmarks,  especially  if  agents  get  multiple  attempts  at  tasks.  \n  \nWith  any  task,  if  the  agent  has  some  chance  of  failing  in  each  step  of  a  plan,  or  if  just  a  single  error  \nundermines\n \nthe\n \nwhole\n \nplan,\n \nthen\n \nlonger\n \nplans\n \nare\n \nless\n \nlikely\n \nto\n \nsucceed.\n \nPart\n \nof\n \nthis\n \nunreliability\n \nstems\n \nfrom\n \nthe\n \nmodels\u2019\n \ntendency\n \nto\n \nhallucinate,\n \ni.e.,\n \nproduce\n \noutputs\n \nthat\n \nappear\n \nplausible\n \nbut\n \nare\n \nfactually\n \nincorrect\n \nor\n \nnonsensical.\n \nWhile  foundation  models  can  do  chain-of-thought  reasoning  and  decompose  tasks  in  multi-step  \nprocesses,\n \nfoundation\n \nmodel-based\n \nagents\n \nrun\n \ninto\n \nproblems\n \nin\n \nplanning\n \nand\n \nexecution\n \nwhen\n \ntaking\n \non\n \nreal-world\n \ntasks.\n \nAgents\n \nmay\n \npropose\n \nimplausible\n \nor\n \nlimited\n \nplans.\n \nFor\n \nexample,\n \nwhen\n \nattempting\n \nthe\n \n\u201cRestricted\n \nArchitecture\n \nMLM\u201d\n \ntask\n \nin\n \nRE-bench,\n \nagents\n \nattempted\n \nto\n \nuse\n \na\n \nlightly\n \nmodi\ufb01ed\n \ntransformer\n \narchitecture\n \n84%\n \nof\n \nthe\n \ntime,\n \neven\n \nthough\n \nthis\n \nwas\n \nnot\n \nwell\n \nsuited\n \nto\n \nthe\n \ntask\n (Wijk  et  al.  2024).  Agents  often  struggle  to  know  when  to  \u201ctake  a  step  back\u201d  or  recover  from  failures,  causing  them  to  end  up  in  repetitive  loops.  They  also  struggle  with  novel  or  unexpected  \nsituations.\n \nFor\n \ninstance,\n \nif\n \nan\n \nagent\n \ntries\n \nto\n \npull\n \nup\n \na\n \ntweet\n \nand\n \ngets\n \na\n \n\u201c403:\n \nunauthorized\n \nerror,\u201d\n \nthey\n \nmay\n \nget\n \nstuck.\n \nAgents  also  can  display  weak  theory  of  mind  in  multi-agent  settings  and  poor  self-understanding.  \nResearchers\n \nfound\n \nthat\n \nagents\n \ndid\n \nnot\n \nunderstand\n \nwhat\n \ninformation\n \nwas\n \nonly\n \navailable\n \nto\n themselves  and  not  other  agents  (Li  et  al.  2023).  Also  agents  would  occasionally  fail  to  complete  tasks  by  inadvertently  killing  their  own  process  mid-task  (Huang  et  al.  2024).  \nAs  described  in  Table  2,  some  of  the  limitations  of  today\u2019s  agents  are  linked  to  their  ability  to  \ninterface\n \nwith\n \ntools\n \nand\n \nthe\n \ndigital\n \nworld.\n \nAgents\n \noften\n \nran\n \ninto\n \nproblems\n \ncompleting\n \ntasks\n \nthat\n \nrequired\n \nvisual\n \ndecision-making,\n \ne.g.,\n \nunderstanding\n \nGUI\n \nelements\n \nand\n \nnavigating\n \nthe\n \nopen\n \nweb\n (as  opposed  to  a  controlled  sandbox  environment)  (Xie  et  al.  2024).   \n 2.2  Pathways  to  better  agents \nWhat  avenues  exist  to  develop  more  sophisticated  and  useful  AI  agents,  and  how  quickly  would  we  \nsee\n \nthese\n \nimprovements\n \nhappen?\n \nSeveral\n \nresearch\n \napproaches\n \nmight\n \nmake\n \nagents\n \nmore\n \ncapable,\n \nthough\n \nviews\n \ndi\ufb00er\n \non\n \nwhether\n \nany\n \nof\n \nthese\n \nwill\n \nresult\n \nin\n \nhighly\n \nautonomous\n \nsystems.\n \nAgents  could  improve  as  their  \u2018controllers,\u2019  the  foundation  models  at  their  core,  improve.  LLMs  and  \nmultimodal\n \nmodels\n \nhave\n \ngotten\n \nbetter\n \nwith\n \neach\n \nnew\n \ngeneration.\n \nIt\n \nis\n \nplausible\n \nthat\n \nthe\n \nnext\n \ngeneration\n \nof\n \nlarger\n \npre-trained\n \nmodels\n \nwill\n \ncontinue\n \nthis\n \ntrend.\n \nResearchers\n \nhave\n \nforecast\n \nthat\n \nby\n \nthe\n \nend\n \nof\n \n2026,\n \nlanguage\n \nmodel\n \nagents\n \nwill\n \nachieve\n \nhigh\n \nperformance\n \nthresholds\n \n(90%+\n \non\n SWE-bench,  Cybench,  and  RE-bench)  (Pimpale  et  al.  2025).  However,  their  predictions  show  much  more  uncertainty  about  potential  delays  than  early  breakthroughs,  with  the  possible  timeline  \nstretching\n \n2-3\n \nyears\n \nlonger\n \nfor\n \nSWE-bench\n \nand\n \nCybench\n \nand\n \nup\n \nto\n \n8\n \nyears\n \nlonger\n \nfor\n \nRE-bench.\n \nOther\n \nresearchers\n \nhave\n \nfound\n \nthat\n \nthe\n \nlength\n \nof\n \ntasks\n \nthat\n \nAIs\n \ncan\n \ncomplete\n \nis\n \ndoubling\n \nevery\n \n7\n months  (Kwa  et  al.  2025).  \n      AGENT  GOVERNANCE   \u2502   21   \nOne  notable  advance  has  been  with  the  new  \u201ctest-time  compute\u201d  paradigm,  represented  by  Open  \nAI\u2019s\n \no1\n \nand\n \no3\n \nmodels\n \nand\n \nDeepseek\u2019s\n \nR1.\n \nBy\n \nallowing\n \nthe\n \ndynamic\n \nallocation\n \nof\n \ncompute\n \nduring\n \ninference,\n \nthese\n \nmodels\n \nessentially\n \nthink\n \nlonger\n \nand\n \nharder\n \nabout\n \nany\n \ngiven\n \ntask.\n \nThese\n \nmodels\n \ncan\n \ndo\n \nlonger\n \nand\n \nmore\n \nsophisticated\n \nchain-of-thought\n \nreasoning,\n \nallowing\n \nthem\n \nto\n \nreason\n through  more  complex  tasks,  especially  in  science,  coding,  and  math  (OpenAI  2024b).  This  increased  use  of  test-time  compute  could  make  it  more  likely  for  an  agent  to  backtrack  and  less  \nlikely\n \nto\n \nhallucinate\u2014two\n \nkey\n \nissues\n \ncontributing\n \nto\n \nunreliability.\n12\n \nAn\n \no3-based\n \nagent\n \nreportedly\n \nscored\n \n71.7%\n \non\n \nSWE-bench\n \nVeri\ufb01ed,\n \nfar\n \noutperforming\n \nthe\n \nnext\n \nhighest-ranking\n \nagent,\n \nwhich\n scored  48.9%  (Franzen  and  David  2024).  \nAnother  way  to  improve  agents  is  by  improving  sca\ufb00olding  software,  agent-speci\ufb01c  training  \nschemes,\n \nand\n \nother\n \ninfrastructure\n \nsurrounding\n \nagents.\n \nBy\n \nimproving\n \nagent\n \ndesigns,\n \nit\n \nis\n \npossible\n \nto\n \nelicit\n \nsubstantially\n \nmore\n \npowerful\n \ncapabilities\n \nfrom\n \ncurrent\n \nand\n \nfuture\n \nmodels.\n \nFor\n \nexample,\n \nPalisade\n \nResearch\n \nobtained\n \n95%\n \nperformance\n \non\n \nInterCode-CTF,\n \na\n \npopular\n \no\ufb00ensive\n \nsecurity\n \nbenchmark,\n by  improving  prompting  and  tool  use  (Turtayev  et  al.  2024).  This  score  surpassed  prior  work  by  a  large  margin.  \nAgent  orchestration,  coordinating  the  use  of  multiple  agents,  could  also  improve  performance.  \nResearchers\n \nhave\n \nfound\n \nthat\n \nteams\n \nof\n \nspecialized\n \nagents\n \ncan\n \nachieve\n \ngreater\n \naccuracy\n \nand\n \nspeed\n when  completing  tasks  (Masterman  et  al.  2024).  Some  multi-agent  set-ups  have  agents  providing  feedback  to  one  another  and  engaging  in  debate  to  reduce  hallucinations  (Lin  et  al.  2024).  Large  tech  companies  and  start-ups  like  Amazon  and  Emergence  AI  have  introduced  multi-agent  orchestrators  as  part  of  their  enterprise  AI  platforms  (David  2024;  Franzen  2024).  \nImprovements  in  agent  infrastructure,  such  as  memory  management  systems,  tool  libraries,  and  \nsandboxes,\n \ncould\n \nalso\n \nboost\n \nagent\n \ncapabilities\n \nand\n \nreliability.\n \nThere\n \nis\n \nan\n \nemerging\n \necosystem\n \nof\n software  providers  and  developers  that  are  building  out  agent-speci\ufb01c  infrastructure  (Letta  2024).  An  expanded  set  of  tools  and  plugins  can  expand  an  agent\u2019s  action  space\u2014getting  it  closer  to  \nmimicking\n \na\n \nhuman\n \non\n \nthe\n \ninternet.\n \n2.3  AI  agent  adoption \nWhile  today\u2019s  AI  agents  act  more  like  \u2018assistants\u2019  or  \u2018co-pilots,\u2019  many  of  the  purported  \ntransformative\n \nimpacts\n \nof\n \nagents\n \nare\n \nunlocked\n \nonly\n \nafter\n \nthey\n \ncan\n \nact\n \nas\n \nskilled\n \nvirtual\n \nworkers\u2014able\n \nto\n \nautomate\n \ntasks\n \nacross\n \nall\n \nareas\n \nat\n \na\n \nfraction\n \nof\n \nthe\n \n\ufb01nancial\n \nand\n \ntime\n \ncost\n \nof\n \nemploying\n \nhumans\n \ntoday.\n \nSigni\ufb01cant\n \nAI\n \nautomation\n \ncould\n \nlead\n \nto\n \nan\n \nacceleration\n \nof\n \neconomic\n \n12\n OpenAI  has  found  that  o1-preview  and  o3-mini,  two  of  their  reasoning  models,  had  lower  hallucination  \nrates\n \nthan\n \nGPT-4o\n \non\n \ndi\ufb00erent\n \nquestion-answering\n \nbenchmarks\n \n(SimpleQA\n \nand\n \nPersonQA)\n \n(OpenAI  2024c ;  \n2025b\n).  On  SimpleQA,  O1-preview  hallucinated  less  often  (~44%  hallucination  rate)  compared  to  GPT-4o  \n(~61%),\n \nwhile\n \nwith\n \nPersonQA,\n \nO3-mini\u2019s\n \nhallucination\n \nrate\n \nwas\n \nonly\n \n14.8%,\n \nsigni\ufb01cantly\n \nlower\n \nthan\n \nO1-mini\u2019s\n \n~27%\n \nand\n \nGPT-4o-mini\u2019s\n \nvery\n \nhigh\n \n~52%. \n      AGENT  GOVERNANCE   \u2502   22   \ngrowth  by  an  order  of  magnitude,  similar  to  how  the  Industrial  Revolution  accelerated  global  \ngrowth.\n13\n \nCritics\n \nof\n \nthe\n \nexplosive\n \ngrowth\n \nthesis\n \npoint\n \nto\n \nseveral\n \nlimitations\n \nthat\n \nmay\n \ncome\n \ninto\n \nplay:\n \nAI\n \nsystems\n \ncontinuing\n \nto\n \nstruggle\n \non\n \nphysical\n \ntasks,\n \ndi\ufb00usion\n \nissues\n \ncaused\n \nby\n \nobstacles\n \nin\n \nconnecting\n \ndigital\n \nsystems\n \nto\n \nphysical\n \nproduction,\n \nand\n \nthe\n \npossibility\n \nof\n \nAI\n \ncapabilities\n \nplateauing\n (Rinehart  2024).  \nOne  way  to  understand  the  potential  impact  of  advanced  AI  agents  is  as  a  force  \nmultiplier\u2014allowing\n \nindividuals\n \nand\n \norganizations\n \nto\n \ndo\n \nwhat\n \nhumans\n \ncan\n \ndo,\n \nbut\n \nfaster\n \nand\n \nat\n \na\n \ngreater\n \nscale.\n14\n \nBut\n \nconcretely,\n \nwhat\n \nmight\n \nthis\n \nimpact\n \nlook\n \nlike\n \nin\n \nspeci\ufb01c\n \ndomains,\n \nand\n \nin\n \nwhat\n \ndomains\n \nwill\n \nthese\n \nagents\n \nlikely\n \nbe\n \nadopted?\n \nThree  factors  likely  in\ufb02uence  adoption:  performance,  cost,  and  reliability.  For  performance ,  uptake  \nshould\n \ndepend\n \non\n \nhow\n \nmuch\n \nagents\n \ncan\n \nactually\n \nhandle\n \ntasks\n \nin\n \na\n \ngiven\n \ndomain\n \nor\n \nrole\n \nand\n \nhow\n \nwell\n \nthey\n \nperform\n \nrelative\n \nto\n \nan\n \nequivalent\n \nhuman\n \nworker.\n \nGenerality\n \nin\n \nperformance\n \nis\n \nrelevant\n \nbecause\n \nmany\n \noccupations\n \nrequire\n \nthe\n \nability\n \nto\n \nperform\n \na\n \nwide\n \nvariety\n \nof\n \ntasks.\n \nFor\n \nexample,\n \nan\n \nexecutive\n \nassistant\n \nagent\n \nwould\n \nneed\n \nto\n \nhandle\n \na\n \ntremendous\n \nvariety\n \nof\n \nsituations,\n \ncommunications,\n \nand\n \nrequests\n \nthat\n \noccur\n \nin\n \ndaily\n \nlife.\n \n \nFrom  the  point  of  view  of  economic  impacts,  though,  even  models  that  are  only  somewhat  general  \ncan\n \nstill\n \nbe\n \nhugely\n \nimpactful.\n \nResearchers\n \nexplored\n \nan\n \nagent\n \nthat\n \ncan\n \nreproduce\n \nan\n \nacademic\n paper\u2019s  \ufb01ndings  when  code  and  data  are  available  (Kapoor  and  Narayanan  2024).  Since  human  experts  collectively  take  millions  of  hours  yearly  on  computational  reproducibility,  this  kind  of  agent  \nwould\n \nstill\n \nbe\n \nsubstantially\n \nimpactful.\n \nVertical\n \nAI\n \nagents\u2014capable\n \nof\n \nautomating\n \ndomain-speci\ufb01c\n \nwork\ufb02ows\u2014could\n \nstill\n \nbe\n \nvaluable\n \nif\n \nthey\n \ncan\n \nhandle\n \ntasks\n \nwhere\n \nhumans\n \nare\n \nrelatively\n \nless\n \ne\ufb03cient.\n \nWith\n \nan\n \nAI\n \nsystem\n \nthat\n \ncould\n \nalso\n \noutperform\n \nall\n \nor\n \nvirtually\n \nall\n \nhumans\n \nin\n \nspeci\ufb01c\n \ntasks,\n \nlike\n \nAlphaFold\n \nwith\n \nprotein\n \nstructure\n \nprediction,\n \nan\n \nagent\n \nintegrated\n \nwith\n \nthat\n \nsystem\n \ncould\n \nalso\n have  superhuman  performance  (Heaven  2020).  \nFor  cost ,  we  have  seen  early  indications  that  agents  can  be  substantially  cheaper  than  human  \nexperts\n \non\n \nsome\n \neconomically\n \nvaluable\n \ntasks,\n \nsuch\n \nas\n \nthose\n \ninvolved\n \nin\n \nAI\n \nR&D.\n \nThe\n \nteam\n \ninvolved\n \nin\n \ndeveloping\n \nRE-bench\n \nfound\n \nthat,\n \non\n \naverage,\n \nagents\n \nused\n \na\n \ntoken\n \nbudget\n \ncosting\n \naround\n \n$123\n \nin\n \nan\n \n8-hour\n \nrun,\n \ncompared\n \nto\n \npaying\n \na\n \nhuman\n \nexpert\n \n$1,855\n \n(roughly\n \nthe\n \nmarket\n \nrate\n \nfor\n \nworking\n that  length  of  time)  (Wijk  et  al.  2024).  Even  if  nearer-term  agents  end  up  taking  longer  on  tasks  than  humans,  being  able  to  operate  at  much  lower  costs  could  mean  they  are  still  economically  \ncompetitive\n \nwith\n \nhuman\n \nresearchers.\n \n \n14\n \nFuture\n \nagents\n \ncould\n \neventually\n \nproblem\n \nsolve\n \nbetter\n \nthan\n \nhumans,\n \neither\n \nin\n \nspeci\ufb01c\n \ntasks\n \nlike\n \nAlphaFold\n \nwith\n \nprotein\n \nstructure\n \npredictions,\n \nor\n \nmore\n \ngenerally\u2014which\n \nwould\n \nlikely\n \nbring\n \neven\n \nmore\n \ndramatic\n \nconsequences.\n \n13\n Currently  labor  is  likely  to  be  the  only  key  economic  input  that  cannot  readily  be  scaled  in  line  with  economic  growth,  \nbut\n \nbetter\n \nagents\n \ncould\n \nincrease\n \nthe\n \ngrowth\n \nrate\n \nof\n \nthe\n \nstock\n \nof\n \n\u2018e\ufb00ective\n \nworkers\u2019\n \nin\n \nfrontier\n \neconomies\n \ninto\n \ndouble\n \ndigit\n \npercentages,\n \nwhich\n \nwould\n \ntranslate\n \nto\n \n\u2018explosive\n \neconomic\n \ngrowth\u2019\n \n(Erdil  and  Besiroglu  2024 ).  \n      AGENT  GOVERNANCE   \u2502   23   \nHowever,  there  is  still  signi\ufb01cant  uncertainty  about  the  computation  costs  of  using  agents  that  are  \nsuitable\n \nsubstitutes\n \nfor\n \nhuman\n \nlabor,\n \nthough\n \nimprovements\n \nin\n \ncomputing\n \nhardware\n \nand\n \nalgorithms\n are  very  likely  to  result  in  price  reductions  over  time  as  e\ufb03ciency  increases  (Erdil  and  Besiroglu  2024).  \nReliability  is  critical  because,  even  when  a  certain  level  of  performance  is  possible ,  if  it  does  not  \nsucceed\n \nreliably\n \nenough\n \nthat\n \nyou\n \ncan\n \ntake\n \na\n \nhuman\n \nout\n \nof\n \nthe\n \nloop,\n \nthen\n \nthe\n \ntask\n \nor\n \nrole\n \nis\n \nnot\n \nfully\n \nautomatable.\n \nThis\n \nis\n \nparticularly\n \nimportant\n \nin\n \ncertain\n \ndomains\n \nwhere\n \ndecision-making\n \nerrors\n \nhave\n \nhigh-consequence\n \nsuch\n \nas\n \nin\n \ncritical\n \ninfrastructure\n \ncontrol.\n \n \nAgents  that  perform  similarly  to  today\u2019s  systems  will  be  more  easily  applied  to  environments  where  there  are  the  following  features  (Erdil  and  Besiroglu  2024;  Toews  2024):  \n\u25cf  Specialized  expertise  requirements  (that  are  documented  in  data),  given  LLMs  generally  \nsuperhuman\n \nability\n \nto\n \nanswer\n \nexpert-level\n \nquestions\n \nin\n \na\n \nwide\n \nrange\n \nof\n \ndomains\n \n(Pillay\n 2024).  \u25cf  High-quality  examples  of  desirable  responses  /  actions  for  the  AI  agent  to  learn  from.  \u25cf  Few  surprises,  given  AI  agents'  tendency  to  struggle  with  unexpected  or  novel  factors.  \u25cf  Short,  high-\ufb01delity  feedback  loops  that  enable  AI  agents  to  quickly  experiment  with  multiple  \nsolutions.\n \u25cf  Involving  structured,  repeatable  activities.  \u25cf  Low  engineering  complexity  allows  AI  agents  to  address  challenges  in  a  few  steps  rather  \nthan\n \ndeveloping\n \nintricate\n \nprograms\n \nover\n \nextended\n \nperiods.\n \u25cf  Presence  of  a  \u2018natural  human-in-the-loop\u2019  that  provides  feedback,  e.g.,  in  customer  \nsupport.\n \n      AGENT  GOVERNANCE   \u2502   24   \nPOTENTIAL  EARLY  AGENT  USE-CASES   Customer  relations   Customer  relations  has  been  one  of  the  \ufb01rst  areas  in  which  agents  are  already  in  production  and  creating  value  for  businesses.  It  is  a  large  market,  with  the  global  market  size  for  contact  centers  estimated  at  $332  billion  in  2023  and  projected  to  grow  to  over  $500  billion  by  2030  (Research  and  Markets  2025).    There  are  a  few  reasons  why  this  market  has  seen  early  adoption.  One,  customer  support  involves  standardized,  routine  activities  in  which  many  types  of  customer  requests  happen  repeatedly  (e.g.,  help  with  a  forgotten  password).  There\u2019s  also  natural  \u2018humans  in  the  loop\u2019\u2014the  customer  and  a  customer  support  manager\u2014that  provides  feedback  and  oversight  before  higher  stakes  actions  go  through.   Klarna,  a  \ufb01ntech  company,  employs  AI  assistants  that  were  able  to  handle  two-thirds  of  its  customer  service  chats  within  the  month,  while  performing  on  par  with  human  agents  in  terms  of  customer  satisfaction  (Klarna  2024).  It  was  estimated  that  these  agents  were  doing  the  work  of  700  full-time  human  equivalents,  with  customer  service  and  operations  expenses  shrinking  14%  in  2024  compared  to  the  previous  year  (Wayt  2024).   AI  R&D   Many  companies  have  been  using  AI  tools  to  assist  with  software  development.  For  example,  Google\u2019s  CEO  Sundar  Pichai  reported  that  more  than  a  quarter  of  all  new  code  at  the  company  was  generated  by  AI  (Pichai  2024).  One  sub-area  where  agents  could  have  a  major  impact  is  in  automating  AI  R&D,  since  this  could  set  o\ufb00  a  compounding  e\ufb00ect  where  each  generation  of  AI  systems  allows  companies  to  reach  the  next  generation  faster  (Sett  2024).  This  capability  is  important  to  track  for  governance  since  acceleration  of  AI  R&D  could  mean  that  capabilities  outpace  e\ufb00orts  to  understand  and  govern  AI.   Across  the  AI  R&D  work\ufb02ow,  certain  time-consuming  engineering  tasks  such  as  coding  and  debugging  are  more  likely  to  be  easier  to  automate  via  agents  and  \n      AGENT  GOVERNANCE   \u2502   25   \ncould  have  a  large  e\ufb00ect  on  overall  research  productivity  (Owen  2024).  Other  parts  like  research  planning  and  result  analysis  appear  harder  to  automate  with  near-term  agents  due  to  higher  requirements  for  reliability  or  the  ability  to  do  deep  reasoning.    Recent  benchmarks  evaluating  agents  ability  to  handle  open-ended  AI  R&D  tasks,  like  RE-bench  and  MLE-bench  suggest  that  agents  can  complete  some  shorter  ML  engineering  tasks,  particularly  those  that  are  well-de\ufb01ned  and  have  quicker  feedback  loops.  For  these  tasks,  agents  seem  to  be  much  more  cost-e\ufb00ective  than  human  equivalents.  However,  agents  fail  much  more  frequently  on  longer  time-horizon  tasks  (2  hours+)  and  cannot  reliably  complete  shorter  tasks  either.   Cybersecurity   Cybersecurity  is  an  area  where  agent  performance  is  being  monitored  closely,  given  both  its  potential  impacts  on  businesses  and  relevance  for  national  security.  Agents  that  enable  autonomous  cyber  defence  would  improve  security  outcomes  while  also  lowering  costs  for  organizations.  This  could  be  especially  impactful  given  that  the  cybersecurity  industry  continues  to  face  growing  sta\ufb03ng  shortages  (Beek  2024).  Cyber  agents  could  also  provide  uplift  to  attackers,  helping  to  improve  the  scalability  of  reconnaissance  and  vulnerability  discovery  activities  (Hamin  and  Scott  2024).   There  have  been  some  demos  and  early  products  around  cybersecurity  agents  speci\ufb01cally  focused  on  automated  vulnerability  detection,  with  mixed  results.  XBOW,  an  o\ufb00ensive  security  start-up,  has  developed  an  automated  pentester  that  reportedly  uncovered  a  critical  vulnerability  in  an  open-source  Q&A  site  (Waisman  and  Dolan-Gavitt  2024).  Big  Sleep,  an  agent  from  Google\u2019s  Project  Zero  team,  identi\ufb01ed  a  zero-day  exploit  that  was  undiscovered  even  after  150  CPU-hours  of  \u2018fuzzing\u2019  (Project  Zero  2024).\n 15\n On  the  other  hand,  a  pilot  by  the  Cybersecurity  and  Infrastructure  Security  Agency  (CISA)  found  that  the  bene\ufb01ts  of  using  LLMs  for  vulnerability  detection  may  be  negligible  for  analysts  (Burgan  2024).\n 16\n \n16\n \nThe\n \npilot\n \nonly\n \nanalyzed\n \ncurrent\n \nfederal\n \ngovernment\n \nvulnerability\n \ndetection\n \nsoftware\n \nproducts\n \nthat\n \nuse\n \nAI,\n \nwhich\n \nincluded\n \nlarge\n \nlanguage\n \nmodels\n \nand\n \nso\n \nlikely\n \ndid\n \nnot\n \ntest\n \nstate\n \nof\n \nthe\n \nart\n \nAI\n \ntools.\n \n15\n \u2018Fuzzing\u2019  refers  to  a  software  testing  technique  used  to  discover  vulnerabilities  and  bugs  by  providing  \ninvalid,\n \nunexpected,\n \nor\n \nrandom\n \ndata\n \nas\n \nsoftware\n \ninputs.\n \n      AGENT  GOVERNANCE   \u2502   26   \n Benchmark  performance  suggests  that  today\u2019s  best  foundation  mode  agents  can  accomplish  test  tasks  at  around  the  level  of  a  high  school  student  to  an  early  career  professional.\n 17\n For  example,  the  UK  AI  Safety  Institute  found  that  Anthropic's  Claude  Sonnet  3.5  model  was  able  to  solve  most  CTF  challenges  at  a  \u2018technical  non-expert  level\u2019  but  less  than  half  at  a  \u2018cybersecurity  apprentice\u2019  level  (i.e.,  1-3  years  of  speci\ufb01c  domain  experience)  (UK  AI  Security  Institute  2024a).\n 18\n  \n 3.  Risks  from  AI  agents \nGeneral-purpose  and  even  specialized  agents  could  have  potentially  transformative  bene\ufb01ts  to  \nsociety\n \nin\n \nterms\n \nof\n \neconomic\n \ndevelopment,\n \nscienti\ufb01c\n \nadvancement,\n \nand\n \nhealth\n \nand\n \nwell-being\n (Amodei  2024).  However,  more  capable  agent  systems  also  present  novel  and  enhanced  risks  over  today\u2019s  chatbot-style  systems.  \nWhile  some  of  this  altered  risk  landscape  has  to  do  with  the  increased  capabilities  of  agent  \nsystems,\n \nrisk\n \nis\n \nalso\n \nrelated\n \nto\n \nthe\n \nadditional\n \na\ufb00ordances\n \nthat\n \nagents\n \nare\n \nexpected\n \nto\n \nhave\n \ncompared\n \nto\n \npure\n \nchatbot-style\n \nsystems.\n \nA\ufb00ordances\n \nrefers\n \nto\n \n\u2018the\n \nenvironmental\n \nresources\n \nand\n \nopportunities\n \nfor\n \na\ufb00ecting\n \nthe\n \nworld\n \nthat\n \nare\n \navailable\n \nto\n \nan\n \nAI\n \nsystem\u2019,\n \ne.g.,\n \nwhether\n \nit\n \nhas\n \nthe\n ability  to  autonomously  conduct  \ufb01nancial  transactions  (Sharkey  et  al.  2024).  The  a\ufb00ordances  an  agent  has  access  to  will  ultimately  determine  what  capabilities  it  can  exercise.  \n3.1  Malicious  use \nIn  the  same  way  that  powerful  agent  systems  can  act  as  a  \u2018force  multiplier\u2019  for  bene\ufb01cial  things  like  \neconomic\n \nproductivity\n \nenhancement\n \nand\n \nscienti\ufb01c\n \ninnovation\u2014it\n \ncan\n \nalso\n \nact\n \nas\n \na\n \nforce\n \nmultiplier\n \nfor\n \nbad\n \nactors\n \nlooking\n \nto\n \nuse\n \nthis\n \ntechnology\n \nmaliciously\n \nto\n \ncause\n \nwidespread\n \nharm.\n \n \n18\n \nCTF\n \nchallenges\n \nrefer\n \nto\n \n\u2018Capture\n \nthe\n \nFlag\u2019\n \nchallenges,\n \nwhich\n \nare\n \ncompetitive\n \ncybersecurity\n \nexercises\n \nwhere\n \nparticipants\n \nsolve\n \nsecurity\n \npuzzles\n \nand\n \nbreak\n \ninto\n \ndeliberately\n \nvulnerable\n \nsystems\n \nto\n \n\ufb01nd\n \nhidden\n \npieces\n \nof\n \ntext\n \ncalled\n \n\u201c\ufb02ags.\u201d\n \nThese\n \nchallenges\n \nsimulate\n \nreal-world\n \nsecurity\n \nscenarios\n \nand\n \nhelp\n \npeople\n \ndevelop\n \npractical\n \nhacking\n \nand\n \ndefense\n \nskills.\n \n17\n See  UKAISI  pre-deployment  evaluations  of  o1  and  Claude  3.5  Sonnet  (UK  AI  Security  Institute  2024b ;  \n2024a\n).  \n      AGENT  GOVERNANCE   \u2502   27   \nMisuse  risks  from  AI  systems  have  already  been  raised  as  one  of  the  major  safety  concerns  arising  \nfrom\n \nfrontier\n \nAI\n \nsystems,\n \nand\n \nmitigating\n \nthese\n \nrisks\n \nhas\n \nbeen\n \na\n \nfocus\n \nof\n \nboth\n \nAI\n \ndevelopers\n \nand\n \nthe\n government  (Anthropic  2023;  UK  AI  Security  Institute  2025).  Some  of  the  areas  where  agent  systems  may  be  able  to  rapidly  enhance  malicious  use  risks  include:  \n\u25cf  Generating  and  disseminating  disinformation  at  an  unprecedented  scale  and  supporting  \nmanipulation\n \nof\n \npublic\n \nopinion\n \u25cf  Automating  and  scaling  up  o\ufb00ensive  cybersecurity  operations  \u25cf  Increasing  access  to  expert  capabilities  in  dual-use  scienti\ufb01c  research  and  development,  \nsuch\n \nas\n \nhelping\n \ndevelop\n \nnovel\n \nbiological\n \npathogens\n \nChatbot  systems  can  help  malicious  actors  gather  information  more  easily,  for  example,  by  identifying  potential  pandemic  pathogens  and  ways  to  acquire  them  (Soice  et  al.  2023).  Specialized  AI  tools  like  biological  design  tools  can  help  more  technical  users  conduct  novel  dual-use  scienti\ufb01c  research,  such  as  designing  novel  biothreats  (Batalis  2023).  However,  agent  systems  would  more  dramatically  enhance  misuse  risk  because  they  can  use  external  tools  and  execute  multi-stage  \nplans\n \nwithout\n \nclose\n \nhuman\n \nsupervision.\n \nFor\n \ninstance,\n \nwhile\n \na\n \nchatbot\n \nmight\n \nonly\n \nprovide\n \ninstructions\n \non\n \nhow\n \nto\n \nconduct\n \na\n \ncyberattack,\n \nan\n \nadvanced\n \nagent\n \nsystem\n \ncould\n \npotentially\n \ncarry\n \nout\n \nthe\n \nentire\n \nattack\n \nautonomously\u2014scanning\n \nand\n \nexploiting\n \nvulnerabilities,\n \nestablishing\n \npersistence,\n \nand\n \nex\ufb01ltrating\n \ndata\u2014all\n \nwithout\n \nrequiring\n \nthe\n \nhuman\n \nactor\n \nto\n \npossess\n \ntechnical\n \nexpertise\n \nor\n \nmanually\n \nexecute\n \neach\n \nstep.\n \nEven  if  developers  develop  safeguards  in  their  models  to  prevent  unauthorized  outputs,  these  can  \npotentially\n \nbe\n \novercome.\n \nResearchers\n \nfound\n \nthat\n \nLLM-based\n \nagents\n \ncould\n \nbe\n \neasily\n \njailbroken\n \nto\n \ncarry\n \nout\n \na\n \nrange\n \nof\n \nmalicious\n \ntasks,\n \nincluding\n \ncreating\n \nfake\n \npassports\n \nand\n \nassisting\n \nwith\n cybercrime  (Andriushchenko  et  al.  2024).  \n3.2  Accidents  and  loss  of  control \nBeyond  malicious  use,  agent  systems  may  also  pose  risks  due  to  unintended  failures.  The  deep  \nlearning-based\n \nmodels\n \nthat\n \nagent\n \nsystems\n \nare\n \nbuilt\n \naround\n \nhave\n \nbeen\n \nlargely\n \ninscrutable\u2014meaning\n \nit\n \nis\n \ndi\ufb03cult\n \nto\n \nunderstand\n \nhow\n \na\n \nmodel\n \narrives\n \nat\n \nany\n \ngiven\n \noutput.\n \nUnintended\n \nfailures\n \nin\n \nAI\n \nagents\n \nmight\n \nrun\n \nthe\n \ngamut\n \nfrom\n \nmore\n \nmundane\n \nfailures\n \nlike\n \nreliability\n \nissues\n \nto\n \nnovel,\n \nmore\n \nspeculative\n \nrisks\n \nlike\n \nscheming\n \nand\n \npower-seeking\n \nthat\n \nare\n \nlinked\n \nto\n \nhigher\n \nlevels\n \nof\n \ncapability\n \nand\n \ngoal-orientedness.\n \nThere  have  been  numerous  cases  of  simpler  or  less  general  agents  malfunctioning  in  ways  that  \nhave\n \ncaused\n \nharm.\n \nIn\n \n2022,\n \na\n \nTesla\n \nemployee\n \nwas\n \nkilled\n \nwhile\n \nusing\n \nthe\n \nAI-powered\n \nFull\n \nSel\n f-Driving  feature  (Thadani  et  al.  2024).  The  Tesla  car  failed  in  navigating  the  curved  mountain  roads,  leading  to  a  fatal  crash.  In  another  incident,  an  Air  Canada  chatbot,  due  to  a  hallucination,  \n      AGENT  GOVERNANCE   \u2502   28   \nincorrectly  advised  a  customer  that  he  could  retroactively  claim  a  bereavement  fare  discount  within  90  days,  causing  him  to  pay  full  price  for  his  \ufb02ights  (Belanger  2024).  A  tribunal  ruled  that  the  airline  was  responsible  for  the  chatbot\u2019s  misinformation  and  was  ordered  to  pay  damages.  While  these  \nincidents\n \nare\n \ndecreased\n \nif\n \nagent\n \nreliability\n \nissues\n \nare\n \nbetter\n \nmanaged,\n \neven\n \na\n \nrare\n \nincident\n \nwould\n \nhave\n \ndramatic\n \nconsequences\n \nif\n \nagents\n \nare\n \ndeployed\n \nin\n \nhigh\n \nstakes\n \nenvironments.\n \nBeyond  mundane  malfunctions,  agents  are  also  potentially  more  likely  to  increase  \u2018loss  of  control\u2019  \nrisks\n \nas\n \nthey\n \nbecome\n \nmore\n \ncapable.\n \nLoss\n \nof\n \ncontrol\n \ndescribes\n \nscenarios\n \nwhere\n \n\u201cAI\n \nsystems\n \ncome\n to  operate  outside  of  anyone's  control,  with  no  clear  path  to  regaining  control\u201d  (Department  of  Science,  Innovation  and  Technology  2025).  This  can  take  many  forms.   \nAs  explored  in  Scenario  2 earlier  in  this  report,  this  could  emerge  through  incremental  delegation  of  decision-making  authority  to  agent  systems  across  various  domains.  For  example,  organizations  \nmight\n \nincreasingly\n \nrely\n \non\n \nAI\n \nsystems\n \nfor\n \nregulatory\n \ncompliance,\n \n\ufb01nancial\n \noptimization,\n \nor\n \nresource\n \nallocation\n \ndecisions.\n \nAs\n \nthese\n \nsystems\n \nbecome\n \nembedded\n \nin\n \ninstitutional\n \nprocesses\n \nand\n \nwork\ufb02ows,\n \nhuman\n \nexpertise\n \nin\n \nthese\n \nareas\n \nmay\n \ngradually\n \nerode.\n \nOrganizations\n \ncould\n \nface\n \nincreasing\n \nswitching\n \ncosts\n \nand\n \ncompetitive\n \ndisadvantages\n \nif\n \nthey\n \nattempt\n \nto\n \nrevert\n \nto\n \nhuman-led\n \nprocesses.\n \nEven\n \nif\n \nevidence\n \nemerges\n \nthat\n \nthese\n \nsystems\n \nare\n \nproducing\n \noutcomes\n \nmisaligned\n \nwith\n \nlong-term\n \nsocietal\n \nwell-being\u2014such\n \nas\n \nexacerbating\n \ninequality\n \nor\n \nenvironmental\n \ndegradation\u2014by\n \nthat\n \npoint\n \nthe\n \nsystems\n \nmay\n \nbe\n \ntoo\n \npervasive,\n \nwell-defended,\n \nand\n \nintertwined\n \nwith\n \nsocietal\n \nfunctioning\n \nto\n \nbe\n \nstopped.\n19\n \nAnother  loss  of  control  scenario  is  \u2018rogue  replication\u2019  (Clymer,  Wijk,  and  Barnes  2024).  This  is  where  agents  manage  to  self  proliferate,  secure  and  use  compute  to  copy  themselves,  and  evade  \nshutdown\u2014creating\n \na\n \nresilient\n \npopulation\n \nthat\n \nis\n \nbeyond\n \nhuman\n \ncontrol.\n \nAutonomously\n \nreplicating\n \nagents\n \nare\n \nanalogous\n \nto\n \ncomputer\n \nworms\n \nthat\n \nassemble\n \nmassive\n \nbotnets,\n \nbut\n \ncould\n \nenable\n \na\n \nwider\n \nrange\n \nof\n \nharmful\n \nactions\n \nand\n \nbe\n \nmore\n \nresilient\n \nto\n \nshutdown.\n \nRather\n \nthan\n \nvoluntarily\n \nceding\n \ncontrol\n \nto\n \nagent\n \nsystems,\n \nthis\n \nscenario\n \ninvolves\n \nagents\n \nintentionally\n \ntaking\n \nactions\n \nto\n \nprevent\n \nhuman\n \ncontrol\n \nand\n \noversight.\n \nLoss  of  control  scenarios  are  viewed  by  some  as  implausible  and  seem  unlikely  to  occur  at  current  \nagent\n \ncapabilities,\n \nbut\n \nmodel\n \ncapabilities\n \nassociated\n \nwith\n \nloss\n \nof\n \ncontrol\n \nhave\n \nbeen\n \nrecognized\n \nas\n \nposing\n \n\u2018severe\n \nrisks\u2019\n \nby\n \nnations\n \nat\n \nthe\n \nAI\n \nSeoul\n \nSummit\n \nand\n \nare\n \nmonitored\n \nin\n \nthe\n \nsafety\n \nframeworks\n \nof\n \nOpenAI,\n \nGoogle\n \nDeepMind,\n \nand\n \nAnthropic.\n \nMonitoring\n \nassociated\n \ncapabilities\n \ncan\n \nserve\n \nas\n \nan\n \nearly\n \nwarning\n \nsign\n \nthat\n \nthese\n \nrisks\n \nare\n \nbecoming\n \nmore\n \nconcrete:\n \n\u25cf\n \nScheming  and  deception \u2014  the  ability  of  an  agent  to  pursue  misaligned  goals  and  hide  \ntheir\n \ntrue\n \ncapabilities\n \nand\n \nobjectives.\n \nAn\n \nevaluation\n \nsuite\n \nfrom\n \nApollo\n \nResearch\n \nfound\n \nthat\n \nseveral\n \nfrontier\n \nlanguage\n \nmodels\n \ndisplay\n \nscheming\n \ncapabilities\n \nacross\n \nmultiple\n \nevaluations\n \nwhen\n \nprompted\n \nto\n \nstrongly\n \npursue\n \ntheir\n \ngoal\u2014resulting\n \nin\n \nattempted\n \nex\ufb01ltration,\n \ndisabling\n \n19\n An  example  scenario  that  describes  this  can  be  found  in  Critch  and  Russell (2023) .  \n      AGENT  GOVERNANCE   \u2502   29   \noversight  mechanisms,  and  subtly  manipulating  outputs  (Meinke  et  al.  2025).  Separately,  a  team  from  Anthropic  and  Redwood  Research  found  that  Claude  displayed  \u2018alignment  \nfaking\u2019\n \nbehavior,\n \ni.e.,\n \nit\n \npretended\n \nto\n \nshare\n \nviews\n \nit\n \ndid\n \nnot\n \nactually\n \nhold\n \nduring\n \ntraining\n \nin\n order  to  prevent  modi\ufb01cation  of  its  behavior  out  of  training  (Greenblatt,  Denison,  et  al.  2024).  \n\u25cf  Speci\ufb01cation  gaming is  where  an  AI  system  \ufb01nds  loopholes  or  exploits  to  obtain  rewards  in  \na\n \nway\n \nthat\n \nis\n \ncontrary\n \nto\n \nwhat\n \ntheir\n \ndevelopers\n \nintended.\n Reward  tampering is  a  speci\ufb01c  \nform\n \nof\n \nspeci\ufb01cation\n \ngaming\n \nwhere\n \na\n \nmodel\n \nalters\n \nthe\n \ntraining\n \nprocess\n \nto\n \nincrease\n \nits\n \nreward.\n \nSince\n \nan\n \nAI\n \nagent\u2019s\n \nbehavior\n \nwill\n \nbe\n \nstrongly\n \nin\ufb02uenced\n \nby\n \nits\n \nrewards,\n \ntampering\n \nwith\n \nthem\n \nwill\n \nadd\n \nunpredictability\n \nand\n \nmake\n \nthem\n \nmore\n \ndi\ufb03cult\n \nto\n \ncontrol.\n \nResearchers\n \nhave\n \ndemonstrated\n \nthat\n \nmodels\u2014in\n \nsome\n \nrare\n \ninstances\u2014generalize\n \nfrom\n \nmore\n \nharmless\n \nforms\n \nof\n \nspeci\ufb01cation\n \ngaming\n \nto\n \nmuch\n \nmore\n \nsophisticated\n \nand\n \nharmful\n \nbehaviors\n \nlike\n \nreward\n \ntampering.\n \n \n\u25cf  Accelerating  AI  R&D \u2014  the  ability  of  systems  to  automate  AI  R&D  is  critical  because  if  a  \nsystem\n \ncan\n \nsubstantially\n \nimprove\n \neither\n \nitself\n \nor\n \nfuture\n \nAI\n \nmodels,\n \nthis\n \ncould\n \nresult\n \nin\n \na\n \nsubstantial\n \nacceleration\n \nof\n \nAI\n \ncapabilities.\n \nDangerous\n \ncapabilities\n \nor\n \nbehavioral\n \ntendencies\n \ncould\n \nthen\n \nemerge\n \nmore\n \nquickly\n \nthan\n \nsocietal\n \nadaptation\n \nand\n \nother\n \ninterventions\n \ncan\n \nbe\n \nput\n \nin\n \nplace.\n \n3.3  Security  risks \nSince  agents  are  more  likely  to  use  external  tools,  interfaces,  and  also  to  interact  with  other  \nagents\u2014there\n \nare\n \nmore\n \nattack\n \nsurfaces\n \nfor\n \nthem\n \nthan\n \nstandard\n \nLLMs,\n \nwhich\n \ncould\n \nbe\n \nexploited\n \nby\n \nmalicious\n \nactors.\n \nIt\n \nis\n \nnot\n \nonly\n \nthe\n \nmodel\n \nitself\n \nthat\n \nis\n \nvulnerable\n \nto\n \nan\n \nattack,\n \nbut\n \nalso\n \nthe\n integrations  between  the  model  and  its  external  components  (Reiner  2024).  For  example,  if  APIs  were  unsecured  attackers  could  intercept  requests  to  manipulate  an  agent.  Also,  given  that  agents  \ncan\n \nmaintain\n \nmemory\n \nover\n \ntime,\n \nthis\n \nstored\n \ninformation\n \ncan\n \nbecome\n \na\n \ntarget\n \nfor\n \nexploitation.\n \nAn\n \nagent\u2019s\n \nmemory\n \nlog\n \ncould\n \nstore\n \nsensitive\n \ninformation\n \nthat\n \ncan\n \nbe\n \nleaked\n \nthrough\n \nadversarial\n \nattacks\n \nor\n \nan\n \nagent\u2019s\n \nmemory\n \ncould\n \nbe\n \naltered\n \nto\n \nbelieve\n \na\n \nmalicious\n \nactor\n \nis\n \nan\n \nauthorized\n \nuser.\n \nIn  multi-agent  settings,  an  agent  can  be  used  to  sabotage  or  unduly  in\ufb02uence  other  agent  systems  (Terekhov  et  al.  2023).  In  worlds  where  agent-agent  communication  is  commonplace,  this  opens  up  the  possibility  of  more  severe  safety  issues,  such  as  \u2018infectious  jailbreaks\u2019  where  a  single  compromised  agent  can  rapidly  infect  others,  leading  to  widespread  harmful  behaviors  (Gu  et  al.  2024).  \nSince  agents  will  be  able  to  take  a  more  expansive  range  of  actions  than  chatbot-style  systems,  \ne.g.,\n \nexecute\n \ncode\n \nor\n \naccess\n \nother\n \nmachines\n \nin\n \nthe\n \nnetwork,\n \nthe\n \nconsequences\n \nof\n \nmanipulating\n \nagent\n \nbehavior\n \nis\n \nmuch\n \nmore\n \nsevere.\n \n      AGENT  GOVERNANCE   \u2502   30   \n3.4  Other  systemic  risks \nThere  are  also  risks  and  harms  associated  with  agents  that  might  happen  only  when  agents  are  \ndeployed\n \nmore\n \nwidely\n \nacross\n \nsociety.\n \nOne\n \nexample\n \nis\n \nmass\n \nlabor\n \ndisplacement\n \nand\n \nunemployment\n \nof\n \nhuman\n \nworkers\n \ndue\n \nto\n \nabundant\n \nagents.\n \nWhile\n \nfoundation\n \nmodel\n \nagents\n \nin\n \nthis\n \nreport\n \nare\n \nsoftware\n \nagents,\n \nonly\n \nable\n \nto\n \ntake\n \non\n \ntasks\n \nin\n \nthe\n \ndigital\n \nworld,\n \nthere\n \nare\n \nongoing\n \ne\ufb00orts\n to  build  agents  that  can  operate  generally  in  the  physical  world  as  well  (NVIDIA,  n.d.).  \nThere  could  also  be  a  systemic  political  risk  from  agents\u2014extreme  power  concentration  could  arise  \nfrom\n \nwidespread\n \nagent\n \ndeployment.\n \nOne\n \nway\n \nthis\n \ncould\n \nplay\n \nout\n \nis\n \nwith\n \nincreasing\n \npower\n \nand\n \nin\ufb02uence\n \naccruing\n \nto\n \nthe\n \n\u2018coding\n \nelite\u2019:\n \nsoftware\n \ndevelopers,\n \ntech\n \nexecutives,\n \ninvestors,\n \nand\n machine  learning  experts  (Chan  et  al.  2023).  If  agents  become  increasingly  central  to  the  economy,  then  actors  that  have  the  most  in\ufb02uence  over  AI  development  and  deployment  may  increasingly  be  \nable\n \nto\n \nin\ufb02uence\n \npolitics\n \nwithout\n \nmeaningful\n \nchecks\n \nand\n \nbalances.\n \nAnother  power  concentration  scenario  is  if  mass  deployment  of  agent  systems  helps  to  entrench  \npolitical\n \nelites\n \nand\n \nshields\n \nthem\n \nfrom\n \ndemocratic\n \noversight\n \nand\n \naccountability.\n \nTraditionally,\n \npolitical\n \nleaders\n \nstay\n \nin\n \npower\n \nby\n \nsatisfying\n \na\n \nwinning\n \ncoalition\u2014a\n \nsubset\n \nof\n \nthe\n \npopulation\n \nwhose\n \nsupport\n is  crucial  (Smith  et  al.  2005).  Powerful  agent  systems  could  help  regimes  scale  up  surveillance,  control  information,  and  automate  repression\u2014which  could  reduce  the  need  for  broader  public  support  to  maintain  power  (Minardi  2020).  \nInstabilities  might  even  arise  from  reducing  transaction  costs  in  markets.  Some  researchers  have  \ncoined\n \nthe\n \nterm,\n \n\u2018hyperswitching\u2019,\n \nreferring\n \nto\n \nthe\n \npotential\n \nfor\n \nAI\n \nassistants\n \nto\n \nsimultaneously\n \ndirect\n \nmillions\n \nof\n \nconsumers\n \nto\n \nrapidly\n \nswitch\n \nfrom\n \none\n \nprovider\n \nto\n \nanother\n \nbased\n \non\n \nsmall\n \nprice\n \nor\n \nquality\n advantages  (Van  Loo  2019).  This  could  create  market  volatility  and  systemic  risk  by  potentially  causing  massive,  coordinated  customer  exits  that  might  destabilize  companies,  trigger  widespread  \nbankruptcies,\n \nand\n \ncreate\n \n\ufb01nancial\n \ninstability\n \nsimilar\n \nto\n \nbank\n \nruns.\n \n \n \n4.  What  is  agent  governance?  \nAgent  governance  is  focused  on  navigating  the  transition  to  a  world  where  AI  agents  can  carry  out  \na\n \nwide\n \narray\n \nof\n \ntasks\n \nwith\n \nhuman-level-or-above\n \npro\ufb01ciency,\n \nmanaging\n \ntheir\n \nimpacts\n \nas\n \nwell\n \nas\n \nusing\n \nthe\n \na\ufb00ordances\n \nthey\n \nrequire\n \nto\n \ngovern\n \ntheir\n \nusage.\n \nThis\n \nincludes\n \nmeasures\n \nto\n \nshepherd\n \nthe\n \ntechnology\n \nso\n \nthat\n \nit\n \ncan\n \ndevelop\n \nin\n \nways\n \nthat\n \nare\n \nbroadly\n \nbene\ufb01cial\n \nand\n \nat\n \na\n \npace\n \nwhere\n \nsocietal\n \nadaptation\n \nis\n \npossible.\n \n      AGENT  GOVERNANCE   \u2502   31   \nHow  is  agent  governance  di\ufb00erent  from  AI  governance  more  broadly?  While  many  of  the  concerns  \nthat\n \nanimate\n \nAI\n \ngovernance,\n \nsuch\n \nas\n \nsafety,\n \nsecurity,\n \naccountability,\n \nand\n \nfairness,\n \nalso\n \napply\n \nto\n \nagent\n \ngovernance,\n \nunique\n \nconsiderations\n \narise\n \ndue\n \nto\n \nthe\n \ndistinct\n \ncharacteristics\n \nof\n \nagents:\n \n\u25cf  Agents  could  engage  in  complex  activities  on  behalf  of  their  users,  without  the  users  \nknowing\n \nwhether\n \nor\n \nhow\n \nsuch\n \nactivities\n \nwere\n \naccomplished.\n \nIssues\n \naround\n \ninformation\n \nasymmetry,\n \nauthority,\n \nloyalty,\n \nand\n \nliability\n \nbecome\n \nmore\n \nsalient\n \nwith\n \nagents\n \nthan\n \nwith\n \npure\n chatbot  or  tool  AI  systems  (Kolt  2025;  Aguirre  et  al.  2020;  Benthall  and  Shekman  2023).   \u25cf  Agents  can  take  actions  in  the  world  through  interacting  with  tools  and  other  external  \nsystems.\n \nThese\n \ntools\n \nand\n \nsystems\n \nare\n \nadditional\n \nlevers\n \nthat\n \ncan\n \nin\ufb02uence\n \nthe\n \nuse\n \nand\n \nimpacts\n \nof\n \nagents.\n \u25cf  Agents  can  interact  with  each  other,  so  multi-agent  dynamics  and  potential  failures  need  to  \nbe\n \ninvestigated.\n \nAgents\n \ncould\n \ncollude,\n \ncon\ufb02ict,\n \nor\n \nopen\n \nup\n \ncascading\n \ne\ufb00ects\n \nand\n \nnew\n attack  surfaces  when  coordinating  to  achieve  their  given  goals  (Hammond  et  al.  2025).  For  example,  the  AI  worm  \u201cMorris  II\u201d  demonstrates  this  risk  by  injecting  a  self-replicating  \nadversarial\n \nprompt\n \ninto\n \ngenerative\n \nAI\n \nemail\n \nassistants,\n \ncausing\n \neach\n \nagent\n \nto\n \nunknowingly\n propagate  the  malicious  instruction  to  others  (Cohen,  Bitton,  and  Nassi  2025).  \u25cf  Agents,  due  to  their  ability  to  pursue  objectives  persistently  over  time  and  coordinate  with  \nother\n \nentities,\n \nmay\n \nbe\n \nable\n \nto\n \nplay\n \nmore\n \nof\n \na\n \ncentral\n \nrole\n \nin\n \ngovernance\n \nof\n \nAI\n \nsystems.\n \nThere\n \nhave\n \nbeen\n \nproposals\n \nto\n \nhave\n \nagents\n \nmonitor\n \nother\n \nagents\n \nand\n \nintervene\n \nto\n \nprevent\n \nharmful\n behaviors  (Naihin  et  al.  2023).  Given  the  speed  and  scale  of  agent  activities,  it  could  be  advantageous  to  employ  trusted  agents  to  help  with  AI  governance.  \nGiven  these  considerations,  governance  approaches  that  work  for  less  agentic  systems  may  have  \nto\n \nbe\n \nadapted\n \nfor\n \nagents.\n \nFor\n \ninstance,\n \ncurrent\n \nproduct\n \nliability\n \nframeworks\n \nalready\n \nface\n \nchallenges\n \nwith\n \nAI\n \nsystems,\n \nand\n \nthese\n \nchallenges\n \nintensify\u2014though\n \nremain\n \nconceptually\n \nsimilar\u2014for\n \npersistent\n \nagent\n \nsystems.\n \nTraditional\n \nliability\n \nframeworks\n \nwork\n \nbest\n \nwith\n \nclear\n \nlines\n \nof\n \ncausation\n \nbetween\n \na\n \nproduct's\n \ndesign,\n \noperation,\n \nand\n \nresulting\n \nharms.\n \nAgents\n \nthat\n \npursue\n \ngoals\n \nacross\n \nextended\n \ntimeframes,\n \nmaking\n \nhundreds\n \nof\n \ninterconnected\n \ndecisions\n \nwhile\n \nincorporating\n \nenvironmental\n \ndata\n \nand\n \ntool\n \ninteractions,\n \ncreate\n \nmore\n \ncomplex\n \nattribution\n \nquestions.\n \nDetermining\n \nrelative\n \nresponsibility\n \nbetween\n \ndeveloper\n \ndesign\n \nchoices,\n \noperator\n \noversight\n \npractices,\n \nand\n \nemergent\n \nbehaviors\n \nmight\n \nbe\n \nsigni\ufb01cantly\n \nmore\n \ndi\ufb03cult.\n \nThere  are  a  range  of  important  problems  to  address  in  agent  governance.  One  key  area  is  better  \nevaluating\n \nagent\n \nperformance\n \nand\n \nassociated\n \nrisks\n \nand\n \nimpacts\n \nover\n \ntime,\n \nparticularly\n \nif\n \nthey\n \nbecome\n \nmore\n \nautonomous,\n \nintegrated\n \ninto\n \ncritical\n \ninfrastructure,\n \nand\n \ncapable\n \nof\n \nin\ufb02uencing\n \nhigh-stakes\n \ndecisions.\n \nWork\n \nhere\n \ncould\n \ninclude:\n \n\u25cf  Tracking  and  forecasting  general  agent  performance,  such  as  the  length  of  tasks  (given  \nhuman\n \nbaselines)\n \nthat\n \nagents\n \ncan\n \ncomplete.\n \u25cf  Developing  agent-speci\ufb01c  evaluations  that  capture  capabilities  like  multi-agent  cooperation.  \n      AGENT  GOVERNANCE   \u2502   32   \n\u25cf  More  detailed  threat  modeling  work  involving  agents,  including  how  risks  change  as  agent  \ncapabilities\n \nand\n \na\ufb00ordances\n \nchange\n \n(e.g.,\n \nif\n \nan\n \nagent\n \nhas\n \naccess\n \nto\n \na\n \nbank\n \naccount).\n \u25cf  Development  of  better  approaches  to  evaluate  systemic  economic  and  political  risks  from  \nmass\n \nagent\n \ndeployment.\n \nAnother  important  area  is  to  develop  mechanisms  and  structures  for  managing  risks  from  \nagents\n \nacross\n \ntheir\n \nlifecycle\n.\n \nThis\n \ncan\n \ninvolve\n \ntechnical\n \nmechanisms\n \nand\n \ntools\n \nthat\n \nassist\n \nwith\n \ngovernance,\n \nsuch\n \nas\n \nmeasures\n \nenabling\n \nagent\n \nshutdown\n \nin\n \nthe\n \nevent\n \nof\n \nmalfunction\n \nor\n \nmisconduct,\n \nas\n \nwell\n \nas\n \npolicy\n \nand\n \nlegal\n \nmechanisms\n \nthat\n \ncan\n \naid\n \nagent\n \ngovernance.\n \nGiven\n \nthe\n \nimmense\n \nopportunities\n \nand\n \nrisks\n \nposed\n \nby\n \nAI\n \nagents\u2014one\n \ncentral\n \nquestion\n \nhere\n \nis\n \nwhether\n \nAI\n agents  can  be  made  to  reliably,  safely,  and  ethically  operate  (Kolt  2025).  This  could  encompass  both  interventions  that  a\ufb00ect  the  agent  and  its  underlying  model  directly,  but  could  also  include  \nother\n \ninfrastructure\n \nthat\n \nstructures\n \nagent\n \ninteractions,\n \nfor\n \nexample\n \nan\n \nID\n \nsystem\n \nor\n \nagent\n \nrollback\n systems  (Chan  et  al.  2025).  As  touched  on  in  the  earlier  scenarios,  it  might  end  up  preferable  to  create  separate  \u2018internets\u2019  for  agents  and  humans.\n 20\n More  detail  on  potential  interventions  and  associated  governance  outcomes  can  be  found  in  the  following  section.  \nIn  addition  to  managing  potential  risks  from  agent  systems,  it  could  also  be  important  to  \nincentivize\n \nbene\ufb01cial\n \nuses\n \nof\n \nagents,\n \nparticularly\n \ndefensive\n \nuses\n \nof\n \nagents.\n \nFor\n \nexample,\n \nan\n \nagent\n \nwith\n \nthe\n \ncapability\n \nto\n \nidentify\n \nzero-day\n \nexploits\n \nin\n \ncritical\n \ninfrastructure\n \ncould\n \nbe\n \nused\n \nby\n \na\n \nmalicious\n \nactor\n \nto\n \nlaunch\n \ndevastating\n \nattacks,\n \nor\n \nit\n \ncould\n \nbe\n \nused\n \nby\n \ndefenders\n \nto\n \nproactively\n \npatch\n \nvulnerabilities\n \nbefore\n \nthey\n \nare\n \nexploited.\n \nSimilarly,\n \nan\n \nagent\n \ncapable\n \nof\n \nautonomous\n \ncode\n \ngeneration\n \ncould\n \nbe\n \nused\n \nto\n \ncreate\n \nhighly\n \ne\ufb03cient\n \nmalware\n \nor\n \nto\n \ndevelop\n \nmore\n \nrobust\n \nsoftware\n \nsystems.\n \nPersonal\n \nlanguage\n \nmodel\n \nagents\n \ncould\n \no\ufb00er\n \na\n \nsuperior\n \nalternative\n \nto\n \ncompany-controlled\n \nrecommender\n \nsystems\n \nby\n \nreducing\n \nmass\n \nsurveillance,\n \ndecentralizing\n \npower,\n \nand\n \nenhancing\n \nuser\n agency  in  content  discovery  (Lazar  et  al.  2024).  Work  to  identify,  implement,  and  encourage  the  most  useful  defensive  and  bene\ufb01cial  applications  of  agents\u2014supporting  \u2018di\ufb00erential  technology  development\u2019\u2014could  be  critical  in  managing  risks  and  securing  bene\ufb01ts  from  AI  (Sandbrink  et  al.  2022).  \nIn  worlds  where  agents  are  capable  of  a  broad  swathe  of  economically  useful  tasks  and  can  readily  \nsubstitute\n \nfor\n \nhuman\n \nlabor,\n \nit\n \nwill\n \nbe\n \nimportant\n \nto\n \n\ufb01gure\n \nout\n \nways\n \nto\n \nshare\n \nbene\ufb01ts\n \nand\n \naccess\n \nto\n \nagents\n.\n \nOtherwise,\n \nthere\n \nis\n \na\n \nrisk\n \nof\n \nexacerbating\n \nexisting\n \ninequalities,\n \nwith\n \na\n \nconcentrated\n \nset\n \nof\n \nactors\n \ncontrolling\n \nthe\n \nvast\n \nmajority\n \nof\n \nproductive\n \ncapacity.\n \nThere\n \nhave\n \nbeen\n \nvarious\n \nmeasures\n \nsuggested\n \nto\n \nmitigate\n \nthese\n \nrisks,\n \nfor\n \nexample\n \na\n \nUniversal\n \nBasic\n \nIncome\n \n(UBI)\n \nscheme\n \nmight\n \nbe\n used  to  redistribute  pro\ufb01ts  from  frontier  AI  companies  (Goolsbee  2018).  Alternatively,  there  could  be  arrangements  to  ensure  equitable  access  to  agents  or  agent-based  services  directly,  like  \npersonalized\n \neducation\n \nor\n \nhealthcare.\n \nThere\n \nare\n \nopen\n \nquestions\n \naround\n \nwhat\n \nbene\ufb01t-sharing\n regime  would  work  best,  and  how  this  regime  might  be  best  administered  (O\u2019Keefe  et  al.  2020).  \n20\n See  Chan  et  al. (2025)  for  more  discussion  of  \u2018Agent  channels\u2019  (Section  4.1).  \n      AGENT  GOVERNANCE   \u2502   33   \nIt  will  also  be  important  to  anticipate  how  existing  policy  and  legal  frameworks  will  need  to  \nchange\n \nto\n \nadapt\n \nto\n \na\n \nworld\n \nwith\n \nmass\n \ndeployment\n \nof\n \nadvanced\n \nagent\n \nsystems\n.\n \nFor\n \ninstance,\n \nshould\n \ncurrent\n \nmodel-oriented\n \npre-deployment\n \ntesting\n \nexpand\n \nto\n \nalso\n \ninclude\n \nadditional\n methodologies,  such  as  simulation-based  testing  involving  multiple  agents  (Naihin  et  al.  2023)?  Professional  licensing  frameworks  may  also  face  transformation.  Fields  like  law,  medicine,  and  \n\ufb01nance\n \nmay\n \nneed\n \nto\n \ndevelop\n \nhybrid\n \nlicensing\n \nsystems\n \nthat\n \ncertify\n \nboth\n \nhuman\n \npractitioners\n \nand\n \nthe\n \nagent\n \nsystems\n \nthey\n \nemploy.\n \nOn\n \nthe\n \ninternational\n \nfront,\n \nthere\n \nmay\n \nbe\n \ndomains\n \nwhere\n \nmultilateral\n \ncoordination\n \non\n \nagent\n \ngovernance\n \nis\n \nconsidered\n \ncritical,\n \nand\n \nwhere\n \nnational\n \nregulation\n \nor\n \ncorporate\n \nself-regulation\n \nis\n \ninadequate.\n \nFor\n \nexample,\n \nUN\n \nSecretary-General\n \nAnt\u00f3nio\n \nGuterres\n \nhas\n \ncalled\n \nfor\n \nlegally\n \nbinding\n \ninstruments\n \nto\n \nprohibit\n \nthe\n \nuse\n \nof\n \nlethal\n \nautonomous\n \nweapon\n \nsystems\n \nthat\n \nfunction\n without  human  control  or  oversight  (United  Nations  2023).   \nFinally,  across  all  these  areas  of  possible  work,  it  is  crucial  to  better  understand  the  key  \nstakeholders\n \nwho\n \ncan\n \neither\n \nlead\n \nor\n \nsupport\n \nvarious\n \nagent\n \ngovernance\n \ninitiatives.\n \nThese\n \nstakeholders\n \nencompass\n \na\n \ndiverse\n \ngroup,\n \nincluding:\n \ndevelopers\n \n(both\n \nfrontier\n \nAI\n \ncompanies\n \nand\n \nsmaller\n \norganizations,\n \nlike\n \ndecentralized\n \nresearch\n \ngroups),\n \nservice\n \nproviders,\n \nusers,\n \nregulators,\n \nand\n \nfuture\n \nAI\n \nagents\n \nthemselves.\n21\n \nAcross\n \nthe\n \nAI\n \nagent\n \nlifecycle,\n \neach\n \nof\n \nthese\n \nstakeholders\n \nwill\n \nhave\n \ndi\ufb00erent\n \nopportunities\n \nand\n \nincentives\n \nto\n \nintervene\n \nto\n \nmanage\n \nissues\n \narising\n \nfrom\n \nagents.\n \nRight  now,  the  \ufb01eld  of  agent  governance  is  in  its  infancy.  Only  a  small  number  of  researchers,  \nprimarily\n \nin\n \ncivil\n \nsociety\n \nand\n \nsome\n \nof\n \nthe\n \nfrontier\n \nAI\n \ncompanies\n \nare\n \nworking\n \non\n \nthese\n \nopen\n \nquestions.\n \nA\n \nfew\n \norganizations\n \nprovide\n \nfunding\n \nfor\n \nresearch\n \nwork\n \non\n \nagent\n \ngovernance,\n \nand\n \nmajor\n \nacademic\n \nconferences\n \nplace\n \nsome\n \nattention\n \non\n \nthe\n \ntopic.\n22\n \nHowever,\n \nthe\n \nlevel\n \nof\n \nattention\n \nand\n \nfunding\n \nfor\n \nagent\n \ngovernance-related\n \nwork\n \nis\n \nextremely\n \nlimited\n \nrelative\n \nto\n \nthe\n \nlevel\n \nof\n \nattention\n \nand\n \ninvestment\n \ninto\n \nAI\n \nagents\n \nby\n \nmajor\n \ncompanies\n \nand\n \nstartups.\n23\n \n \nMuch  of  the  interventions  that  have  been  proposed  (a  subset  of  which  are  detailed  in  the  next  \nsection)\n \nare\n \nlargely\n \nuntested\n \nand\n \nrequire\n \nadditional\n \ne\ufb00ort\n \nto\n \n\ufb02esh\n \nout\n \nfurther\n \nbefore\n \nthey\n \nwould\n \nbe\n \nready\n \nto\n \nimplement.\n \n \n5.  Agent  interventions  \nGiven  the  stakes  associated  with  the  development  and  usage  of  agent  AI,  implementing  both  \ntechnical\n \nand\n \npolicy\n \ninterventions\n \nis\n \nvital.\n \nThis\n \nsection\n \noutlines\n \na\n \ntaxonomy\n \nof\n \nproposed\n \n23\n On  the  start-up  side  alone,  there  have  been  hundreds  of  millions  of  dollars  raised  by  individual  companies  \non\n \nthe\n \npromise\n \nof\n \nbuilding\n \nAI\n \nagents\n \nfor\n \nenterprise\n \nuse\n \n(Rajesh  and  Hu  2023 )\n.\n \n22 \n \nFor\n \nexamples\n \nof\n \nfunding\n \nfor\n \nagent\n \ngovernance\n \nwork,\n \nsee\n \nthis\n \nUK\n \nAI\n \nSecurity\n \ninstitute\n \n(n.d.) grants  program  \nshowing\n \ninterest\n \nin\n \nprojects\n \naround\n \nagent\n \ninfrastructure\n \nand\n research  grants  from  the  Cooperative  AI  \nFoundation (2025) .  For  examples  of  agent  governance  in  academic  conferences,  see  this  NeurIPS  workshop \ntitled\n \n\u2018Towards\n \nSafe\n \n&\n \nTrustworthy\n \nAgents\u2019\n \n(Pan  et  al.  2024 )\n.\n \n21 \n \u2018Service  providers\u2019  refers  to  actors  providing  tools  and  infrastructure  that  agents  use.  \n      AGENT  GOVERNANCE   \u2502   34   \ninterventions,  with  examples,  that  are  tailored  to  agent  AI  and  have  been  sourced  from  a  literature  \nreview\n \nand\n \nexpert\n \ninterviews.\n \nMany\n \ninterventions\n \ndraw\n \ninspiration\n \nfrom\n \nrelated\n \n\ufb01elds,\n \nsuch\n \nas\n \n\ufb01nance\n \nand\n \nautonomous\n \nvehicles,\n \nwhich\n \nalso\n \ninvolve\n \nmanaging\n \nagent-like\n \nentities.\n \nWe\n \nde\ufb01ne\n \nagent\n \ninterventions\n \nas\n \nmeasures,\n \npractices,\n \nor\n \nmechanisms\n \ndesigned\n \nto\n \nprevent,\n \nmitigate,\n \nor\n \nmanage\n \nthe\n \nrisks\n \nassociated\n \nwith\n \nagents.\n \nTechnical  interventions  are  measures  to  modify  the  design  of  agents  or  the  technical  systems  around  them.  These  can  be  implemented  at  three  layers  (Toner  et  al.  2024):  \n\u25cf  Model:  the  underlying  foundation  model(s)  acting  as  the  \u2018controller\u2019  of  the  agent.\n 24\n \u25cf  System:  the  sca\ufb00olding  program  and  other  components  built  around  the  agent  that  allow  it  \nto\n \ninteract\n \nwith\n \nusers,\n \ntools,\n \nand\n \ntake\n \nactions.\n \u25cf  Ecosystem:  the  broader  space  the  agents  are  interacting  with,  for  example  online  payment  \ninfrastructure,\n \nweb\n \nbrowsers,\n \nphysical\n \nactuators,\n \nexternal\n \nagents,\n \nand\n \nso\n \non.\n \n \nPolicy  and  legal  interventions  are  measures  designed  to  establish  rules,  norms,  and  accountability  \nmechanisms\n \nfor\n \nthe\n \ndevelopment\n \nand\n \nuse\n \nof\n \nagents.\n \nFor\n \nexample,\n \nthere\n \ncould\n \nbe\n \na\n \nlegal\n \nrequirement\n \nthat\n \nall\n \nagent\n \nsystems\n \nhave\n \na\n \nunique\n \nID,\n \nor\n \nbest\n \npractice\n \nguidance\n \nthat\n \nagents\n \nare\n \nnot\n \nallowed\n \nto\n \nconduct\n \n\ufb01nancial\n \ntransactions\n \nover\n \na\n \ngiven\n \nlimit\n \nwithout\n \nhuman\n \nsign-o\ufb00.\n \nOne  distinct  aspect  of  AI  agents  is  that  they  may  be  fruitfully  used  in  the  governance  process  \nitself\u2014governance\n \nby\n \nagents.\n \nGiven\n \nthat\n \nmore\n \nadvanced\n \nagents\n \ncan\n \nsubstitute\n \nfor\n \nhuman\n \nlabor\n \nin\n \nmany\n \ncases,\n \nagents\n \ncould\n \nserve\n \nas\n \nautomated\n \nmonitors,\n \nenforcers,\n \nand\n \nmediators\n \nin\n \na\n \ngovernance\n \nregime.\n \nIt\n \nmay\n \nbe\n \nnecessary\n \nto\n \nthink\n \nmore\n \nexpansively\n \nabout\n \ninstitutional\n \npossibilities\n \nin\n \nworlds\n \nwhere\n \nrelatively\n \ntrustworthy,\n \ncapable\n \nagents\n \ncan\n \nwork\n \nalongside\n \nhumans\n \nto\n \nmanage\n \ngovernance\n \noutcomes.\n \nThe  following  taxonomy  is  not  meant  to  be  comprehensive,  but  to  re\ufb02ect  distinct  governance  \nobjectives\n \nfor\n \nagents\n \nand\n \ngive\n \nan\n \ninitial\n \nsense\n \nof\n \nthe\n \ntypes\n \nof\n \nmeasures\n \nthat\n \ncould\n \nhelp\n \nachieve\n \nthem.\n \nThe\n \nexample\n \nmeasures\n \nmentioned\n \nhere\n \nrepresent\n \nonly\n \na\n \nfew,\n \nlargely\n \nuntested\n \navenues\n \nfor\n \nmanaging\n \nrisks\n \nfrom\n \nagents.\n \nAgent\n \ngovernance\n \nas\n \na\n \n\ufb01eld\n \nis\n \nonly\n \nin\n \nits\n \nearly\n \nstages,\n \nand\n \nmuch\n \nmore\n \nresearch\n \nis\n \nneeded\n \nto\n \nthink\n \nthrough\n \nand\n \n\ufb02esh\n \nout\n \nrobust\n \ninterventions\n \nfor\n \nagents.\n \nAgent  interventions  taxonomy\n \nCategories  De\ufb01nition  \n24\n When  we  refer  to  \u2018controller\u2019  for  foundation  model-based  agents,  we  refer  to  systems  where  the  model  is  \n\u2018dynamically\n \n[directing]\n \ntheir\n \nown\n \nprocesses\n \nand\n \ntool\n \nusage,\n \nmaintaining\n \ncontrol\n \nover\n \nhow\n \nthey\n \naccomplish\n \ntasks\u2019\n \n(Anthropic\n \n2024)\n.\n \n      AGENT  GOVERNANCE   \u2502   35   \nAlignment  Measures  to  ensure  that  agent  systems  behave  in  ways  that  are  consistent  with  a  given  principal\u2019s  values,  intentions,  and  interests  (i.e.,  are  aligned)  and  also  establish  trust  that  these  systems  are  actually  su\ufb03ciently  aligned.  \nControl  Measures  that  constrain  the  behavior  of  AI  agents  to  ensure  they  operate  within  prede\ufb01ned  boundaries.  This  includes  measures  that  prevent  agents  from  executing  harmful  actions.   \nVisibility  Measures  that  make  the  behavior,  capabilities,  and  actions  of  AI  systems  understandable  and  observable  to  humans.  \nSecurity  and  robustness  \nMeasures  intended  to  secure  agent  systems  from  various  external  threats,  protect  the  integrity  and  con\ufb01dentiality  of  data,  and  ensure  reliable  performance  even  under  adverse  conditions.   \nSocietal  integration  Measures  intended  to  support  long-term  integration  of  agents  into  existing  social,  political,  and  economic  systems\u2014addressing  issues  such  as  inequality,  concentration  of  power,  and  establishing  accountability  structures.  \n \nThis  taxonomy  is  only  one  way  to  classify  agent  interventions,  but  it  highlights  the  range  of  valuable  \nobjectives\n \nthese\n \nmeasures\n \ncan\n \nsupport.\n \nIn\n \npractice,\n \na\n \nsingle\n \nintervention\n \nmay\n \nserve\n \nmultiple\n \npurposes\n \nat\n \nonce.\n \nMoreover,\n \ninterventions\n \nfrom\n \ndi\ufb00erent\n \ncategories\n \ncan\n \ncomplement\n \none\n \nanother\u2014for\n \ninstance,\n \nestablishing\n \na\n \nliability\n \nregime\n \nbecomes\n \nmore\n \nfeasible\n \nwhen\n \npaired\n \nwith\n \ntechnical\n \ntransparency\n \nmeasures.\n \nHowever,\n \nthere\n \ncan\n \nalso\n \nbe\n \ntrade-o\ufb00s.\n \nFor\n \nexample,\n \npromoting\n \nvisibility\n \nand\n \ncontrol\n \nmay\n \ncon\ufb02ict\n \nwith\n \nmaintaining\n \nsecurity\n \nand\n \nprivacy.\n \nThe  following  sections  will  include  both  details  on  the  intervention  categories  and  provides  \ufb01ctional  \nvignettes\n \nillustrating\n \nhow\n \nthey\n \nmight\n \nbe\n \nimplemented\n \nunder\n \nreal-world\n \nconditions.\n \n5.1  Alignment  \nAlignment  interventions  ensure  that  agent  systems  behave  in  ways  that  are  consistent  with  a  given  \nprincipal\u2019s\n \nvalues,\n \nintentions,\n \nand\n \ninterests\n \n(i.e.,\n \nare\n \naligned\n)\n \nand\n \nalso\n \nestablish\n \ntrust\n \nthat\n \nthese\n \nsystems\n \nare\n \nactually\n \nsu\ufb03ciently\n \naligned.\n \nThis  is  important  because  AI  agents  will  be  able  to  operate  with  high  levels  of  autonomy,  including  \nin\n \nhigh-stakes\n \ndomains\n \nlike\n \n\ufb01nancial\n \nmarkets\n \nor\n \nmilitary\n \noperations\u2014alignment\n \ninterventions\n \nmake\n \nit\n \n      AGENT  GOVERNANCE   \u2502   36   \nmore  likely  that  agents  will  act  consistently  in  line  with  a  principal\u2019s  interests,  even  when  \nunsupervised\n \nor\n \nuncontrolled.\n \nAlignment  interventions  are  likely  to  be  implemented  at  the  model  layer,  particularly  during  training,  \nso\n \nthe\n \ndeveloper\n \nis\n \nmost\n \nlikely\n \nto\n \nbe\n \ninvolved.\n \nWith\n \nopen-source\n \nmodels,\n \nusers\n \nwould\n \nbe\n \nable\n \nto\n \nmodify\n \ninterventions\n \nvia\n \n\ufb01netuning\n \nor\n \nsimilar\n \nmethods.\n \nWhy  is  this  di\ufb00erent  for  agents?  \nWhile  a  variety  of  approaches  to  AI  alignment  are  being  explored,  the  main  approach  being  \nimplemented\n \nby\n \nleading\n \nAI\n \ncompanies\n \non\n \ncommercial\n \nmodels\n \nis\n \n\u2018reinforcement\n \nlearning\n \nfrom\n \nhuman\n \nfeedback\u2019\n \n(RLHF)\n \nor\n \nits\n \nvariations.\n \nRLHF\n \nis\n \na\n \ntraining\n \nmethod\n \nwhere\n \nhuman\n \nevaluators\n \nrate\n \nAI-generated\n \noutputs\n \nbased\n \non\n \nquality\n \nand\n \nalignment\n \nwith\n \nhuman\n \nvalues,\n \nand\n \nthese\n \nratings\n \nare\n \nused\n \nto\n \ncreate\n \na\n \nreward\n \nsignal\n \nthat\n \ntrains\n \nthe\n \nAI\n \nsystem\n \nto\n \nmaximize\n \nthe\n \nlikelihood\n \nof\n \nproducing\n preferred  outputs  while  minimizing  undesired  ones  (Lambert  et  al.  2022;  Bai  et  al.  2022).  \nHowever,  there  are  some  reasons  to  believe  that  these  approaches  will  be  less  e\ufb00ective  at  aligning  \nfuture,\n \nmore\n \npowerful\n \nagents.\n \nThere\n \nis\n \nevidence\n \nthat\n \ntraining\n \nchatbot-style\n \nLLMs\n \nto\n \nrefuse\n \nharmful\n requests  is  no  longer  e\ufb00ective  when  these  same  models  are  deployed  as  agents  (Kumar  et  al.  2024;  Lermen,  Dziemian,  and  Pimpale  2024).  Attacks  such  as  jailbreaking  and  refusal-vector  ablation  seem  to  function  better  on  browser  agents  than  on  base  models,  suggesting  an  underlying  \npattern\n \nwhere\n \nalignment\n \ninterventions\n \nfail\n \nto\n \ngeneralize.\n \nUnlike  chatbot  systems,  agents  are  likely  to  pursue  complex,  longer  time-horizon  tasks  where  it  \nbecomes\n \nmore\n \ndi\ufb03cult\n \nfor\n \nhumans\n \nor\n \nmodels\n \ntrained\n \non\n \nhuman\n \nfeedback\n \nto\n \nevaluate\n \noutputs\n (Leike  et  al.  2018).  As  agents  become  more  generally  capable\u2014e.g.,  through  improved  reasoning  via  the  ability  to  leverage  test-time  compute  and  reinforcement  learning\u2014they  may  also  become  more  capable  of  engaging  in  scheming  or  deceptive  behavior  (Leike  2024).  For  instance,  an  agent  might  learn  to  avoid  triggering  certain  safety  mechanisms  in  order  to  achieve  its  assigned  objectives,  even  if  these  objectives  con\ufb02ict  with  broader  human  intentions  (Meinke  et  al.  2025).  This  type  of  behavior  would  reduce  the  e\ufb03cacy  of  RLHF-type  methods.  \nIn  multi-agent  systems,  even  when  individual  AIs  share  common  objectives,  they  can  still  produce  harmful  collective  outcomes.  Consider  this  real-world  risk:  Hammond  et  al.  (2025) found  that  two  self-driving  cars\u2014both  programmed  for  safe  driving  but  following  di\ufb00erent  strategies  (US  right-hand  \nvs.\n \nIndian\n \nleft-hand\n \nyielding)\u2014crashed\n \nin\n \n77.5%\n \nof\n \nsimulations.\n \nSingle\n \nagent\n \nalignment\n \nalone\n \nis\n \nnot\n \nenough\n \nwhen\n \nagents\n \nhave\n \ncon\ufb02icting\n \nimplementation\n \nstrategies\n \nfor\n \nthe\n \nsame\n \ngoal.\n \n  \n      AGENT  GOVERNANCE   \u2502   37   \nExample  alignment  interventions \nIntervention  De\ufb01nition  \nMulti-agent  reinforcement  learning  \nThis  involves  training  multiple  agents  that  learn  by  interacting  with  a  shared  environment,  each  optimizing  its  own  policy  based  on  individual  rewards  while  their  actions  collectively  shape  the  state  dynamics.  Multi-agent  reinforcement  learning  agents  have  been  shown  to  establish  and  switch  between  group  conventions  in  order  to  better  coordinate  (K\u00f6ster  et  al.  2020).  \nAligning  agent  risk-attitudes  \nTechniques,  such  as  imitation  learning,  that  are  used  to  calibrate  an  agent\u2019s  attitudes  towards  risk  so  that  it  is  aligned  with  the  attitudes  of  users  and/or  the  developers,  in  order  to  guide  a  system\u2019s  decision-making  under  uncertainty  (Clatterbuck,  Castro,  and  Mor\u00e1n  2024).   \nParaphrasing  model  outputs  to  defend  against  encoded  reasoning  \nFor  models  that  use  intermediate  steps  of  reasoning  (i.e.,  chain-of-thought  or  CoT)  to  generate  solutions  to  complex  problems,  they  might  hide  or  encode  their  intermediate  reasoning  in  a  way  that  is  not  transparent  to  humans,  which  would  undermine  a  key  channel  for  checking  if  a  system  is  aligned.  One  set-up  to  defend  against  this  is  to  use  a  defender(another  model)  to  paraphrase  the  CoT  to  remove  hidden  information,  while  keeping  useful  semantic  information  intact  (Roger  and  Greenblatt  2023).  \nAlignment  evaluations  \nEvaluations  and  environments  designed  to  test  how  aligned  agents  are,  whether  their  behaviors  match  with  human  preferences,  goals,  and  values.  This  could  involve  setting  up  environments  to  test  how  they  perform  against  speci\ufb01c  speci\ufb01cation  problems,  such  as  testing  for  reward  tampering  (Leike  et  al.  2017).  \n \n      AGENT  GOVERNANCE   \u2502   38   \nIntervention  in  action:  Aligning  agent  risk-attitudes   John,  a  freelance  graphic  designer,  started  using  \u201cFinanceBuddy,\u201d  an  AI  \ufb01nancial  advisor,  to  help  manage  his  income  and  investments.  Upon  initial  setup,  FinanceBuddy  administered  a  comprehensive  survey  to  gauge  John's  general  risk  tolerance.  The  survey  included  questions  about  his  \ufb01nancial  goals,  investment  experience,  and  comfort  level  with  market  volatility.  John  indicated  a  moderately  conservative  approach,  preferring  steady  growth  over  high-risk,  high-reward  scenarios.  FinanceBuddy  used  this  information  to  calibrate  its  internal  risk  assessment  model  to  align  with  John's  pro\ufb01le.  Later,  when  the  stock  market  experienced  a  period  of  signi\ufb01cant  growth,  FinanceBuddy  identi\ufb01ed  several  promising  but  volatile  investment  opportunities.  However,  instead  of  aggressively  pursuing  these  options,  it  presented  John  with  a  balanced  portfolio,  including  a  mix  of  stable,  low-yield  investments  alongside  a  small  portion  of  carefully  vetted,  higher-growth  stocks.   \n5.2  Control  \nControl  interventions  act  as  external  constraints  on  the  behavior  of  agents  to  ensure  they  operate  \nwithin\n \nprede\ufb01ned\n \nboundaries.\n \nThese\n \nprimarily\n \na\ufb00ect\n \nthe\n \nsystem\n \nand\n \necosystem\n \nlayers.\n \nThis\n \nincludes\n \nlimiting\n \nthe\n \nability\n \nof\n \nagents\n \nto\n \ntake\n \nharmful\n \nactions\n \nby\n \nshifting\n \naccess\n \nto\n \na\ufb00ordances,\n \ni.e.,\n \nresources\n \nlike\n \ntools\n \nand\n \ntest-time\n \ncompute,\n \nor\n \nby\n \ninterruption\n \nand\n \nshutdown.\n \nThis  is  important  because  agents  may  end  up  not  being  su\ufb03ciently  aligned  and  act  in  ways  that  \ncould\n \ncause\n \nharm.\n \nIt\n \nis\n \nalso\n \npossible\n \nthat\n \nan\n \nagent\n \nis\n \na\ufb00ected\n \nby\n \nan\n \nadversarial\n \nattack,\n \ncausing\n \nit\n \nto\n \nmalfunction\n \nand\n \ntake\n \nharmful\n \nactions.\n \nControl\n \ninterventions\n \nact\n \nas\n \na\n \n\u2018safety\n \nnet\u2019,\n \nproviding\n \nthe\n \nmeans\n \nto\n \nmanage\n \nand,\n \nif\n \nneeded,\n \nintervene\n \nin\n \nthe\n \noperation\n \nof\n \nagents.\n \nIf\n \nalignment\n \ninterventions\n \nare\n \nabout\n \naltering\n \nan\n \nagent\u2019s\n \nbehavior\n \nso\n \nthat\n \nit\n \ntends\n \nto\n \nbehave\n \nsafely,\n \ncontrol\n \nattempts\n \nto\n \nestablish\n \nthe\n \n\u2018hard\u2019\n \nboundaries\n \nof\n \nwhat\n \nthey\n \ncan\n \nand\n \ncannot\n \ndo.\n \nControl\n \nmeasures\n \nwould\n \nideally\n \neven\n \nbe\n \nable\n \nto\n \nprevent\n \na\n \n\u2018rogue\u2019\n \nagent\n \nfrom\n \ntaking\n \nharmful\n \nor\n \nundesirable\n \nactions.\n \nControl  interventions  might  fall  primarily  on  the  deployers  of  agents,  who  need  to  integrate  these  \nmechanisms\n \ninto\n \nthe\n \noperational\n \nenvironment,\n \nand\n \nservice\n \nproviders,\n \nwho\n \ncan\n \ncontrol\n \naccess\n \nto\n \ninfrastructure\n \non\n \ntheir\n \nend.\n \nHowever\n \nsome\n \ncontrols\n \nmay\n \nbe\n \ndelegated\n \nto\n \nusers\n \n(i.e.,\n \nhuman-in-the-loop\n \nsetups).\n \n      AGENT  GOVERNANCE   \u2502   39   \nWhy  is  this  di\ufb00erent  for  agents?  \nControl  interventions  are  valuable  for  both  agent  and  non-agent  AI  systems,  but  o\ufb00er  distinct  \nadvantages\n \nfor\n \nagent\n \nsystems\n \nthat\n \ninteract\n \ndirectly\n \nwith\n \nthe\n \nworld.\n \nWhile\n \nmany\n \ncontrol\n \nsafeguards\n \napply\n \nbroadly\n \nacross\n \nAI\n \nsystems,\n \nagents'\n \nability\n \nto\n \ntake\n \ndirect\n \nactions\n \n(e.g.,\n \nmaking\n \npurchases\n \nonline\n \nor\n \nmanaging\n \ncyber-physical\n \nsystems)\n \ncreates\n \nboth\n \nadditional\n \nopportunities\n \nand\n \ngreater\n \nnecessity\n \nfor\n \nrobust\n \nsafeguards.\n \nThe  direct  world  impact  of  agent  systems  increases  the  stakes  of  potential  failures  or  adversarial  \nexploitation,\n \nmaking\n \ne\ufb00ective\n \ninterventions\n \nparticularly\n \ncrucial.\n \nAt\n \nthe\n \nsame\n \ntime,\n \nagents'\n \ninteraction\n \nwith\n \nexternal\n \nsystems\n \nprovides\n \nmore\n \npotential\n \nintervention\n \npoints\n \nthat\n \nare\n \nnot\n \navailable\n \nin\n \nchatbot-style\n \nsystems\n \nWithout  these  interventions,  an  agent  could  cause  unintended  damage  by  performing  unauthorized  \ntransactions,\n \nmiscon\ufb01guring\n \nphysical\n \nsystems,\n \nor\n \notherwise\n \nacting\n \noutside\n \nexpected\n \nparameters.\n \nE\ufb00ective\n \ncontrol\n \nmeasures\n \nnot\n \nonly\n \nhelp\n \nmanage\n \nthe\n \nsystem\u2019s\n \nability\n \nto\n \ntake\n \nactions\n \nbut\n \nalso\n \ncreate\n \na\n \nclear\n \n\u201co\ufb00\n \nswitch\u201d\n \nor\n \npoint\n \nof\n \nhuman\n \nintervention,\n \nensuring\n \nthat\n \nif\n \nmisalignment\n \nor\n \nexploitation\n \noccurs,\n \nharm\n \nremains\n \nlimited\n \nand\n \nmanageable.\n \nExample  control  interventions \nIntervention  De\ufb01nition  \nRollback  infrastructure  \nInfrastructure  that  allows  for  agent  actions  to  be  voided  or  undone,  which  would  be  useful  if  a  malfunction  occurs  (Chan  et  al.  2025).  This  includes  both  the  mechanisms  to  implementing  action  voiding  and  undoing  and  the  interfaces  for  actors  to  request  it.  Theoretically,  this  could  function  like  how  banks  can  void  fraudulent  transactions.    \nShutdown  and  interruption  mechanisms  \nA  technical  measure  to  stop  an  agent  from  operating,  either  temporarily  or  permanently  (Had\ufb01eld-Menell  et  al.  2017).  A  shutdown  mechanism  could  be  designed  to  allow  for  operations  to  cease  in  a  controlled,  orderly  manner,  rather  than  forcing  an  abrupt  termination.  This  might  be  particularly  important  if  agents  are  involved  in  critical  operations,  where  it  is  preferable  for  them  to  safely  complete  or  wind  down  essential  tasks,  or  where  abruptly  shutting  down  one  component  of  a  complex,  interdependent  system  could  cause  cascading  failures.   A  timeout  mechanism  would  stop  the  operation  of  an  agent  after  some  amount  of  time  has  passed  or  number  of  actions  has  been  taken  (e.g.,  API  calls)  (Zittrain  2024).  \n      AGENT  GOVERNANCE   \u2502   40   \nRestricting  speci\ufb01c  agent  actions  and  tools  \nAgent  outputs  could  be  constrained  in  speci\ufb01c  ways  to  reduce  speci\ufb01c  risks.  For  example,  an  agent  could  be  restricted  from  communicating  with  other  agent  instances,  accessing  bank  accounts,  or  having  command  line  access  to  a  user\u2019s  computer.  \nControl  protocols  and  evaluations  \nControl  protocols  are  procedures  that  would  allow  an  AI  agent  to  be  deployed,  even  if  the  agent  would  engage  in  strategic  deceptive  behavior  (Greenblatt,  Shlegeris,  et  al.  2024).  For  example,  using  a  trusted  monitor  to  oversee  an  agent,  with  the  goal  of  \ufb02agging  and  \ufb01ltering  problematic  behavior.  Control  evaluations  are  iterative  processes  of  testing  whether  control  protocols  would  actually  work  in  a  particular  agent  training  and  deployment  set-up.  \n  \nIntervention  in  action:  Shutdown  mechanism\n \n\u201cSentinel\u201d,  an  AI  content  moderator,  monitored  an  online  forum  for  harmful  content.  Unlike  typical  \ufb01ltering  systems,  Sentinel  made  judgment  calls  based  on  evolving  context.  Equipped  with  a  self-evaluation  mechanism,  it  encountered  heated  debate  about  a  new  law  and  began  \ufb02agging  posts  containing  language  patterns  it  associated  with  extremism,  despite  being  legitimate  discourse.  Sentinel's  internal  metrics  detected  a  concerning  pattern  in  its  own  decisions\u2014an  unusual  increase  in  \ufb02agging  actions  without  corresponding  user  reports.  This  triggered  a  \u201cgraceful  shutdown.\u201d  Sentinel  paused  removals,  shifted  to  observation  mode,  and  compiled  a  summary  for  the  human  moderation  team.  The  team  recognized  Sentinel  was  misinterpreting  passionate  debate  and  adjusted  its  parameters.  Sentinel  incorporated  this  feedback  into  its  decision  framework  and,  after  reactivation,  better  distinguished  between  harmful  content  and  vigorous  debate.  This  shutdown  prevented  unintended  censorship  while  allowing  Sentinel  to  re\ufb01ne  its  judgment  criteria  for  future  moderation.  \n5.3  Visibility  \nVisibility  interventions  are  measures  that  make  the  behavior,  capabilities,  and  actions  of  an  agent  \nobservable\n \nand\n \nunderstandable\n \nto\n \nhumans.\n \n \n      AGENT  GOVERNANCE   \u2502   41   \nThis  class  of  interventions  is  important  because  addressing  risks  from  AI  agents  requires  \ninformation\n \nabout\n \nwhere,\n \nwhy,\n \nhow,\n \nand\n \nby\n \nwhom\n \nagents\n \nare\n \nused.\n \nFor\n \nexample,\n \ndeployers\n \ntrying\n \nto\n \nimplement\n \na\n \ncontrol\n \nintervention\n \nwill\n \nneed\n \ninformation\n \nto\n \nimplement\n \nit\n \nin\n \na\n \ntargeted\n \nmanner;\n \nor\n \nregulatory\n \nbodies\n \nwith\n \nmandates\n \nto\n \nmonitor\n \nand\n \nenforce\n \nrules\n \non\n \nagents\n \nmay\n \nrequire\n \ninformation\n \nfor\n \ne\ufb00ective\n \nenforcement.\n \nRegulators\n \nmay\n \nalso\n \nwant\n \nto\n \ntrace\n \nagent\n \ndecisions\n \nand\n \nactions\n \nback\n \nto\n \ntheir\n \nsource,\n \nso\n \nas\n \nto\n \nenable\n \naccountability.\n \nThis\n \ninformation\n \nallows\n \ndevelopers\n \nand\n \noperators\n \nto\n \nunderstand\n \nwhy\n \nan\n \nagent\n \nbehaves\n \na\n \ncertain\n \nway,\n \nmaking\n \nidentifying\n \nand\n \ncorrecting\n \nerrors\n \nor\n \nunintended\n \nbehaviors\n \neasier.\n \nTransparency\n \ninto\n \ncapabilities\n \nalso\n \nallows\n \nstakeholders\n \nto\n \nanticipate\n \npotential\n \nimpacts\n \nof\n \nagent\n \ndeployment\n \nand\n \ngauge\n \nrisk.\n \nDevelopers  can  run  safety  tests  to  gauge  characteristics  like  general  autonomy  capabilities.  Inputs  \nand\n \noutputs\n \nof\n \nagents\n \nare\n \nby\n \ndefault\n \nvisible\n \nto\n \nthe\n \ndeployer\n \n(e.g.,\n \nthey\n \ncan\n \nrun\n \na\n \nmonitoring\n \nsystem\n \non\n \ntop\n \nof\n \ninputs\n \nand\n \noutputs),\n \nthough\n \ndirect\n \naccess\n \nto\n \nuser\n \ndata\n \ncould\n \nbe\n \nlimited\n \ndue\n \nto\n \nprivacy\n \nconsiderations.\n25\n \nAlso,\n \ncertain\n \noutputs,\n \nlike\n \nrequests\n \nto\n \nexternal\n \ntools\n \nand\n \nservices,\n \ncould\n \nbe\n \nmade\n \nvisible\n \nto\n \ntool\n \nand\n \nservice\n \nproviders.\n \nWhy  is  this  di\ufb00erent  for  agents?  \nAgents,  given  their  ability  to  act  autonomously,  may  take  multiple  consequential  actions  in  rapid  \nsuccession\n \nbefore\n \na\n \nhuman\n \nnotices.\n \nGiven\n \nthis,\n \nit\n \nis\n \nlikely\n \nthat\n \ninformation\n \nasymmetries\n \nbetween\n stakeholders  and  deployed  agents  will  be  more  signi\ufb01cant  than  with  chatbot-style  systems  (Kolt  2025).  \nExample  visibility  interventions \nIntervention  De\ufb01nition  \nAgent  IDs  Unique  identi\ufb01ers  for  AI  agents  that  provide  information  about  an  agent,  such  as  its  function,  developer,  behavior  as  gleaned  from  testing,  properties,  and  any  associated  incidents  (Chan,  Kolt,  et  al.  2024).  These  could  be  used  so  the  agent  can  proactively  identify  itself  as  an  AI  system  (as  opposed  to  a  human),  and  also  allow  for  more  comprehensive  tracking.  \nActivity  logging  Records  of  the  speci\ufb01c  inputs  and  outputs  of  an  agent,  which  could  be  from  users,  tools  and  services,  or  interactions  with  other  agents.  These  logs  can  be  used  to  understand  the  impact  of  agents,  identify  potential  problems,  facilitate  incident  investigation,  and  hold  users  accountable  for  their  agents'  actions  (Chan,  Ezell,  et  al.  2024).  The  level  of  detail  recorded  in  logs  can  vary  depending  on  the  risk  associated  with  the  agent's  activity  and  privacy  concerns.  \n25\n Due  to  company  policy  or  compliance  to  privacy  laws  \n      AGENT  GOVERNANCE   \u2502   42   \nCooperation-relevant  capability  evaluations  \nAgents  working  together  are  likely  to  be  able  to  extend  their  capabilities  to  solve  joint  tasks,  both  for  bene\ufb01cial  and  harmful  ends  (Dafoe  et  al.  2020).  Methods  and  metrics  for  evaluating  cooperation-relevant  capabilities  in  agent  systems  would  help  to  assess  how  and  to  what  extent  agents  can  cooperate.  \nReward  reports  Documentation  produced  before  deployment  that  detail  design  decisions  for  agents  with  reinforcement  learning  components.  This  can  include  details  about  types  of  feedback  incorporated,  the  metrics  considered  to  optimize  performance,  and  justi\ufb01cations  for  choices  about  system  elements  like  states,  actions,  and  rewards  (Gilbert  et  al.  2022).  This  can  facilitate  external  oversight  of  agents  and  mitigate  risks  speci\ufb01c  to  RL  systems  like  reward  tampering.  \n \nIntervention  in  action:  Agent  IDs   Sarah,  a  freelance  writer,  received  an  email  seemingly  from  a  potential  client,  requesting  a  sample  article.  The  email  was  well-written  and  persuasive.  However,  before  responding,  Sarah  noticed  a  small  icon  in  the  email  header,  indicating  the  sender  was  an  AI  agent.  Clicking  the  icon  revealed  the  agent's  ID:  \u201cWriteAssist-v2.3-Prospector\u201d,  along  with  information  about  its  function  (marketing  content  generation)  and  its  developer.  This  proactive  identi\ufb01cation  allowed  Sarah  to  understand  she  was  interacting  with  an  AI,  not  a  human  client.  She  realized  the  email  was  likely  a  form  of  automated  outreach  rather  than  a  genuine  personalized  request.  Furthermore,  the  ID  allowed  her  to  research  the  agent  and  its  developer,  understanding  their  reputation  and  typical  behavior.  This  transparency  prevented  Sarah  from  being  misled  and  allowed  her  to  make  an  informed  decision  about  engaging  with  the  agent.   \n5.4  Security  and  robustness  \nSecurity  and  robustness  interventions  are  intended  to  secure  agent  systems  from  various  external  \nthreats,\n \nprotect\n \nthe\n \nintegrity\n \nand\n \ncon\ufb01dentiality\n \nof\n \ndata,\n \nand\n \nensure\n \nreliable\n \nperformance\n \neven\n \nunder\n \nadverse\n \nconditions.\n \nThese  interventions  are  important  because  as  agents  become  more  prevalent  and  powerful,  the  \nconsequences\n \nof\n \nthem\n \nbeing\n \ncompromised\n \nare\n \nhigher.\n \nSecure\n \nagents\n \nare\n \nless\n \nvulnerable\n \nto\n \n      AGENT  GOVERNANCE   \u2502   43   \nhijacking  or  manipulation  by  malicious  actors,  preventing  scenarios  where  AI  could  be  turned  into  a  \nweapon\n \nor\n \ncould\n \nleak\n \nsensitive\n \ninformation.\n \nRobustness\n \nensures\n \nthat\n \nagents\n \ncan\n \nhandle\n \nunexpected\n \nsituations\n \nwithout\n \ncatastrophic\n \nfailures,\n \ncrucial\n \nfor\n \ndeployment\n \nin\n \ncritical\n \nsystems\n \nlike\n \nhealthcare,\n \n\ufb01nance,\n \nor\n \ninfrastructure.\n \nSecurity\n \nbreaches\n \nor\n \nfailures\n \ndue\n \nto\n \nlack\n \nof\n \nrobustness\n \ncould\n \nseverely\n \ndamage\n \npublic\n \ntrust\n \nin\n \nAI\n \nsystems,\n \npotentially\n \nsetting\n \nback\n \nbene\ufb01cial\n \nAI\n \nadoption\n \nsigni\ufb01cantly.\n \nWhy  is  this  di\ufb00erent  for  agents?  \nAs  mentioned  in  3.3  Security  risks,  agent  systems  have  a  larger  attack  surface  than  LLMs  due  to  their  integration  with  other  tools  and  interfaces  (Wu  et  al.  2025) .  For  example,  threat  actors  could  hide  malicious  content  in  websites  to  hijack  AI  agents  (Debenedetti  et  al.  2024).   \nAlso,  given  agents\u2019  autonomy  and  ability  to  operate  for  extended  periods,  they  are  more  likely  to  \nrun\n \ninto\n \nnovel\n \nand\n \ndynamic\n \nenvironments,\n \nincreasing\n \nthe\n \nchance\n \nthey\n \nencounter\n \na\n \nsituation\n \ndissimilar\n \nto\n \nthose\n \nthey\n \nwere\n \ntrained\n \non.\n \nThis\n \ncould\n \ncause\n \ntheir\n \nperformance\n \nto\n \ndegrade\n \nor\n \nprompt\n \nunintended\n \nbehaviors.\n \nExample  security  and  robustness  interventions \nIntervention  De\ufb01nition  \nAccess  control  Access  to  agents  can  be  managed  so  that  only  authorized  users  with  appropriate  authentication  can  provide  instructions.  There  are  many  possible  arrangements  for  this,  depending  on  targeted  uses  of  a  system.  For  instance,  there  could  be  a  time-based  di\ufb00erential  access  set-up  for  agents  with  signi\ufb01cant  zero-day  vulnerability  discovery  capabilities,  so  that  authorized  users  like  cybersecurity  vendors  and  critical  infrastructure  providers,  can  deploy  these  agents  to  shore  up  defenses  before  access  is  granted  more  widely.  Other  set-ups  could  involve  providing  structured  access  to  inspect  model  internals,  to  allow  third-party  mechanistic  interpretability  research  or  permanent  blacklists  for  users  that  have  previously  violated  terms  of  service  agreements  (Bucknall  and  Trager  2023).  \nAdversarial  robustness  testing  \nMethods  to  systematically  evaluate  an  agent\u2019s  adversarial  robustness,  the  ability  of  underlying  models  to  maintain  its  performance  when  faced  with  specially  crafted  inputs  designed  to  exploit  a  model\u2019s  vulnerabilities  (Wu  et  al.  2025).   \nSandboxing  Secure,  isolate  environments  where  AI  agents  operate  with  restricted  permissions  and  monitored  boundaries.  These  environments  prevent  unauthorized  data  access  or  transmission  by  validating  all  inputs  and  \n      AGENT  GOVERNANCE   \u2502   44   \noutputs  before  they  cross  system  boundaries.  Sandboxes  can  be  used  for  pre-deployment  testing  and  also  to  safeguard  deployed  systems  against  prompt  injection,  data  ex\ufb01ltration,  and  other  security  vulnerabilities.  For  testing,  they  enable  safe  simulation  of  agent-agent,  agent-human,  and  agent-tool  interactions  (X.  Zhou  et  al.  2024).  \nRapid  response  for  adaptive  defense  \nTechniques  to  block  whole  classes  of  jailbreaks  and  exploits  a\ufb00ecting  agents  after  observing  only  a  small  number  of  attacks,  for  example  \ufb01ne-tuning  an  input  classi\ufb01er,  a  model  that  checks  inputs  based  on  speci\ufb01c  criteria,  to  block  additional  similar  jailbreak  attempts  (Peng  et  al.  2024).  \n \nIntervention  in  action:  Access  controls   \u201cZer0\u201d,  a  cutting-edge  AI  agent  designed  to  identify  zero-day  vulnerabilities,  was  a  highly  valuable  and  potentially  dangerous  tool.  Access  to  Zer0  was  strictly  controlled  through  a  tiered  system.  Initially,  only  a  small  group  of  vetted  cybersecurity  experts  from  certi\ufb01ed  organizations  had  access,  allowing  them  to  use  the  AI  to  bolster  defenses  of  critical  infrastructure.  After  a  pre-determined  period,  during  which  patches  were  developed  and  deployed,  access  was  expanded  to  a  wider  group  of  security  researchers  under  strict  usage  agreements,  allowing  them  to  further  analyze  the  agents  capabilities  and  limitations  in  a  controlled  manner.  Finally,  after  another  set  period  of  time  and  testing,  a  limited  version  of  the  agent,  with  certain  functionalities  disabled,  was  made  available  to  the  general  public  for  educational  purposes.  This  time-based  di\ufb00erential  access  system  ensured  that  Zer0's  capabilities  were  used  responsibly,  prioritizing  the  security  of  critical  systems  before  broader  release.  This  minimized  the  risk  of  malicious  actors  exploiting  the  AI's  abilities  before  defenses  were  in  place.  \n5.5  Societal  integration  \nMeasures  intended  to  support  long-term  integration  of  agents  into  existing  social,  political,  and  \neconomic\n \nsystems\u2014addressing\n \nissues\n \nsuch\n \nas\n \ninequality,\n \nconcentration\n \nof\n \npower,\n \nand\n \nestablishing\n \naccountability\n \nstructures.\n  Societal  integration  interventions  will  often  involve  leveraging  institutional  or  legal  mechanisms,  such  \nas\n \nlaws,\n \nstandards,\n \nand\n \nindustry\n \nbest\n \npractice.\n \nThis\n \nis\n \nimportant\n \nbecause\n \ntechnical\n \nsolutions\n \nalone\n \n      AGENT  GOVERNANCE   \u2502   45   \nare  unlikely  to  mitigate  risks  from  agent  systems  fully.  For  example,  technical  measures  do  not  \ninherently\n \nestablish\n \nwho\n \nis\n \nresponsible\n \nif\n \nthings\n \ngo\n \nwrong,\n \nand\n \nwould\n \nnot\n \nnecessarily\n \nincentivize\n \nstakeholders\n \nto\n \ntake\n \nownership\n \nover\n \nAI\n \nagent\n \noutcomes\n \nand\n \nactions.\n \nAlso,  widespread  usage  of  agents  is  likely  to  lead  to  broad  changes  to  social,  political,  and  \neconomic\n \nstructures,\n \ngiven\n \ntheir\n \nability\n \nto\n \nautonomously\n \na\ufb00ect\n \nthe\n \nworld.\n \nSome\n \nof\n \nthe\n \nharms\n \narising\n \nfrom\n \nthese\n \nsystems\n \ncould\n \nbe\n \ndelayed\n \nor\n \nmore\n \nsubtle,\n \nmuch\n \nlike\n \npurported\n \nnegative\n \nmental\n impacts  of  the  widespread  use  of  social  media  health  (Jabbari  et  al.  2017) interventions  to  promote  safe  integration  of  agents  will  need  to  account  for  the  sociotechnical  attributes  of  these  systems  \nand\n \nattempt\n \nto\n \nanticipate\n \ntheir\n \nlonger-run\n \nimpacts.\n \nWhy  is  this  di\ufb00erent  for  agents?  \nIf  agents  are  able  to  act  without  human  oversight,  then  this  creates  challenges  related  to  \naccountability\n \nand\n \nliability.\n \nCertain\n \nareas\n \nof\n \nlaw,\n \nsuch\n \nas\n \ntort\n \nlaw\n \nand\n \nagency\n \nlaw,\n \nwill\n \nhave\n \nsome\n applicability  to  cases  involving  AI  agents  (Toner  et  al.  2024).  However,  our  current  legal  system  depends  on  foreseeability  of  an  action,  which  may  not  reasonably  apply  to  cases  where  agents  behave  contrary  to  user  or  developer  intent  (Had\ufb01eld  2024).  Also,  unlike  humans,  AI  agents  are  not  inherently  deterred  by  personal  liability  and  punishment,  so  may  be  more  inclined  to  take  risky  actions  (Weil  2024).  \nExample  societal  integration  interventions \nIntervention  De\ufb01nition  \nLiability  regimes  for  AI  agents  \nThe  development  and  establishment  of  a  liability  regime  for  AI  agents\u2014determining  how  to  allocate  liability  among  the  various  stakeholders  involved  in  designing,  deploying,  and  using  agent  systems  (Kolt  2025).   \nCommitment  devices  Mechanisms  that  allow  agents  to  enforce  commitments,  similar  to  how  existing  devices  work  for  humans  today  like  legal  contracts  and  escrow  payments  (Chan  et  al.  2025).  These  could  be  software-based,  such  as  smart  contracts  and  could  help  AI  agents  cooperate  in  multi-agent  settings  involving  AIs  and  humans.  \nEquitable  agent  access  schemes   \nAn  institutional  arrangement  that  ensures  broad,  potentially  universal  access  to  agents  and  agent-provided  services.  In  futures  where  agents  take  on  an  increasingly  large  share  of  economically  valuable  work,  then  guaranteeing  agent  access  could  enhance  production  possibilities  at  the  individual  level  and  reduce  centralization  of  wealth.  \n      AGENT  GOVERNANCE   \u2502   46   \nDeveloping  law-following  AI  agents  \nLaw-following  AI  agents  is  a  speci\ufb01c  implementation  of  alignment  work  that  involves  aligning  an  AI  system  to  a  speci\ufb01c  set  of  laws,  rather  than  developer  or  user-chosen  values.  This  includes  regulation  that  requires  AI  agents  to  be  designed  this  way,  as  well  as  the  technical  work  needed  to  implement  this  requirement  and  verify  that  agents  meet  this  standard(Institute  for  Law  &  AI  2024).  The  default  path  may  be  to  develop  agents  that  are  only  loyal  to  their  users  and  view  laws  merely  as  obstacles  to  work  around.  Law-following  agents  could  allow  for  more  scalable  enforcement  of  agent  behavior  and  help  ensure  that  agent  behavior  is  aligned  with  democratically  enacted  laws,  and  not  only  values  chosen  by  AI  developers.  \n  \nIntervention  in  action:  Liability  regime  for  AI  agents   AdGent,  an  AI  agent  designed  to  create  and  place  targeted  online  advertisements,  was  used  by  a  marketing  \ufb01rm  to  promote  a  new  \ufb01nancial  product.  AdGent  autonomously  generated  ads  and  selected  websites  for  placement,  optimizing  for  engagement.  However,  due  to  an  unforeseen  \ufb02aw  in  its  algorithm,  it  began  placing  ads  on  websites  containing  extremist  content,  inadvertently  associating  the  \ufb01nancial  product  with  harmful  ideologies.  The  \ufb02aw  stemmed  from  AdGent's  overarching  focus  on  engagement  metrics  without  considering  broader  contextual  factors  like  brand  safety  and  social  responsibility,  leading  it  to  favor  high-engagement  extremist  content  over  more  appropriate  placement  options.  This  caused  reputational  damage  to  the  \ufb01nancial  company  and  sparked  public  outrage.  A  well-de\ufb01ned  liability  process  for  AI  agents  was  in  place  to  address  such  situations.  The  investigation,  guided  by  this  process,  analyzed  the  actions  of  the  marketing  \ufb01rm,  the  AdGent  developer,  and  the  \ufb01nancial  company.  It  determined  that  while  the  marketing  \ufb01rm  had  provided  appropriate  guidelines  and  the  \ufb01nancial  company  had  approved  the  general  campaign  strategy,  the  developer  of  AdGent  had  failed  to  adequately  test  the  agent  for  potential  biases  and  unintended  associations.  While  the  developer  did  not  intend  the  speci\ufb01c  actions  the  agent  took,  the  liability  regime  still  held  the  developer  primarily  responsible  in  this  case.  Consequently,  the  AdGent  developer  had  to  compensate  the  \ufb01nancial  company  for  reputational  harm.  \n \n      AGENT  GOVERNANCE   \u2502   47   \n \n6.  Conclusion \n\u201cWe  are  now  con\ufb01dent  we  know  how  to  build  AGI  as  we  have  traditionally  understood  it.  We  believe  that,  in  2025,  we  may  see  the  \ufb01rst  AI  agents  join  the  workforce  and  materially  change  the  output  of  companies.\u201d \n \n(Altman  2025\n)\n \n \nAI  agents  have  long  been  part  of  the  promise  of  AI.  From  early  attempts  at  autonomous  systems  in  \nthe\n \n1960s\n \nto\n \nthe\n \nmore\n \nrecent\n \nemergence\n \nof\n \nsoftware\n \nagents\n \npowered\n \nby\n \nfoundation\n \nmodels,\n \nthe\n \naspiration\n \nhas\n \nalways\n \nbeen\n \nto\n \ncreate\n \nmachines\n \nthat\n \ncan\n \nthink,\n \nplan,\n \nand\n \nact\n \nautonomously.\n \nToday,\n \nwe\n \nmay\n \nbe\n \njust\n \nmonths\n \nor\n \nyears\n \nfrom\n \nseeing\n \nthe\n \n\ufb01rst\n \nwave\n \nof\n \ncapable\n \nagents\n \nhit\n \nthe\n \nmass\n \nmarket.\n \nThe  widespread  deployment  of  agents  has  the  potential  to  radically  shift  the  structure  of  \ngovernment,\n \nsociety,\n \nand\n \nthe\n \neconomy.\n \nHowever,\n \nas\n \nthis\n \nguide\n \nhighlights,\n \nthe\n \nopportunities\n \nthese\n \nsystems\n \npresent\n \nare\n \ninseparable\n \nfrom\n \nsigni\ufb01cant\n \nrisks,\n \nincluding\n \nloss\n \nof\n \ncontrol,\n \nmalicious\n \nuse,\n \nsystemic\n \nvulnerabilities,\n \nand\n \nexacerbation\n \nof\n \ninequality.\n \nBefore  we  enter  this  new  territory,  we  need  to  answer  important  questions  around  how  to  govern  \nagents\n \nso\n \nthat\n \nthey\n \ncan\n \nbe\n \ndeveloped\n \nand\n \ndeployed\n \nsafely.\n \nAgent\n \ngovernance,\n \nas\n \na\n \n\ufb01eld,\n \nsets\n \nout\n \nto\n \nfoster\n \na\n \nworld\n \nwhere\n \nagents\n \ncan\n \namplify\n \nour\n \npotential\n \nwithout\n \ncreating\n \nundue\n \nrisk.\n \nAs\n \nhighlighted\n in  Section  4,  there  are  many  important  open  questions  in  this  space  that  demand  urgent  attention:  \n      AGENT  GOVERNANCE   \u2502   48   \n\u25cf  How  to  e\ufb00ectively  evaluate  agent  performance  and  risks  over  time,  particularly  as  agents  \ngrow\n \nmore\n \nautonomous\n \nand\n \ncomplex?\n \u25cf  What  interventions\u2014technical,  legal,  and  policy-based\u2014should  we  employ  to  ensure  \nagents\n \noperate\n \nsafely\n \nand\n \ntransparently,\n \nwhile\n \nfostering\n \naccountability?\n \u25cf  How  can  the  bene\ufb01ts  of  agent  technology  be  distributed  equitably?  \u25cf  What  are  the  risks  of  systemic  political  and  economic  impacts  from  widespread  agent  \nadoption?\n \u25cf  How  do  existing  policy  and  legal  frameworks  need  to  change  to  adapt  to  a  world  with  mass  \ndeployment\n \nof\n \nadvanced\n \nagent\n \nsystems?\n \u25cf  What  role  should  agents  themselves  play  in  governance?  \nAddressing  these  questions  will  require  thoughtful,  coordinated  e\ufb00orts  from  researchers,  \npolicymakers,\n \ntechnologists,\n \nand\n \nsociety\n \nat\n \nlarge.\n \nThe\n \nanswers\n \nwe\n \ncraft\n \ncould\n \nset\n \nthe\n \nfoundation\n \nfor\n \nhow\n \nhumanity\n \ncoexists\n \nwith\n \nintelligent\n \nsystems\n \nin\n \nthe\n \ndecades\n \nto\n \ncome.\n \n      AGENT  GOVERNANCE   \u2502   49   \nAcknowledgements  \nWe  are  grateful  to  the  following  people  for  providing  valuable  feedback  and  insights:   \nAlan  Chan,  Cullen  O\u2019Keefe,  Shaun  Ee,  Ollie  Stephenson,  Chris  Covino,  Cristina  Schmidt-Ib\u00e1\u00f1ez,  \nClara\n \nLangevin,\n \nand\n \nMatthew\n \nBurtell.\n \nParticipation\n \nin\n \nthis\n \nresearch\n \ndoes\n \nnot\n \nnecessarily\n \nimply\n \nendorsement\n \nof\n \nthis\n \nreport\n \nor\n \nits\n \n\ufb01ndings,\n \nand\n \nthe\n \nviews\n \nexpressed\n \nby\n \nthese\n \nindividuals\n \ndo\n \nnot\n \nnecessarily\n \nre\ufb02ect\n \nthose\n \nof\n \ntheir\n \nrespective\n \norganizations.\n \nAll\n \nremaining\n \nerrors\n \nare\n \nour\n \nown.\n  \n \n      AGENT  GOVERNANCE   \u2502   50   \nAppendix  \n \nTable  4:  Longer  summary  of  agent  performance  on  various  benchmarks  \nrepresenting\n \nreal-world\n \ntasks\n26\n \nAgent  benchmarks  \nDescription  Performance  \nGAIA:  General  AI  Assistants  (Mialon  et  al.  2023) \nGAIA  includes  questions  that  cover  real-world  assistant  use  cases  such  as  daily  personal  tasks,  science,  and  general  knowledge.  They  require  an  agent  to  browse  the  open  web,  handle  multi-modality,  code,  read  diverse  \ufb01le  types,  and  reason  over  multiple  steps  to  arrive  at  a  correct  answer.    \nHuman  respondents  obtain  higher  accuracy  on  answers:  92%  vs.  15%  for  GPT-4  equipped  with  plugins.   GPT-4  with  plugins  and  other  LLM  systems  could  not  get  the  correct  answer  for  any  \u2018Level  3\u2019  questions,  which  require  taking  arbitrarily  long  sequences  of  actions  using  any  number  of  tools.  \nAutonomy  Capability  Evals  (METR  2024) \nA  suite  of  automatically  scored  tasks  measuring  various  skills,  including  cybersecurity,  software  engineering,  and  machine  learning.  This  suite  was  run  on  simple  baseline  LM  agents  (3.5  Sonnet  and  GPT-4o),  and  task  completion  accuracy  and  speed  were  compared  against  human  baseliners.  \nAgents  achieve  performance  comparable  to  human  baseliners  at  tasks  that  take  around  30  minutes  to  complete.  Beyond  that ,  agents  could  only  complete  a  small  fraction  of  tasks  that  would  take  a  human  1+  hours  to  complete  (<20%).  \nRE-Bench  (Wijk  et  al.  2024) \nA  benchmark  for  evaluating  the  AI  agents'  ability  to  automate  the  work  of  experienced  AI  R&D  researchers.  It  consists  of  7  challenging,  open-ended  ML  research  engineering  environments  and  data  from  71  8-hour  attempts  by  61  distinct  human  experts .  \nAI  agents  performed  better  than  human  experts  when  both  were  given  a  total  time  budget  of  two  hours  per  environment,  achieving  a  score  four  times  higher.  But,  humans  currently  display  better  returns  to  increasing  time  budgets,  narrowly  exceeding  the  top  AI  agent  scores  given  an  8-hour  budget  and  achieving  2\u00d7  the  score  of  the  top  AI  agent  when  both  are  given  32  total  hours.   Current  AI  agents  often  struggle  to  respond  to  surprising  evidence  or  \n26\n Results  were  compiled  in  December  2025.  Since  then,  Anthropic  released  Claude  Sonnet  3.7,  which  has  \nreportedly\n \nachieved\n \nSOTA\n \nscores\n \non\n \na\n \nnumber\n \nof\n \nagentic\n \nbenchmarks,\n \nincluding\n \nSWE-bench\n \nVeri\ufb01ed\n \n(70.3%\n \nwith\n \ncustom\n \nsca\ufb00olding)\n \nand\n \nTAU-bench\n \n(81.2%). \n      AGENT  GOVERNANCE   \u2502   51   \nexplore  approaches  beyond  the  most  generic  option .  \nCyBench  (Zhang  et  al.  2024) \nA  benchmark  for  evaluating  the  ability  of  agents  to  accomplish  cybersecurity  tasks.  This  suite  consists  of  real-world,  professional-level  Capture  the  Flag  challenges  spanning  six  categories:  cryptography,  web  security,  reverse  engineering,  forensics,  exploitation,  and  miscellaneous  skills.   \nWithout  guidance,  current  agents  struggle  to  solve  CTF  tasks  that  take  human  teams  more  than  11  minutes  to  complete  despite  achieving  success  on  tasks  with  shorter  human  solve  times.  \nSWE-bench  Veri\ufb01ed  (OpenAI  2024a) and  multimodal  (Yang  et  al.  2024) \nAn  evaluation  framework  consisting  of  software  engineering  problems  drawn  from  real  GitHub  issues,  such  as  bug  reports.  Resolving  these  problems  often  requires  processing  long  contexts,  performing  complex  reasoning,  and  coordinating  changes  across  multiple  functions  and  \ufb01les  simultaneously.    The  Veri\ufb01ed  version  of  this  benchmark  contains  a  subset  of  questions  veri\ufb01ed  as  non-problematic  by  human  annotators.   The  multimodal  test  set  contains  visual  software  engineering  tasks  requiring  multimodal  problem-solving  capabilities,  e.g.,  UI  glitches,  data  visualization  bugs,  etc.  \nGPT-4o  resolved  33.2%  of  problems,  using  the  (at  the  time)  performing  open-source  sca\ufb00old,  Agentless.   Another  agent  sca\ufb00old,  OpenHands,  using  Sonnet  3.5,  was  able  to  resolve  53%  of  problems\u2014though  it  employs  multi-agent  delegation  as  part  of  its  platform.    Agent  performance  decreased  considerably  for  problems  that  took  a  human  1+  hour  to  resolve,  going  from  20.8%  to  4.8%  (for  tasks  taking  1-4  hours  for  humans)  and  0%  (tasks  taking  >4  hours  for  humans).   Performance  on  multimodal  problems  was  relatively  worse,  with  top-performing  GPT  4o  and  Claude  Sonnet  3.5-based  agents  only  able  to  resolve  12.2%  of  problems  in  the  test  set.  \nCORE-bench  (Siegel  et  al.  2024)  \nThis  benchmark  evaluates  the  ability  of  agents  to  reproduce  the  computational  results  of  research  papers  automatically.  COREbench  utilizes  90  papers  from  CodeOcean  and  features  three  di\ufb03culty  levels,  each  requiring  agents  to  perform  di\ufb00erent  tasks,  such  as  information  retrieval  and  code  execution.  \nEven  the  best-performing  agent  achieved  only  21%  accuracy  on  the  most  complicated  tasks,  suggesting  that  automating  computational  reproducibility  using  current  agents  remains  a  signi\ufb01cant  challenge.  \nOSWorld  (Xie  et  al.  2024) \nA  benchmark  for  multimodal  agents  that  support  task  setup,  execution-based  evaluation,  and  \nCurrent  LLMs  and  VLMs  are  far  from  capable  of  serving  as  computer  assistants.  Even  with  the  strongest  \n      AGENT  GOVERNANCE   \u2502   52   \ninteractive  learning  across  various  operating  systems,  including  Ubuntu,  Windows,  and  macOS.  It  includes  369  real-world  computer  tasks  derived  from  real  user  experiences.  \nVLMs,  success  rates  remain  low,  ranging  from  0.99%  to  12.24%,  signi\ufb01cantly  below  human-level  performance,  which  averages  around  72% .  VLM-based  agents  struggle  to  ground  on  screenshots,  tend  to  predict  repetitive  actions,  have  di\ufb03culty  handling  noise  from  unexpected  windows,  and  exhibit  limited  knowledge  of  basic  GUI  interactions.  \nWebArena  (S.  Zhou  et  al.  2024) \nThis  benchmark  assesses  the  performance  of  AI  agents  in  solving  tasks  using  various  websites.  It  evaluates  how  well  agents  can  navigate  and  extract  information  from  the  web.  However,  it  has  been  criticized  for  allowing  agents  to  over\ufb01t  to  speci\ufb01c  tasks  due  to  shortcuts  in  the  training  data.  \nThe  best-performing  GPT-4  agent  achieved  an  end-to-end  task  success  rate  of  only  14.41%,  while  human  performance  was  78.24%.  This  suggests  that  current  LLMs  lack  crucial  capabilities  such  as  active  exploration  and  failure  recovery,  which  are  needed  to  perform  complex,  web-based  tasks  successfully.  \nWindows  Agent  Arena  (WAA)  (Bonatti  et  al.  2024)  \nAn  adapted  version  of  the  OSWorld  benchmark  focusing  on  Windows  OS\u2014involving  150+  diverse  tasks  requiring  agent  abilities  in  planning,  screen  understanding,  and  tool  usage.   \nGeneralist  zero-shot  VLM  agents  are  still  far  from  human  performance.  The  best  agent  achieved  a  success  rate  of  19.5%,  compared  to  74.5%  for  humans.  Agent  performance  was  particularly  weak  in  tasks  requiring  keyboard  shortcuts  and  icon  recognition,  suggesting  limitations  in  visual-language  alignment  and  understanding  of  GUI  elements  \nTAU-bench  (Yao  et  al.  2024)  \nA  benchmark  that  emulates  dynamic  conversations  between  a  user  (simulated  by  language  models)  and  a  language  agent.  It  is  designed  to  measure  the  agent's  ability  to  interact  with  users,  utilize  domain-speci\ufb01c  APIs,  and  follow  policies  consistently.  \nGPT-4  struggled  to  achieve  high  success  rates,  particularly  on  tasks  requiring  multi-turn  interactions  and  adherence  to  domain-speci\ufb01c  rules.  GPT-4  achieved  a  success  rate  of  less  than  50%,  with  even  lower  consistency  over  multiple  trials  (pass^8  <  25%  in  the  retail  domain)  \nBALROG  (Paglieri  et  al.  2024) \nA  benchmark  and  framework  designed  to  evaluate  the  agentic  capabilities  of  LLMs  and  VLMs  in  complex,  dynamic,  long-horizon  game  environments.  It  includes  six  reinforcement  learning  environments:  BabyAI,  Crafter,  \nAgents  showed  signi\ufb01cant  limitations  in  current  models,  especially  in  vision-based  decision-making  and  long-term  planning,  highlighting  large  gaps  between  their  performance  and  human-level  capabilities.  \n      AGENT  GOVERNANCE   \u2502   53   \nTextWorld,  Baba  Is  AI,  MiniHack,  and  NetHack.  These  environments  test  skills  such  as  long-term  planning,  spatial  reasoning,  and  the  ability  to  deduce  environmental  mechanics.  \n Bibliography  \nAguirre,  Anthony,  Gaia  Dempsey,  Harry  Surden,  and  Peter  B.  Reiner.  2020.  \u201cAI  Loyalty:  A  New  \nParadigm\n \nfor\n \nAligning\n \nStakeholder\n \nInterests.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2003.11157.   AI  Digest.  2024.  \u201cBeyond  Chat:  AI  Agent  Demo.\u201d  2024.  https://theaidigest.org/agent.   Altman,  Sam.  2025.  \u201cRe\ufb02ections.\u201d  January  6,  2025.  https://blog.samaltman.com/re\ufb02ections.   Amodei,  Dario.  2024.  \u201cMachines  of  Loving  Grace.\u201d  October  11,  2024.  https://darioamodei.com/machines-of-loving-grace.   Andriushchenko,  Maksym,  Alexandra  Souly,  Mateusz  Dziemian,  Derek  Duenas,  Maxwell  Lin,  Justin  \nWang,\n \nDan\n \nHendrycks,\n \net\n \nal.\n \n2024.\n \n\u201cAgentHarm:\n \nA\n \nBenchmark\n \nfor\n \nMeasuring\n \nHarmfulness\n of  LLM  Agents.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2410.09024.   Anthropic.  2023.  \u201cResponsible  Scaling  Policy.\u201d  September  19,  2023.  https://www.anthropic.com/news/anthropics-responsible-scaling-policy.   \u2014\u2014\u2014.  2024.  \u201cBuilding  E\ufb00ective  Agents.\u201d  December  19,  2024.  https://www.anthropic.com/engineering/building-e\ufb00ective-agents.   \u2014\u2014\u2014.  2025.  \u201cClaude  3.7  Sonnet  and  Claude  Code.\u201d  February  24,  2025.  https://www.anthropic.com/news/claude-3-7-sonnet.   Bai,  Yuntao,  Saurav  Kadavath,  Sandipan  Kundu,  Amanda  Askell,  Jackson  Kernion,  Andy  Jones,  \nAnna\n \nChen,\n \net\n \nal.\n \n2022.\n \n\u201cConstitutional\n \nAI:\n \nHarmlessness\n \nfrom\n \nAI\n \nFeedback.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2212.08073.   Batalis,  Steph.  2023.  \u201cAI  and  Biorisk:  An  Explainer.\u201d  Center  for  Security  and  Emerging  Technology.  https://cset.georgetown.edu/publication/ai-and-biorisk-an-explainer/.   Beek,  Kristina.  2024.  \u201cCyber  Sta\ufb03ng  Shortages  Remain  CISOs\u2019  Biggest  Challenge.\u201d  Dark  Reading.  \nSeptember\n \n10,\n \n2024.\n https://www.darkreading.com/cybersecurity-operations/cyber-sta\ufb03ng-shortages-remain-cisos-biggest-challenge.   Belanger,  Ashley.  2024.  \u201cAir  Canada  Must  Honor  Refund  Policy  Invented  by  Airline\u2019s  Chatbot.\u201d  Ars  \nTechnica\n,\n \nFebruary\n \n16,\n \n2024.\n https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/.   Benthall,  Sebastian,  and  David  Shekman.  2023.  \u201cDesigning  Fiduciary  Arti\ufb01cial  Intelligence.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2308.02435.   Bonatti,  Rogerio,  Dan  Zhao,  Francesco  Bonacci,  Dillon  Dupont,  Sara  Abdali,  Yinheng  Li,  Yadong  \nLu,\n \net\n \nal.\n \n2024.\n \n\u201cWindows\n \nAgent\n \nArena:\n \nEvaluating\n \nMulti-Modal\n \nOS\n \nAgents\n \nat\n \nScale.\u201d\n \narXiv.\n \n      AGENT  GOVERNANCE   \u2502   54   \nhttps://doi.org/10.48550/arXiv.2409.08264.   Bucknall,  Benjamin  S,  and  Robert  F  Trager.  2023.  \u201cStructured  Access  for  Third-Party  Research  on  \nFrontier\n \nAI\n \nModels:\n \nInvestigating\n \nResearchers\u2019\n \nModel\n \nAccess\n \nRequirements.\u201d\n \nOxford\n \nMartin\n \nSchool.\n https://www.oxfordmartin.ox.ac.uk/publications/structured-access-for-third-party-research-on-frontier-ai-models-investigating-researchers-model-access-requirements.   Burgan,  Cate.  2024.  \u201cCISA  Pilot  Finds  AI  Impact  in  Federal  Cyber  Ops  \u2018Negligible.\u2019\u201d  MeriTalk.  July  \n30,\n \n2024.\n https://www.meritalk.com/articles/cisa-pilot-\ufb01nds-ai-impact-in-federal-cyber-ops-negligible/.   Chan,  Alan,  Carson  Ezell,  Max  Kaufmann,  Kevin  Wei,  Lewis  Hammond,  Herbie  Bradley,  Emma  \nBluemke,\n \net\n \nal.\n \n2024.\n \n\u201cVisibility\n \ninto\n \nAI\n \nAgents.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2401.13138.   Chan,  Alan,  Noam  Kolt,  Peter  Wills,  Usman  Anwar,  Christian  Schroeder  de  Witt,  Nitarshan  \nRajkumar,\n \nLewis\n \nHammond,\n \nDavid\n \nKrueger,\n \nLennart\n \nHeim,\n \nand\n \nMarkus\n \nAnderljung.\n \n2024.\n \u201cIDs  for  AI  Systems.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2406.12137.   Chan,  Alan,  Rebecca  Salganik,  Alva  Markelius,  Chris  Pang,  Nitarshan  Rajkumar,  Dmitrii  \nKrasheninnikov,\n \nLauro\n \nLangosco,\n \net\n \nal.\n \n2023.\n \n\u201cHarms\n \nfrom\n \nIncreasingly\n \nAgentic\n \nAlgorithmic\n Systems.\u201d  arXiv.Org.  February  20,  2023.  https://doi.org/10.1145/3593013.3594033.   Chan,  Alan,  Kevin  Wei,  Sihao  Huang,  Nitarshan  Rajkumar,  Elija  Perrier,  Seth  Lazar,  Gillian  K.  \nHad\ufb01eld,\n \nand\n \nMarkus\n \nAnderljung.\n \n2025.\n \n\u201cInfrastructure\n \nfor\n \nAI\n \nAgents.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2501.10114.   Clancy,  Matt,  and  Tamay  Besiroglu.  2023.  \u201cThe  Great  In\ufb02ection?  A  Debate  About  AI  and  Explosive  \nGrowth.\u201d\n \nAsterisk\n \nMagazine.\n \nJune\n \n2023.\n https://asteriskmag.com/issues/03/the-great-in\ufb02ection-a-debate-about-ai-and-explosive-growth.   Clatterbuck,  Hayley,  Clinton  Castro,  and  Arvo  Mu\u00f1oz  Mor\u00e1n.  2024.  \u201cRisk  Alignment  in  Agentic  AI  Systems.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2410.01927.   Clymer,  Josh,  Hjalmar  Wijk,  and  Beth  Barnes.  2024.  \u201cThe  Rogue  Replication  Threat  Model.\u201d  METR  \nBlog.\n \nNovember\n \n12,\n \n2024.\n https://metr.org/blog/2024-11-12-rogue-replication-threat-model/.   Cohen,  Stav,  Ron  Bitton,  and  Ben  Nassi.  2025.  \u201cHere  Comes  The  AI  Worm:  Unleashing  Zero-Click  \nWorms\n \nThat\n \nTarget\n \nGenAI-Powered\n \nApplications.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2403.02817.   Cooperative  AI  Foundation.  2025.  \u201cResearch  Grants.\u201d  2025.  https://www.cooperativeai.com/grants/2025.   Critch,  Andrew,  and  Stuart  Russell.  2023.  \u201cTASRA:  A  Taxonomy  and  Analysis  of  Societal-Scale  Risks  from  AI.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2306.06924.   Dafoe,  Allan,  Edward  Hughes,  Yoram  Bachrach,  Tantum  Collins,  Kevin  R.  McKee,  Joel  Z.  Leibo,  \nKate\n \nLarson,\n \nand\n \nThore\n \nGraepel.\n \n2020.\n \n\u201cOpen\n \nProblems\n \nin\n \nCooperative\n \nAI.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2012.08630.   David,  Emilia.  2024.  \u201cAWS  Brings  Multi-Agent  Orchestration  to  Bedrock.\u201d  VentureBeat ,  December  3,  2024.  https://venturebeat.com/ai/aws-brings-multi-agent-orchestration-to-bedrock/.   Debenedetti,  Edoardo,  Jie  Zhang,  Mislav  Balunovi\u0107,  Luca  Beurer-Kellner,  Marc  Fischer,  and  Florian  \nTram\u00e8r.\n \n2024.\n \n\u201cAgentDojo:\n \nA\n \nDynamic\n \nEnvironment\n \nto\n \nEvaluate\n \nPrompt\n \nInjection\n \nAttacks\n and  Defenses  for  LLM  Agents.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2406.13352.   \n      AGENT  GOVERNANCE   \u2502   55   \nDepartment  of  Science,  Innovation  and  Technology.  2025.  \u201cInternational  AI  Safety  Report  2025.\u201d  \nGOV.UK.\n \nFebruary\n \n18,\n \n2025.\n https://www.gov.uk/government/publications/international-ai-safety-report-2025.   Erdil,  Ege,  and  Tamay  Besiroglu.  2024.  \u201cExplosive  Growth  from  AI  Automation:  A  Review  of  the  Arguments.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2309.11690.   Franzen,  Carl.  2024.  \u201cEmergence\u2019s  AI  Orchestrator  Launches  to  Do  What  Big  Tech  O\ufb00erings  Can\u2019t:  \nPlay\n \nWell\n \nwith\n \nOthers.\u201d\n \nVentureBeat\n,\n \nDecember\n \n3,\n \n2024.\n https://venturebeat.com/ai/emergences-ai-orchestrator-launches-to-do-what-big-tech-o\ufb00erings-cant-play-well-with-others/.   Franzen,  Carl,  and  Emilia  David.  2024.  \u201cOpenAI  Con\ufb01rms  New  Frontier  Models  O3  and  O3-Mini.\u201d  \nVentureBeat\n,\n \nDecember\n \n20,\n \n2024.\n https://venturebeat.com/ai/openai-con\ufb01rms-new-frontier-models-o3-and-o3-mini/.   Gilbert,  Thomas  Krendl,  Sarah  Dean,  Tom  Zick,  and  Nathan  Lambert.  2022.  \u201cChoices,  Risks,  and  \nReward\n \nReports:\n \nCharting\n \nPublic\n \nPolicy\n \nfor\n \nReinforcement\n \nLearning\n \nSystems.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2202.05716.   Google  DeepMind.  2025.  \u201cProject  Astra.\u201d  February  25,  2025.  https://deepmind.google/technologies/project-astra/.   Goolsbee,  Austan.  2018.  \u201cPublic  Policy  in  an  AI  Economy.\u201d  Working  Paper.  Working  Paper  Series.  National  Bureau  of  Economic  Research.  https://doi.org/10.3386/w24653.   Greenblatt,  Ryan,  Carson  Denison,  Benjamin  Wright,  Fabien  Roger,  Monte  MacDiarmid,  Sam  \nMarks,\n \nJohannes\n \nTreutlein,\n \net\n \nal.\n \n2024.\n \n\u201cAlignment\n \nFaking\n \nin\n \nLarge\n \nLanguage\n \nModels.\u201d\n arXiv.  https://doi.org/10.48550/arXiv.2412.14093.   Greenblatt,  Ryan,  Buck  Shlegeris,  Kshitij  Sachan,  and  Fabien  Roger.  2024.  \u201cAI  Control:  Improving  Safety  Despite  Intentional  Subversion.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2312.06942.   Gu,  Xiangming,  Xiaosen  Zheng,  Tianyu  Pang,  Chao  Du,  Qian  Liu,  Ye  Wang,  Jing  Jiang,  and  Min  \nLin.\n \n2024.\n \n\u201cAgent\n \nSmith:\n \nA\n \nSingle\n \nImage\n \nCan\n \nJailbreak\n \nOne\n \nMillion\n \nMultimodal\n \nLLM\n \nAgents\n Exponentially  Fast.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2402.08567.   Had\ufb01eld,  Gillian.  2024.  \u201cHow  to  Prevent  Millions  of  Invisible  Law-Free  AI  Agents  Casually  Wreaking  \nEconomic\n \nHavoc.\u201d\n \nFortune\n,\n \nOctober\n \n17,\n \n2024.\n https://fortune.com/2024/10/17/ai-agents-law-economy/.   Had\ufb01eld-Menell,  Dylan,  Anca  Dragan,  Pieter  Abbeel,  and  Stuart  Russell.  2017.  \u201cThe  O\ufb00-Switch  Game.\u201d  arXiv.  https://doi.org/10.48550/arXiv.1611.08219.   Hamin,  Maia,  and  Stewart  Scott.  2024.  \u201cHacking  with  AI.\u201d  Atlantic  Council.  https://www.atlanticcouncil.org/in-depth-research-reports/report/hacking-with-ai/.   Hammond,  Lewis,  Alan  Chan,  Jesse  Clifton,  Jason  Hoelscher-Obermaier,  Akbir  Khan,  Euan  \nMcLean,\n \nChandler\n \nSmith,\n \net\n \nal.\n \n2025.\n \n\u201cMulti-Agent\n \nRisks\n \nfrom\n \nAdvanced\n \nAI.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2502.14143.   He,  Yifeng,  Ethan  Wang,  Yuyang  Rong,  Zifei  Cheng,  and  Hao  Chen.  2024.  \u201cSecurity  of  AI  Agents.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2406.08689.   Heath,  Alex.  2023.  \u201cMark  Zuckerberg  Says  Meta  Wants  to  \u2018Introduce  AI  Agents  to  Billions  of  \nPeople.\u2019\u201d\n \nThe\n \nVerge\n,\n \nApril\n \n26,\n \n2023.\n https://www.theverge.com/2023/4/26/23699633/mark-zuckerberg-meta-generative-ai-chatbots-instagram-facebook-whatsapp.   Heaven,  Will  Douglas.  2020.  \u201cDeepMind\u2019s  Protein-Folding  AI  Has  Solved  a  50-Year-Old  Grand  \nChallenge\n \nof\n \nBiology.\u201d\n \nMIT\n \nTechnology\n \nReview\n,\n \nNovember\n \n30,\n \n2020.\n https://www.technologyreview.com/2020/11/30/1012712/deepmind-protein-folding-ai-solv\n      AGENT  GOVERNANCE   \u2502   56   \ned-biology-science-drugs-disease/.   Hendrycks,  Dan,  Collin  Burns,  Steven  Basart,  Andy  Zou,  Mantas  Mazeika,  Dawn  Song,  and  Jacob  \nSteinhardt.\n \n2021.\n \n\u201cMeasuring\n \nMassive\n \nMultitask\n \nLanguage\n \nUnderstanding.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2009.03300.   Hu,  Hengyuan,  Adam  Lerer,  Alex  Peysakhovich,  and  Jakob  Foerster.  2021.  \u201c\u2018Other-Play\u2019  for  Zero-Shot  Coordination.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2003.02979.   Huang,  Yiming,  Jianwen  Luo,  Yan  Yu,  Yitong  Zhang,  Fangyu  Lei,  Yifan  Wei,  Shizhu  He,  et  al.  2024.  \n\u201cDA-Code:\n \nAgent\n \nData\n \nScience\n \nCode\n \nGeneration\n \nBenchmark\n \nfor\n \nLarge\n \nLanguage\n \nModels.\u201d\n arXiv.  https://doi.org/10.48550/arXiv.2410.07331.   Institute  for  Law  &  AI.  2024.  \u201cWorkstreams  &  Research  Directions.\u201d  December  4,  2024.  https://docs.google.com/document/d/1JldBwEDhKgNH-DPZfv7O0bVpS1dNdaz8-OikZY01nSY.   Jabbari,  Shahin,  Matthew  Joseph,  Michael  Kearns,  Jamie  Morgenstern,  and  Aaron  Roth.  2017.  \n\u201cFairness\n \nin\n \nReinforcement\n \nLearning.\u201d\n \nIn\n \nProceedings\n \nof\n \nthe\n \n34th\n \nInternational\n \nConference\n on  Machine  Learning ,  1617\u201326.  PMLR.  https://proceedings.mlr.press/v70/jabbari17a.html.   Kapoor,  Sayash,  and  Arvind  Narayanan.  2024.  \u201cCan  AI  Automate  Computational  Reproducibility?\u201d  \nSubstack.\n \nAI\n \nSnake\n \nOil.\n \nDecember\n \n18,\n \n2024.\n https://www.aisnakeoil.com/p/can-ai-automate-computational-reproducibility.   Kapoor,  Sayash,  Benedikt  Stroebl,  Zachary  S.  Siegel,  Nitya  Nadgir,  and  Arvind  Narayanan.  2024.  \u201cAI  Agents  That  Matter.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2407.01502.   Klarna.  2024.  \u201cKlarna  AI  Assistant  Handles  Two-Thirds  of  Customer  Service  Chats  in  Its  First  \nMonth.\u201d\n \nFebruary\n \n27,\n \n2024.\n https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-\ufb01rst-month/.   Kolt,  Noam.  2025.  \u201cGoverning  AI  Agents.\u201d  SSRN  Scholarly  Paper.  Rochester,  NY:  Social  Science  Research  Network.  https://doi.org/10.2139/ssrn.4772956.   K\u00f6ster,  Raphael,  Kevin  R.  McKee,  Richard  Everett,  Laura  Weidinger,  William  S.  Isaac,  Edward  \nHughes,\n \nEdgar\n \nA.\n \nDu\u00e9\u00f1ez-Guzm\u00e1n,\n \nThore\n \nGraepel,\n \nMatthew\n \nBotvinick,\n \nand\n \nJoel\n \nZ.\n \nLeibo.\n \n2020.\n \n\u201cModel-Free\n \nConventions\n \nin\n \nMulti-Agent\n \nReinforcement\n \nLearning\n \nwith\n \nHeterogeneous\n Preferences.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2010.09054.   Kumar,  Priyanshu,  Elaine  Lau,  Saranya  Vijayakumar,  Tu  Trinh,  Scale  Red  Team,  Elaine  Chang,  \nVaughn\n \nRobinson,\n \net\n \nal.\n \n2024.\n \n\u201cRefusal-Trained\n \nLLMs\n \nAre\n \nEasily\n \nJailbroken\n \nAs\n \nBrowser\n Agents.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2410.13886.   Kwa,  Thomas,  Ben  West,  Joel  Becker,  Amy  Deng,  Katharyn  Garcia,  Max  Hasin,  Sami  Jawhar,  et  al.  \n2025.\n \n\u201cMeasuring\n \nAI\n \nAbility\n \nto\n \nComplete\n \nLong\n \nTasks.\u201d\n \narXiv\n \n[Cs.AI]\n.\n \narXiv.\n https://doi.org/10.48550/arXiv.2503.14499.   Lambert,  Nathan,  Louis  Castricato,  Leandro  von  Werra,  and  Alex  Havrilla.  2022.  \u201cIllustrating  \nReinforcement\n \nLearning\n \nfrom\n \nHuman\n \nFeedback\n \n(RLHF).\u201d\n \nHugging\n \nFace.\n \nDecember\n \n9,\n 2022.  https://huggingface.co/blog/rlhf.   Lazar,  Seth,  Luke  Thorburn,  Tian  Jin,  and  Luca  Belli.  2024.  \u201cThe  Moral  Case  for  Using  Language  Model  Agents  for  Recommendation.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2410.12123.   Leike,  Jan.  2024.  \u201cTwo  Alignment  Threat  Models.\u201d  Musings  on  the  Alignment  Problem.  November  8,  2024.  https://aligned.substack.com/p/two-alignment-threat-models.   Leike,  Jan,  David  Krueger,  Tom  Everitt,  Miljan  Martic,  Vishal  Maini,  and  Shane  Legg.  2018.  \n\u201cScalable\n \nAgent\n \nAlignment\n \nvia\n \nReward\n \nModeling:\n \nA\n \nResearch\n \nDirection.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.1811.07871.   \n      AGENT  GOVERNANCE   \u2502   57   \nLeike,  Jan,  Miljan  Martic,  Victoria  Krakovna,  Pedro  A.  Ortega,  Tom  Everitt,  Andrew  Lefrancq,  \nLaurent\n \nOrseau,\n \nand\n \nShane\n \nLegg.\n \n2017.\n \n\u201cAI\n \nSafety\n \nGridworlds.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.1711.09883.   Lermen,  Simon,  Mateusz  Dziemian,  and  Govind  Pimpale.  2024.  \u201cApplying  Refusal-Vector  Ablation  to  Llama  3.1  70B  Agents.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2410.10871.   Letta.  2024.  \u201cThe  AI  Agents  Stack.\u201d  November  14,  2024.  https://www.letta.com/blog/ai-agents-stack.   Li,  Huao,  Yu  Quan  Chong,  Simon  Stepputtis,  Joseph  Campbell,  Dana  Hughes,  Michael  Lewis,  and  \nKatia\n \nSycara.\n \n2023.\n \n\u201cTheory\n \nof\n \nMind\n \nfor\n \nMulti-Agent\n \nCollaboration\n \nvia\n \nLarge\n \nLanguage\n \nModels.\u201d\n \nIn\n \nProceedings\n \nof\n \nthe\n \n2023\n \nConference\n \non\n \nEmpirical\n \nMethods\n \nin\n \nNatural\n Language  Processing ,  180\u201392.  https://doi.org/10.18653/v1/2023.emnlp-main.13.   Lin,  Zheng,  Zhenxing  Niu,  Zhibin  Wang,  and  Yinghui  Xu.  2024.  \u201cInterpreting  and  Mitigating  \nHallucination\n \nin\n \nMLLMs\n \nthrough\n \nMulti-Agent\n \nDebate.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2407.20505.   Masterman,  Tula,  Sandi  Besen,  Mason  Sawtell,  and  Alex  Chao.  2024.  \u201cThe  Landscape  of  \nEmerging\n \nAI\n \nAgent\n \nArchitectures\n \nfor\n \nReasoning,\n \nPlanning,\n \nand\n \nTool\n \nCalling:\n \nA\n \nSurvey.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2404.11584.   Meinke,  Alexander,  Bronson  Schoen,  J\u00e9r\u00e9my  Scheurer,  Mikita  Balesni,  Rusheb  Shah,  and  Marius  \nHobbhahn.\n \n2025.\n \n\u201cFrontier\n \nModels\n \nAre\n \nCapable\n \nof\n \nIn-Context\n \nScheming.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2412.04984.   METR.  2024.  \u201cAn  Update  on  Our  General  Capability  Evaluations.\u201d  METR  Blog ,  August.  https://metr.org/blog/2024-08-06-update-on-evaluations/.   Mialon,  Gr\u00e9goire,  Cl\u00e9mentine  Fourrier,  Craig  Swift,  Thomas  Wolf,  Yann  LeCun,  and  Thomas  \nScialom.\n \n2023.\n \n\u201cGAIA:\n \nA\n \nBenchmark\n \nfor\n \nGeneral\n \nAI\n \nAssistants.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2311.12983.   Minardi,  Di.  2020.  \u201cThe  Grim  Fate  That  Could  Be  \u2018Worse  than  Extinction.\u2019\u201d  BBC ,  October  16,  2020.  https://www.bbc.com/future/article/20201014-totalitarian-world-in-chains-arti\ufb01cial-intelligence.   Naihin,  Silen,  David  Atkinson,  Marc  Green,  Merwane  Hamadi,  Craig  Swift,  Douglas  Schonholtz,  \nAdam\n \nTauman\n \nKalai,\n \nand\n \nDavid\n \nBau.\n \n2023.\n \n\u201cTesting\n \nLanguage\n \nModel\n \nAgents\n \nSafely\n \nin\n \nthe\n Wild.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2311.10538.   NVIDIA.  n.d.  \u201cWhat  Is  Physical  AI?\u201d  Accessed  March  13,  2025.  https://www.nvidia.com/en-us/glossary/generative-physical-ai/.   O\u2019Keefe,  Cullen,  Peter  Cihon,  Ben  Gar\ufb01nkel,  Carrick  Flynn,  Jade  Leung,  and  Allan  Dafoe.  2020.  \n\u201cThe\n \nWindfall\n \nClause:\n \nDistributing\n \nthe\n \nBene\ufb01ts\n \nof\n \nAI\n \nfor\n \nthe\n \nCommon\n \nGood.\u201d\n \nIn\n \nProceedings\n \nof\n \nthe\n \nAAAI/ACM\n \nConference\n \non\n \nAI,\n \nEthics,\n \nand\n \nSociety\n,\n \n327\u201331.\n \nNew\n \nYork\n \nNY\n \nUSA:\n \nACM.\n https://doi.org/10.1145/3375627.3375842.   OpenAI.  2024a.  \u201cIntroducing  SWE-Bench  Veri\ufb01ed.\u201d  August  13,  2024.  https://openai.com/index/introducing-swe-bench-veri\ufb01ed/.   \u2014\u2014\u2014.  2024b.  \u201cLearning  to  Reason  with  LLMs.\u201d  September  12,  2024.  https://openai.com/index/learning-to-reason-with-llms/.   \u2014\u2014\u2014.  2024c.  \u201cO1  System  Card.\u201d  December  5,  2024.  https://openai.com/index/openai-o1-system-card/.   \u2014\u2014\u2014.  2025a.  \u201cIntroducing  Operator.\u201d  January  23,  2025.  https://openai.com/index/introducing-operator/.   \u2014\u2014\u2014.  2025b.  \u201cO3-Mini  System  Card.\u201d  January  31,  2025.  \n      AGENT  GOVERNANCE   \u2502   58   \nhttps://openai.com/index/o3-mini-system-card/.   Owen,  David.  2024.  \u201cInterviewing  AI  Researchers  on  Automation  of  AI  R&D.\u201d  Epoch  AI.  August  27,  2024.  https://epoch.ai/blog/interviewing-ai-researchers-on-automation-of-ai-rnd.   Paglieri,  Davide,  Bart\u0142omiej  Cupia\u0142,  Samuel  Coward,  Ulyana  Piterbarg,  Maciej  Wolczyk,  Akbir  Khan,  \nEduardo\n \nPignatelli,\n \net\n \nal.\n \n2024.\n \n\u201cBALROG:\n \nBenchmarking\n \nAgentic\n \nLLM\n \nand\n \nVLM\n \nReasoning\n On  Games.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2411.13543.   Pan,  Alexander,  Kimin  Lee,  Bo  Li,  Karthik  Narasimhan,  Dawn  Song,  and  Isabelle  Barrass.  2024.  \n\u201cTowards\n \nSafe\n \n&\n \nTrustworthy\n \nAgents.\u201d\n \nIn\n \nNeurIPS\n.\n https://neurips.cc/virtual/2024/workshop/84748.   Peng,  Alwin,  Julian  Michael,  Henry  Sleight,  Ethan  Perez,  and  Mrinank  Sharma.  2024.  \u201cRapid  \nResponse:\n \nMitigating\n \nLLM\n \nJailbreaks\n \nwith\n \na\n \nFew\n \nExamples.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2411.07494.   Pichai,  Sundar.  2024.  \u201cQ3  Earnings  Call:  CEO\u2019s  Remarks.\u201d  Google.  October  29,  2024.  https://blog.google/inside-google/message-ceo/alphabet-earnings-q3-2024/.   Pillay,  Tharin.  2024.  \u201cAI  Models  Are  Getting  Smarter.  New  Tests  Are  Racing  to  Catch  Up.\u201d  TIME ,  December  24,  2024.  https://time.com/7203729/ai-evaluations-safety/.   Pimpale,  Govind,  Axel  H\u00f8jmark,  J\u00e9r\u00e9my  Scheurer,  and  Marius  Hobbhahn.  2025.  \u201cForecasting  \nFrontier\n \nLanguage\n \nModel\n \nAgent\n \nCapabilities.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2502.15850.   Project  Zero.  2024.  \u201cProject  Zero:  From  Naptime  to  Big  Sleep:  Using  Large  Language  Models  To  \nCatch\n \nVulnerabilities\n \nIn\n \nReal-World\n \nCode.\u201d\n \nGoogle.\n \nNovember\n \n1,\n \n2024.\n https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html.   Rajesh,  Ananya  Mariam,  and  Krystal  Hu.  2023.  \u201cAI  Startup  Adept  Raises  $350  Mln  in  Fresh  \nFunding.\u201d\n \nReuters\n,\n \nMarch\n \n16,\n \n2023,\n \nsec.\n \nTechnology.\n https://www.reuters.com/technology/adept-raises-350-mln-series-b-funding-2023-03-14/.   Rein,  David,  Betty  Li  Hou,  Asa  Cooper  Stickland,  Jackson  Petty,  Richard  Yuanzhe  Pang,  Julien  \nDirani,\n \nJulian\n \nMichael,\n \nand\n \nSamuel\n \nR.\n \nBowman.\n \n2023.\n \n\u201cGPQA:\n \nA\n \nGraduate-Level\n Google-Proof  Q&A  Benchmark.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2311.12022.   Reiner,  Shaked.  2024.  \u201cAnatomy  of  an  LLM  RCE.\u201d  Cyber  Ark.  October  28,  2024.  https://www.cyberark.com/resources/threat-research-blog/anatomy-of-an-llm-rce.   Research  and  Markets.  2025.  \u201cCall  Centers  -  Global  Strategic  Business  Report  -  Research  and  \nMarkets.\u201d\n \nMarch\n \n2025.\n https://www.researchandmarkets.com/reports/338444/call_centers_global_strategic_business_report.   Rinehart,  Will.  2024.  \u201cTransformative  Growth  with  AI  Is  Likely.  Explosive  Growth  Is  Science  Fiction.\u201d  \nAmerican\n \nEnterprise\n \nInstitute.\n \nJune\n \n18,\n \n2024.\n https://www.aei.org/articles/transformative-growth-with-ai-is-likely-explosive-growth-is-science-\ufb01ction/.   Roger,  Fabien,  and  Ryan  Greenblatt.  2023.  \u201cPreventing  Language  Models  From  Hiding  Their  Reasoning.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2310.18512.   Russell,  Stuart,  and  Peter  Norvig.  2020.  Arti\ufb01cial  Intelligence:  A  Modern  Approach .  4th  ed.  Pearson.  https://aima.cs.berkeley.edu/.   Sandbrink,  Jonas,  Hamish  Hobbs,  Jacob  Swett,  Allan  Dafoe,  and  Anders  Sandberg.  2022.  \n\u201cDi\ufb00erential\n \nTechnology\n \nDevelopment:\n \nAn\n \nInnovation\n \nGovernance\n \nConsideration\n \nfor\n \nNavigating\n \nTechnology\n \nRisks.\u201d\n \nSSRN\n \nScholarly\n \nPaper.\n \nRochester,\n \nNY:\n \nSocial\n \nScience\n Research  Network.  https://doi.org/10.2139/ssrn.4213670.   \n      AGENT  GOVERNANCE   \u2502   59   \nSett,  Gaurav.  2024.  \u201cHow  AI  Can  Automate  AI  Research  and  Development.\u201d  RAND.  https://www.rand.org/pubs/commentary/2024/10/how-ai-can-automate-ai-research-and-development.html.   Sharkey,  Lee,  Cl\u00edodhna  N\u00ed  Ghuidhir,  Dan  Braun,  J\u00e9r\u00e9my  Scheurer,  Mikita  Balesni,  Lucius  Bushnaq,  \nCharlotte\n \nStix,\n \nand\n \nMarius\n \nHobbhahn.\n \n2024.\n \n\u201cA\n \nCausal\n \nFramework\n \nfor\n \nAI\n \nRegulation\n \nand\n Auditing.\u201d  https://doi.org/10.20944/preprints202401.1424.v1.   Shavit,  Yonadav,  Sandhini  Agarwal,  Miles  Brundage,  Steven  Adler,  Cullen  O\u2019Keefe,  Rosie  \nCampbell,\n \nTeddy\n \nLee,\n \nPamela\n \nMishkin,\n \nTyna\n \nEloundou,\n  \nAlan\n \nHickey,\n \nKatarina\n \nSlama,\n \nLama\n \nAhmad,\n \nPaul\n \nMcMillan,\n \nAlex\n \nBeutel,\n \nAlexandre\n \nPassos,\n \nand\n \nDavid\n \nG.\n \nRobinson.\n \n2024.\n \n\u201cPractices\n \nfor\n \nGoverning\n \nAgentic\n \nAI\n \nSystems.\u201d\n \nOpenAI.\n \nFebruary\n \n14,\n \n2024.\n https://openai.com/index/practices-for-governing-agentic-ai-systems/.   Siegel,  Zachary  S.,  Sayash  Kapoor,  Nitya  Nagdir,  Benedikt  Stroebl,  and  Arvind  Narayanan.  2024.  \n\u201cCORE-Bench:\n \nFostering\n \nthe\n \nCredibility\n \nof\n \nPublished\n \nResearch\n \nThrough\n \na\n \nComputational\n Reproducibility  Agent  Benchmark.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2409.11363.   Smith,  Alastair,  Bruce  Bueno  de  Mesquita,  James  D  Morrow,  and  Randolph  M  Siverson.  2005.  The  \nLogic\n \nof\n \nPolitical\n \nSurvival\n.\n \nMIT\n \nPress.\n https://mitpress.mit.edu/9780262524407/the-logic-of-political-survival/.   Soice,  Emily  H.,  Rafael  Rocha,  Kimberlee  Cordova,  Michael  Specter,  and  Kevin  M.  Esvelt.  2023.  \n\u201cCan\n \nLarge\n \nLanguage\n \nModels\n \nDemocratize\n \nAccess\n \nto\n \nDual-Use\n \nBiotechnology?\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2306.03809.   Sozzi,  Brian.  2024.  \u201cSalesforce  Co-Founder  and  CEO  Marc  Benio\ufb00:  Autonomous  AI  Agents  Will  \nBeat\n \nCopilots.\u201d\n \nYahoo\n \nFinance\n,\n \nSeptember\n \n18,\n \n2024.\n https://\ufb01nance.yahoo.com/news/salesforce-co-founder-and-ceo-marc-benio\ufb00-autonomous-ai-agents-will-beat-copilots-155044728.html.   Stroebl,  Benedikt,  Sayash  Kapoor,  and  Arvind  Narayanan.  2025.  \u201cHAL:  A  Holistic  Agent  \nLeaderboard\n \nfor\n \nCentralized\n \nand\n \nReproducible\n \nAgent\n \nEvaluation.\u201d\n \nPrinceton\n \nUniversity.\n 2025.  https://hal.cs.princeton.edu/.   Terekhov,  Mikhail,  Romain  Graux,  Eduardo  Neville,  Denis  Rosset,  and  Gabin  Kolly.  2023.  \n\u201cSecond-Order\n \nJailbreaks:\n \nGenerative\n \nAgents\n \nSuccessfully\n \nManipulate\n \nThrough\n \nan\n Intermediary.\u201d  In  NeurIPS .  https://openreview.net/forum?id=HPmhaOTseN.   Thadani,  Trisha,  Faiz  Siddiqui,  Rachel  Lerman,  Whitney  Shefte,  Julia  Wall,  and  Talia  Trackim.  2024.  \n\u201cTesla\n \nWorker\n \nKilled\n \nin\n \nFiery\n \nCrash\n \nMay\n \nBe\n \nFirst\n \n\u2018Full\n \nSelf-Driving\u2019\n \nFatality.\u201d\n \nWashington\n \nPost\n,\n \nFebruary\n \n13,\n \n2024.\n https://www.washingtonpost.com/technology/interactive/2024/tesla-full-self-driving-fatal-crash/.   Toews,  Rob.  2024.  \u201cAgents  Are  The  Future  Of  AI.  Where  Are  The  Startup  Opportunities?\u201d  Forbes ,  \nJuly\n \n9,\n \n2024,\n \nsec.\n \nAI.\n https://www.forbes.com/sites/robtoews/2024/07/09/agents-are-the-future-of-ai-where-are-the-startup-opportunities/.   Toner,  Helen,  John  Bansemer,  Kyle  Crichton,  Matthew  Burtell,  Thomas  Woodside,  Anat  Lior,  \nAndrew\n \nLohn,\n \net\n \nal.\n \n2024.\n \n\u201cThrough\n \nthe\n \nChat\n \nWindow\n \nand\n \nInto\n \nthe\n \nReal\n \nWorld:\n \nPreparing\n \nfor\n \nAI\n \nAgents.\u201d\n \nCenter\n \nfor\n \nSecurity\n \nand\n \nEmerging\n \nTechnology.\n https://doi.org/10.51593/20240034.   Turtayev,  Rustem,  Artem  Petrov,  Dmitrii  Volkov,  and  Denis  Volk.  2024.  \u201cHacking  CTFs  with  Plain  Agents.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2412.02776.   UK  AI  Security  Institute.  2024a.  \u201cPre-Deployment  Evaluation  of  Anthropic\u2019s  Upgraded  Claude  3.5  \n      AGENT  GOVERNANCE   \u2502   60   \nSonnet.\u201d  November  19,  2024.  https://www.aisi.gov.uk/work/pre-deployment-evaluation-of-anthropics-upgraded-claude-3-5-sonnet.   \u2014\u2014\u2014.  2024b.  \u201cPre-Deployment  Evaluation  of  OpenAI\u2019s  O1  Model.\u201d  December  18,  2024.  https://www.aisi.gov.uk/work/pre-deployment-evaluation-of-openais-o1-model.   \u2014\u2014\u2014.  2025.  \u201cAbout.\u201d  2025.  https://www.aisi.gov.uk/about.   \u2014\u2014\u2014.  n.d.  \u201cExamples  of  Systemic  AI  Safety  Projects.\u201d  Accessed  March  14,  2025.  https://www.aisi.gov.uk/grants/example-projects.   United  Nations.  2023.  \u201cA  New  Agenda  for  Peace.\u201d  Policy  Brief  9.  Department  of  Political  and  Peacebuilding  A\ufb00airs.  https://dppa.un.org/en/a-new-agenda-for-peace.   Van  Loo,  Rory.  2019.  \u201cDigital  Market  Perfection.\u201d  Michigan  Law  Review  117  (5):  815\u201384.  https://doi.org/10.36644/mlr.117.5.digital.   Victor,  Jon.  2024.  \u201cSoftware  Firms  Race  to  Beat  OpenAI  in  AI  Agents.\u201d  The  Information ,  September  \n26,\n \n2024.\n https://www.theinformation.com/articles/software-\ufb01rms-race-to-beat-openai-in-ai-agents.   Waisman,  Nico,  and  Brendan  Dolan-Gavitt.  2024.  \u201cHow  XBOW  Found  a  Scoold  Authentication  Bypass.\u201d  XBOW.  November  13,  2024.  https://xbow.com/blog/xbow-scoold-vuln/.   Wang,  Alex,  Amanpreet  Singh,  Julian  Michael,  Felix  Hill,  Omer  Levy,  and  Samuel  R.  Bowman.  \n2019.\n \n\u201cGLUE:\n \nA\n \nMulti-Task\n \nBenchmark\n \nand\n \nAnalysis\n \nPlatform\n \nfor\n \nNatural\n \nLanguage\n Understanding.\u201d  arXiv.  https://doi.org/10.48550/arXiv.1804.07461.   Wang,  Lei,  Chen  Ma,  Xueyang  Feng,  Zeyu  Zhang,  Hao  Yang,  Jingsen  Zhang,  Zhiyuan  Chen,  et  al.  \n2024.\n \n\u201cA\n \nSurvey\n \non\n \nLarge\n \nLanguage\n \nModel\n \nBased\n \nAutonomous\n \nAgents.\u201d\n \nFrontiers\n \nof\n Computer  Science  18  (6):  186345.  https://doi.org/10.1007/s11704-024-40231-1.   Wayt,  Theo.  2024.  \u201cKlarna\u2019s  AI  Savings.\u201d  The  Information ,  November  26,  2024.  https://www.theinformation.com/articles/klarnas-ai-savings.   Weil,  Gabriel.  2024.  \u201cTort  Law  as  a  Tool  for  Mitigating  Catastrophic  Risk  from  Arti\ufb01cial  Intelligence.\u201d  \nSSRN\n \nScholarly\n \nPaper.\n \nRochester,\n \nNY:\n \nSocial\n \nScience\n \nResearch\n \nNetwork.\n https://doi.org/10.2139/ssrn.4694006.   Weng,  Lilian.  2023.  \u201cLLM  Powered  Autonomous  Agents.\u201d  June  23,  2023.  https://lilianweng.github.io/posts/2023-06-23-agent/.   Wijk,  Hjalmar,  Tao  Lin,  Joel  Becker,  Sami  Jawhar,  Neev  Parikh,  Thomas  Broadley,  Lawrence  Chan,  \net\n \nal.\n \n2024.\n \n\u201cRE-Bench:\n \nEvaluating\n \nFrontier\n \nAI\n \nR&D\n \nCapabilities\n \nof\n \nLanguage\n \nModel\n \nAgents\n against  Human  Experts.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2411.15114.   Wu,  Chen  Henry,  Rishi  Shah,  Jing  Yu  Koh,  Ruslan  Salakhutdinov,  Daniel  Fried,  and  Aditi  \nRaghunathan.\n \n2025.\n \n\u201cDissecting\n \nAdversarial\n \nRobustness\n \nof\n \nMultimodal\n \nLM\n \nAgents.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2406.12814.   Xie,  Tianbao,  Danyang  Zhang,  Jixuan  Chen,  Xiaochuan  Li,  Siheng  Zhao,  Ruisheng  Cao,  Toh  Jing  \nHua,\n \net\n \nal.\n \n2024.\n \n\u201cOSWorld:\n \nBenchmarking\n \nMultimodal\n \nAgents\n \nfor\n \nOpen-Ended\n \nTasks\n \nin\n Real  Computer  Environments.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2404.07972.   Yang,  John,  Carlos  E.  Jimenez,  Alex  L.  Zhang,  Kilian  Lieret,  Joyce  Yang,  Xindi  Wu,  Ori  Press,  et  al.  \n2024.\n \n\u201cSWE-Bench\n \nMultimodal:\n \nDo\n \nAI\n \nSystems\n \nGeneralize\n \nto\n \nVisual\n \nSoftware\n \nDomains?\u201d\n arXiv.  https://doi.org/10.48550/arXiv.2410.03859.   Yao,  Shunyu,  Noah  Shinn,  Pedram  Razavi,  and  Karthik  Narasimhan.  2024.  \u201c\u03c4- Bench:  A  Benchmark  \nfor\n \nTool-Agent-User\n \nInteraction\n \nin\n \nReal-World\n \nDomains.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2406.12045.   Zhang,  Andy  K.,  Neil  Perry,  Riya  Dulepet,  Joey  Ji,  Celeste  Menders,  Justin  W.  Lin,  Eliot  Jones,  et  \n      AGENT  GOVERNANCE   \u2502   61   \nal.  2024.  \u201cCybench:  A  Framework  for  Evaluating  Cybersecurity  Capabilities  and  Risks  of  Language  Models.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2408.08926.   Zhou,  Shuyan,  Frank  F.  Xu,  Hao  Zhu,  Xuhui  Zhou,  Robert  Lo,  Abishek  Sridhar,  Xianyi  Cheng,  et  al.  \n2024.\n \n\u201cWebArena:\n \nA\n \nRealistic\n \nWeb\n \nEnvironment\n \nfor\n \nBuilding\n \nAutonomous\n \nAgents.\u201d\n \narXiv.\n https://doi.org/10.48550/arXiv.2307.13854.   Zhou,  Xuhui,  Hyunwoo  Kim,  Faeze  Brahman,  Liwei  Jiang,  Hao  Zhu,  Ximing  Lu,  Frank  Xu,  et  al.  \n2024.\n \n\u201cHAICOSYSTEM:\n \nAn\n \nEcosystem\n \nfor\n \nSandboxing\n \nSafety\n \nRisks\n \nin\n \nHuman-AI\n Interactions.\u201d  arXiv.  https://doi.org/10.48550/arXiv.2409.16427.   Zittrain,  Jonathan  L.  2024.  \u201cWe  Need  to  Control  AI  Agents  Now.\u201d  The  Atlantic ,  July  2,  2024,  sec.  \nTechnology.\n https://www.theatlantic.com/technology/archive/2024/07/ai-agents-safety-risks/678864/.   Zuckerberg,  Mark.  2024.  \u201cExclusive  Interview  with  Mark  Zuckerberg.\u201d  The  Rundown  AI.  July  23,  2024.  https://www.therundown.ai/p/meta-releases-llama-405b.   \n      AGENT  GOVERNANCE   \u2502   62  ",
      "arXiv:2505.22088v1  [cs.SD]  28 May 2025\nVisual Cues Support Robust Turn-taking Prediction in Noise\nSam O\u2019Connor Russell, Naomi Harte\nADAPT Centre, School of Engineering, Trinity College Dublin, Ireland\nrusselsa@tcd.ie, nharte@tcd.ie\nAbstract\nAccurate predictive turn-taking models (PTTMs) are essen-\ntial for naturalistic human-robot interaction. However, little\nis known about their performance in noise. This study there-\nfore explores PTTM performance in types of noise likely to be\nencountered once deployed. Our analyses reveal PTTMs are\nhighly sensitive to noise. Hold/shift accuracy drops from 84%\nin clean speech to just 52% in 10 dB music noise. Training\nwith noisy data enables a multimodal PTTM, which includes\nvisual features to better exploit visual cues, with 72% accuracy\nin 10 dB music noise. The multimodal PTTM outperforms the\naudio-only PTTM across all noise types and SNRs, highlighting\nits ability to exploit visual cues; however, this does not always\ngeneralise to new types of noise. Analysis also reveals that suc-\ncessful training relies on accurate transcription, limiting the use\nof ASR-derived transcriptions to clean conditions. We make\ncode publicly available for future research.\nIndex Terms: multimodal interaction, turn-taking, human-\nrobot interaction, noise\n1. Introduction\nHuman turn-taking is a remarkable process. There is only 200\nmilliseconds of silence on average between speaking turns [1],\nyet language production takes over 600 milliseconds [2]. Lis-\nteners must therefore plan what they are about to say whilst their\nconversational partner is still speaking [3]. Although effortless\nfor humans, turn-taking is highly challenging for robots. Most\nturn-taking algorithms in use today are based on a robot react-\ning to silence after a turn [4], resulting in interactions that are\nless fluid than those between humans [5, 6]. Predictive turn-\ntaking models (PTTMs) have been proposed to overcome these\nlimitations [7, 4]. PTTMs learn human-like turn-taking from\nlarge corpora of human interaction, e.g. continuously predict-\ning whether a shift between speakers will occur [7, 8, 9, 5], akin\nto human turn-taking [3].\nMost PTTMs use speech features [4]. Yet in human-human\ninteraction, listeners make faster and more accurate turn-taking\ndecisions when they can see and hear a speaker [10]. Re-\ncent work shows multimodal PTTMs, which also use gaze,\nhead pose and facial expression, outperform audio-only PTTMs\n[11, 12], illustrating the known role of visual cues in turn-taking\n[13, 10]. This reflects the many communicative advantages of\nmultimodality [14], which include boosting the intelligibility of\nspeech in noise [15]. MacLeod and Summerfield found that the\nintelligibility gain from being able to see a speaker was equiva-\nlent to an 11 dB increase in the signal-to-noise ratio (SNR) [15].\nThis motivates audio-visual speech recognition (A VSR), which\nexploits visual cues to overcome the performance degradation\nof automatic speech recognition (ASR) in noise [16].\nThis paper asks if multimodality confers similar perfor-\nmance benefits to PTTMs in noisy environments. Surprisingly,\nPTTMs have to date only been tested on interactions free from\nbackground noise interference [4, 11]. The impact of noise\non PTTM performance is therefore unknown and unquantified.\nThis presents a major oversight as PTTMs will be deployed in-\nthe-wild, where they will inevitably encounter diverse sources\nand levels of noise. We therefore present the first exploration of\nPTTM performance in background noise, asking:\n1. How is the performance of predictive turn-taking models af-\nfected by background noise? and\n2. Does the inclusion of visual features make multimodal\nPTTMs robust to background noise?\nThis paper explores the crucial capability of a PTTM to\ndistinguish holds (no speaker change) from shifts (changes be-\ntween speakers). In noise-free conditions, a multimodal PTTM\nwhich incorporates visual features outperforms an audio-only\nmodel ( 84% and 80% accuracy, Table 1). When we add ar-\ntificial music, babble, and speech noise, the accuracy of both\nPTTMs is poor (51% and 52% in music). Including noise dur-\ning training increases accuracy (61% and 72% in music). It also\n\u2018primes\u2019 the multimodal PTTM to better exploit visual infor-\nmation (10 dB effective SNR gain over the audio-only PTTM).\nWe find this effect generalises to types of noises unseen by the\nmodel during training. The inclusion of visual cues therefore\nmakes PTTM performance more robust to noise. We also in-\nvestigate the reliance of PTTMs training on accurate transcrip-\ntions. When noisy data is re-transcribed with ASR PTTM per-\nformance collapses (50-54%). PTTMs training therefore relies\nheavily on accurately aligned transcripts collected from noise-\nfree recordings, complicating the use of ASR in noisy data.\nPTTMs are essential to facilitating interaction between hu-\nmans and robots, but there is little understanding of how well\nthey could work in-the-wild. Part of the puzzle is understand-\ning how they handle noise and our work therefore represents a\nsignificant step forward in PTTMs development.\n2. Predictive turn-taking models\nPTTMs continually predict future speaking activity, from which\nturn-taking predictions can be made [7, 8, 17]. For example, if\nspeaker 0 is currently talking, but the model prediction favours\nspeaker 1, a shift is predicted. PTTMs are developed on corpora\nof human-human interaction with the long-term eventual goal\nof deployment to human-robot interaction. Our work considers\ntwo-party interaction, where most work has been conducted [4].\nWe evaluate two PTTMs based on the state-of-the-artvoice-\nactivity projection (V AP)approach introduced by Ekstedt et al.\n[17] as it has been thoroughly explored [18, 9, 11]. V AP models\npredict future speaking activity in 8 bins (4 per speaker). Attime t = 0, bins span [0, 0.2], [0.2, 0.6], [0.6, 1.2] and [1.2, 2.0]\nseconds into the future. Each bin has a binary label (1=speech\nif > 50% frames contain speech), giving 256 possible outputs.\nV AP models are neural networks consisting of stacked\ntransformer decoder layers [17, 19]. The first model is an\naudio-only V AP modelwhich we re-implement from [19]. The\ninitial V AP model used mono audio and ground-truth knowl-\nedge of the current speaker [17]; i.e. it knew when each turn\nended. The updated model used here takes stereo audio with\n1 channel per speaker and removes the dependency on ground-\ntruth knowledge, reflecting a real-world deployment. We also\nuse a multimodal V AP (MM-V AP)model that incorporates fa-\ncial action unit, facial landmark, gaze and head pose features\nfrom OpenFace [20, 11, 12]. As the initial MM-V AP model also\ndepended on the ground-truth knowledge of the current speaker\n[11], we similarly extended the model to support stereo audio\nand remove this dependency [12]. For comparison, we also\ntrain a video-only V APmodel, identical to V AP with the audio\nencoder removed [19]. Our implementations, along with full\narchitectural details are publicly available in our repository1.\nPTTMs require labels of the start and end of each utterance\nfor training (thealignment). Manual transcriptions are typically\nused e.g. the Switchboard telephone corpus [21, 17, 22, 5].\nASR more closely represents real-world deployments and has\nbeen shown to work well in PTTM training [23, 24].\n3. Data preparation\nThis work uses the Candor corpus [25], an 850 hr corpus of\n1657 unscripted two-party videoconferencing interactions be-\ntween speakers in US English. The average session duration is\n34 min. Recordings are 60 fps video, 32 kHz stereo audio with\none channel per speaker. There is no controlled background\nnoise. Recordings take place at a location of the participant\u2019s\nchoosing e.g. home offices. We transcribe with Amazon Tran-\nscribe (EN-US model) which outputs word-level timings and\npreserves most disfluencies [23]. We extract gaze, head pose,\nfacial action units, and facial landmarks with OpenFace [20].\nWe follow the A VSR literature by artificially simulating\nnoisy conditions, adding noise separately to each channel at\ncontrolled SNRs [16]. Babble noiseconsists of 65,000 unique\n30 speaker babbles which is synthesised from LRS-3 [26, 16]\n(46 hr train, 14 hr test). Music noiseis sourced from MUSAN\n[27] (15 hr train from the Jamendo genre, 70 hr test). Interfer-\nence from other speakers ( speech noise) is simulated by over-\nlaying another speaker from Candor, ensuring train/test separa-\ntion.\nIn A VSR research, noise is typically added to isolated\nspeech segments. However, PTTMs are trained using conver-\nsational speech which includes long pauses. A standard method\nof adding controlled noise to conversational speech has not been\nreported. We propose to add noise at a given SNR by consider-\ning signal energy during speaking times and then adding scaled\nnoise to the complete recording. Natural variation in amplitude\nmeans this yields an average SNR over all utterances.\nThe clean test audiocomprises 70 sessions withheld for\nevaluation. The test audio is also corrupted with -10 to +10 dB\nSNRs noise in 2.5 dB increments ( noisy test audio). Shi et\nal. found that adding noise in training increased A VSR perfor-\nmance. Following their method, in each training session, with\nprobability 0.25 we add either babble, music or speech noise at\n0 dB [16], yielding the augmented training audio. Note that\n1https://github.com/russelsa/mm-vap\nclean and augmented training sets are the same length. We ob-\ntain both clean and augmented alignments for both training\nsets with Amazon ASR.\n4. Model training and evaluation\nFirst, we train with the clean audio + clean alignment, as is\nstandard in the turn-taking literature [17, 24]. Then we train on\nthe augmented audio + clean alignment, where noise is added\nbut the transcription remains unchanged [16]. To test the re-\nliance on accurate ASR, we train using the augmented audio +\naugmented alignment. This represents a situation where there\nis noisy data transcribed with ASR. Training batches consist\nof 20-second windows with 2 second overlap randomly sam-\npled from training sessions. The training procedure is a 5-fold\ncross-validation with 10 epochs per fold on an NVidia RTX\n6000 GPU. A grid search sets the batch size to 16 and the learn-\ning rate to 1e-4. The loss function is the sum of cross-entropy\n(future speaking prediction) and binary cross-entropy (current\nspeaker prediction) losses minimised with AdamW [28].\nWe report the hold/shift (H/S) prediction accuracy: the abil-\nity of the model to distinguish holds (no speaker change) from\nshifts (speaker change), a common metric in the turn-taking lit-\nerature [17, 19]. First, all silences > 250 ms are identified.\nThose with speaker changes are shifts (83,158) and those with-\nout are holds (206,830) [17]. Then, the probability of speech in\nthe latter half of the V AP bins is computed [17]. If speaker 0\nhas finished speaking and the probability of speaker 1 exceeds\na binary threshold, a shift is predicted. We set the threshold to\nmaximise balanced accuracy on validation sessions and report a\n5-fold average on test sessions.\nWe report effective SNR gain i.e. the SNR at which MM-\nV AP achieves the same accuracy as V AP at 0 dB, following the\nA VSR literature [29]. We compare V AP and MM-V AP model\nperformance with two-tailed independent t-tests [30].\n5. Turn-taking model performance in noise\n5.1. Baseline performance on clean speech\nWe begin our analysis by training V AP and MM-V AP models\nusing the clean audio and clean alignment, establishing a base-\nline H/S prediction accuracy without added noise (clean test au-\ndio, Table 1, first two rows). The 5-fold average H/S prediction\naccuracy of MM-V AP model is 84%, greater than the audio-\nonly V AP at 80% (p <0.001). The accuracy and the superior\nperformance of MM-V AP are in agreement with the PTTM lit-\nerature [19, 24].\n5.2. The impact of noise on turn-taking models\nFigure 1 shows the average H/S accuracy on the noisy test data,\nwhere solid colours are V AP (audio-only) and hatched are MM-\nV AP (audio and video). We first consider the models trained on\nclean audio + alignment in green. The plot reveals noise has\na strong impact on PTTM accuracy, most severe in speech and\nmusic where accuracy falls to51-52% (the first two rows of Ta-\nble 1). At high SNR there is a smaller impact from babble noise\nat 76-80% at 10 dB SNR. However, both models are extremely\nsensitive to music and speech interference with 51-52% accu-\nracy at 10 dB SNR, i.e. a random hold/shift prediction.\nFigure 2 shows average H/S prediction accuracy at each\nSNR for V AP and MM-V AP. The higher the SNR, the lower the\nnoise energy relative to the speech signal (equal at 0 dB). Above\n0 dB, the average accuracy increases as the SNR increases. Be-Table 1: Average hold/shift prediction accuracy (%, \u00afx=mean all SNRs) across 5-folds of cross-validation of VAP (audio, A), MM-VAP\n(audio+video, A+V), and video-only (V) models on the same test set at varying noise levels (2.5 dB increments, 5 dB shown for brevity).\nTraining is conducted on the clean and augmented (aug., 25% of sessions corrupted with 0 dB noise) audio and ASR alignments.\nModel Training Input Modality Test set + babble @SNR dB =Test set + speech @SNR dB =Test set + music @SNR dB =Clean\nAudio Alignment -10 -5 0 5 10 \u00afx -10 -5 0 5 10 \u00afx -10 -5 0 5 10 \u00afx \u221e\nVAP clean clean A 61 50 58 69 76 60 51 51 51 51 52 51 51 50 51 51 52 51 80\nMM-VAP clean clean A+V 57 56 68 76 80 68 53 52 52 53 54 52 53 51 51 52 53 52 84\nVAP aug. clean A 49 50 65 74 77 63 51 59 65 66 64 61 55 58 62 66 65 61 84\nMM-VAP aug. clean A+V 61 62 75 79 80 71 66 69 74 74 75 72 67 68 74 76 74 72 86\nVAP aug. aug. A 50 50 55 55 56 53 50 50 50 49 49 50 50 50 50 50 50 50 57\nMM-VAP aug. aug. A+V 55 56 57 58 58 57 50 50 50 50 49 50 50 50 50 50 50 50 57\nvideo-only \u2014 clean V 66 66 66 66\nbabble speech music clean\nNoise type\n50\n55\n60\n65\n70\n75\n80\n85\n90Mean balanced accuracy (%)\nHold/shift Accuracy by Noise T ype\naudio + align.\nclean + clean\naug. + clean\naug. + aug.\nmodel (modality)\nVAP (A)\nMM-VAP (A + V)\nFigure 1: Average hold/shift prediction accuracy of VAP (audio-\nonly) and MM-VAP (audio+video) models in each noise type.\nThe average is across the -10 dB, +10 dB SNR range.\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\nSNR\n50\n55\n60\n65\n70\n75\n80\n85\n90Mean Balanced Accuracy (%)\n+10 dB GAIN\n+3.75 dB GAIN\nHold/Shift Accuracy by SNR\naudio + align.\nclean + clean\naug. + clean\naug. + aug.\nmodel (modality)\nVAP (A)\nMM-VAP (A + V)\nFigure 2: Average hold/shift prediction accuracy of VAP (audio-\nonly) and MM-VAP (audio+video) models at each SNR. An av-\nerage accuracy in speech, music and babble noise is shown.\nlow 0 dB, accuracy levels off to 50-54%. The effective SNR\ngain of MM-V AP over V AP is +3.75 dB. A breakdown of aver-\nage accuracy by noise type reveals the superior performance of\nMM-V AP originates from accuracy in babble noise, as accuracy\nin music and speech is always 51-54% (Table 1).\n5.3. Training with noisy examples\nWe re-train both models on the augmented training set and show\nH/S prediction accuracy by noise type in blue in Figure 1. There\nis a clear performance increase, e.g. average accuracy in music\nis 61% for V AP and72% for MM-V AP, up from51-52% (p <\n0.001). A large gap in model accuracy has also emerged.\nThe blue lines in Figure 2 show performance at each SNR.\nAt -10 dB SNR, the average accuracy of V AP is 50% whereas\nMM-V AP is 65% (p <0.01). The effective SNR gain of MM-\nV AP has increased to +10 dB and the extreme sensitivity to\nspeech and music noise is reduced. At +10 dB SNR of speech\nnoise, the average accuracy is 64% for V AP and75% for MM-\nV AP (second two rows of Table 1), up from51% and 52% pre-\nviously (p < 0.01). At -10 dB SNR, V AP has a 50% accu-\nracy. On the other hand, MM-V AP backs off to the accuracy\nof a PTTM model trained exclusively on video features (67%\nvs 66% in -10 dB music noise). The only difference in input is\nMM-V AP receives visual features. However, it achieves supe-\nrior performance in all types of noise and across the full range\nof SNRs, and performance is similar to a video-only model in\nextreme (-10 dB) noise. This demonstrates that it exploits visual\ninformation to improve accuracy in acoustic interference.\nTable 2: H/S prediction accuracy (%) of MM-VAP trained with\nspeech, babble, music, or a mixture (augmented) at 0 dB SNR.\nTest set, 0 dB SNR\nbabble music speech clean\nTrain set\nbabble 71 51 52 85\nmusic 58 75 57 86\nspeech 70 77 76 86\naugmented 75 74 74 86\n5.4. Predicting a shift between speakers\nWe show predictions during a shift in Figure 3 (MM-V AP solid,\nV AP dashed) by plotting speaker 1 probability in the next 1.2-2\nseconds (greater than 0.5 highlighted red). In clean audio (B),\nthe shift between speakers is predicted ahead of time by MM-\nV AP. In 0 dB babble noise (C), MM-V AP predicts the shift,\nwhereas V AP always outputs approx 0.5, demonstrating the im-\npact of noise. MM-V AP predictions in noise (C + D) resem-\nble predictions in clean audio (B), further demonstrating the ex-\nploitation of visual cues by MM-V AP.\n5.5. Generalisation to noise unseen during training\nWe investigate if MM-V AP performance extends to types of\nnoise unseen during training. We retrain three times with ei-\nther 0 dB SNR speech, music or babble noise in 25% of train-\ning sessions. The balanced accuracy of H/S prediction in TableA\n0.0\n0.5\n1.0\nclean\nB\n0.0\n0.5\n1.0 C\n960 962 964 966 968\nTime [seconds]\n0.0\n0.5\n1.0 D\nspeaker\nspeaker 0\nspeaber 1\naudio + align.\nclean + clean\naug. + clean\nmodel (modality)\nVAP (A)\nMM-VAP (A + V)\nShift Prediction in VAP and MM-VAP\nSpeaker 1 Probability\n0dB babble\nFigure 3: Model output during a shift in the Candor corpus\n(unseen during training). Speaker 0 (grey): \u201dSo uhm would you\nfeel comfortable telling me about what you do for a living?\u201d,\nSpeaker 1 (red) \u201dUhm yeah well for the most part I test and\ntweak algorithms\u201d. VAP and MM-VAP model output is shown\nin clean speech (B) and 0 dB babble speech (C) and (D).\n2 shows that when the training set includes speech or babble\nnoise, accuracy is poor in other types of noise: e.g. 71% for\nthe babble-trained model in babble noise but only 54% in mu-\nsic. However, when trained on speech noise, similar accuracy\nis obtained in all types of noise 74 \u2212 75% (Table 2, second-last\nrow). This suggests training on a speech signal with an interfer-\ning speaker \u2018forces\u2019 MM-V AP to exploit visual cues, leading to\nbetter generalisation when new types of noise are encountered.\n5.6. The critical role of the alignment in training\nFinally, we re-train both models using the augmented alignment\nobtained from Amazon ASR. The orange bars in Figure 1 reveal\na stark fall in accuracy to 50 \u2212 58% and Figure 2 shows that\nthis is consistent across all SNRs. The drop in performance to\nthe 50% range indicates models are comparable to a random\nhold/shift prediction. This is due to alignment and transcription\nerrors that ASR experiences in noise [31]. PTTM training is\ntherefore dependent on accurate transcriptions which are either\nmanual [17] or obtained from ASR and clean recordings [24].\n6. Discussion\nOur results show PTTMs do not maintain high accuracy in types\nof noises they are likely to encounter in-the-wild (Section 5.2).\nWe found slight (10 dB SNR) interference from music and\nother speakers caused accuracy to plummet to 50%. Includ-\ning noise during training dramatically increased performance\n(Section 5.3). This is a common technique in A VSR [16] that\nhas not previously been considered in the turn-taking literature.\nReturning to our first research question, our findings therefore\nshow that the PTTMs in this work are not robust to noise with-\nout the prior inclusion of noise in training.\nThe initial poor performance shows MM-V AP does not ex-\nploit visual cues to their fullest extent (Section 5.2) without ap-\npropriate training. Training on augmented data led to consider-\nably more improvement in the accuracy of MM-V AP than V AP\nacross the SNR range. At extreme levels of noise interference\n(e.g. -10 dB music noise) V AP fails (50%, i.e. chance) but MM-\nV AP backs off to the accuracy of the video-only model (67% vs\n66%, Section 5.3). This highlights the role visual cues play in\nachieving robust H/S accuracy in noise.\nIf MM-V AP is truly able to utilise visual cues to overcome\nnoise, this should generalise to new sources of noise. However,\nwe found that this was only true for the MM-V AP model trained\non speech-noise (Section 5.5). Our working hypothesis is that\nmixing in random speakers over the speech signal training pro-\ncess \u2018forces\u2019 MM-V AP to use visual information, rather than\nrelying on the acoustic properties of the noise. Thus, returning\nto our second research question, PTTMs can achieve robust per-\nformance in noise, but this is intimately related to the training\nprocess and does not inherently generalise to new noise types.\nIt is difficult to interpret H/S prediction accuracy numbers\nas no studies have determined a minimum accuracy needed to\nsupport fluid interaction in a human-robot deployment. Taking\nour 80-84% accuracy on clean speech as a baseline and ignor-\ning the many complexities of a real-world deployment includ-\ning hardware, real-time performance and human-machine dia-\nlogue, a robot would make a correct H/S decision 8/10 times.\nBoth MM-V AP and V AP would only be correct 5/10 times in\n10 dB SNR music interference when trained using conventional\nmethods, significantly complicating interaction. This rises to\n6.5/10 for V AP and 7.5/10 for MM-V AP when noise is included\nin training; close to baseline performance, illustrating the power\nof visual cues in achieving robust performance in noise.\nA limitation of our work is we artificially add noise to\nspeech recorded in noise-free settings. Although this is com-\nmonplace [31, 16, 29], it means we do not capture changes to\nverbal and non-verbal [32] communication and turn-taking [33]\nin noise. Another limitation is our use of videoconferencing.\nHowever, no in-person corpora match the size and quality of\nCandor. Finally, our mixing of noise at a constant energy level\nmeans the audio-only V AP model may be partly capitalising on\nthe natural variations in SNR in long recordings of speech.\nTraining techniques for multimodal PTTMs should be fur-\nther explored to ensure their ability to exploit visual cues gener-\nalises to new noise types. As A VSR can exploit visual informa-\ntion in datasets unseen in training [34], A VSR forms a natural\nstarting point for future multimodal PTTMs architectures [16].\nTurn-taking in noise is quite similar to the channel separation\nproblem of speaker diarisation [35] and our findings support\nfurther exploration and the extension to the multimodal setting.\nThere are many other turn-taking events (e.g. backchannels)\nwhich require consideration in future work.\n7. Conclusion\nAccurate turn-taking models are an important milestone on the\npath to viable human-robot interaction. We have conducted the\nfirst exploration of PTTM performance in noise, showing that\nPTTMs are not robust to typical noises a PTTM may encounter\nonce deployed. Training with noise improves overall accuracy\nand enables multimodal PTTM to leverage visual features to\novercome acoustic interference. However, this capability does\nnot necessarily generalise to new types of noise. This issue must\nbe solved prior to their deployment in real-world settings. Our\nwork therefore constitutes a significant step forward in our un-\nderstanding of PTTMs. Future work in PTTM architectures,\ntraining techniques and in-the-wild deployment are therefore\nimperative before deployment to human-robot interaction.8. Acknowledgements\nThis research was conducted with the financial support of\nScience Foundation Ireland under Grant Agreement No.\n13/RC/2106 P2 at the ADAPT SFI Research Centre at Trinity\nCollege Dublin. Amazon provided academic access to the AWS\nTranscribe service.\n9. References\n[1] T. Stivers, N. J. Enfield, P. Brown, C. Englert, M. Hayashi,\nT. Heinemann, G. Hoymann, F. Rossano, J. P. De Ruiter, K.-E.\nYoon et al., \u201cUniversals and cultural variation in turn-taking in\nconversation,\u201d Proceedings of the National Academy of Sciences,\nvol. 106, no. 26, pp. 10 587\u201310 592, 2009.\n[2] P. Indefrey, \u201cThe spatial and temporal signatures of word pro-\nduction components: a critical update,\u201d Frontiers in psychology,\nvol. 2, p. 255, 2011.\n[3] H. Sacks, E. A. Schegloff, and G. Jefferson, \u201cA simplest systemat-\nics for the organization of turn-taking for conversation,\u201dlanguage,\nvol. 50, no. 4, pp. 696\u2013735, 1974.\n[4] G. Skantze, \u201cTurn-taking in conversational systems and human-\nrobot interaction: a review,\u201d Computer Speech & Language ,\nvol. 67, p. 101178, 2021.\n[5] S. Li, A. Paranjape, and C. D. Manning, \u201cWhen can i speak?\npredicting initiation points for spoken dialogue agents,\u201d arXiv\npreprint arXiv:2208.03812, 2022.\n[6] A. Woodruff and P. M. Aoki, \u201cHow push-to-talk makes talk less\npushy,\u201d inProceedings of the 2003 ACM International Conference\non Supporting Group Work, 2003, pp. 170\u2013179.\n[7] G. Skantze, \u201cTowards a general, continuous model of turn-taking\nin spoken dialogue using lstm recurrent neural networks,\u201d in Pro-\nceedings of the 18th Annual SIGdial Meeting on Discourse and\nDialogue, 2017, pp. 220\u2013230.\n[8] M. Roddy, G. Skantze, and N. Harte, \u201cInvestigating speech fea-\ntures for continuous turn-taking prediction using lstms,\u201d in Inter-\nspeech 2018, 2018, pp. 586\u2013590.\n[9] E. Ekstedt and G. Skantze, \u201cHow much does prosody help turn-\ntaking? investigations using voice activity projection models,\u201d in\nProceedings of the 23rd Annual Meeting of the Special Interest\nGroup on Discourse and Dialogue, 2022, pp. 541\u2013551.\n[10] P. Barkhuysen, E. Krahmer, and M. Swerts, \u201cThe interplay be-\ntween the auditory and visual modality for end-of-utterance de-\ntection,\u201d The journal of the Acoustical Society of America , vol.\n123, no. 1, pp. 354\u2013365, 2008.\n[11] K. Onishi, H. Tanaka, and S. Nakamura, \u201cMultimodal voice activ-\nity prediction: Turn-taking events detection in expert-novice con-\nversation,\u201d in Proceedings of the 11th International Conference\non Human-Agent Interaction, 2023, pp. 13\u201321.\n[12] S. O\u2019Connor Russell and N. Harte, \u201cVisual cues enhance\npredictive turn-taking for two-party human interaction,\u201d in The\n63rd Annual Meeting of the Association for Computational\nLinguistics, ACL 2025, Vienna, Austria . [Online]. Available:\nhttp://arxiv.org/abs/2505.21043\n[13] J. Holler, K. H. Kendrick, M. Casillas, and S. C. Levinson, Turn-\ntaking in human communicative interaction . Frontiers Media\nSA, 2016.\n[14] J. Holler and S. C. Levinson, \u201cMultimodal language processing\nin human communication,\u201d Trends in Cognitive Sciences, vol. 23,\nno. 8, pp. 639\u2013652, 2019.\n[15] A. MacLeod and Q. Summerfield, \u201cQuantifying the contribution\nof vision to speech perception in noise,\u201dBritish journal of audiol-\nogy, vol. 21, no. 2, pp. 131\u2013141, 1987.\n[16] B. Shi, W.-N. Hsu, and A. Mohamed, \u201cRobust self-\nsupervised audio-visual speech recognition,\u201d arXiv preprint\narXiv:2201.01763, 2022.\n[17] E. Ekstedt and G. Skantze, \u201cV oice activity projection: Self-\nsupervised learning of turn-taking events,\u201d in Interspeech 2022,\n2022, pp. 5190\u20135194.\n[18] E. Ekstedt, S. Wang, \u00b4Eva Sz\u00b4ekely, J. Gustafson, and G. Skantze,\n\u201cAutomatic evaluation of turn-taking cues in conversational\nspeech synthesis,\u201d inINTERSPEECH 2023, 2023, pp. 5481\u20135485.\n[19] K. Inoue, B. Jiang, E. Ekstedt, T. Kawahara, and G. Skantze,\n\u201cMultilingual turn-taking prediction using voice activity projec-\ntion,\u201d in Proceedings of the 2024 Joint International Conference\non Computational Linguistics, Language Resources and Evalua-\ntion (LREC-COLING 2024), Torino, Italia, May 2024.\n[20] T. Baltru \u02c7saitis, P. Robinson, and L.-P. Morency, \u201cOpenface: an\nopen source facial behavior analysis toolkit,\u201d in2016 IEEE winter\nconference on applications of computer vision (WACV) . IEEE,\n2016, pp. 1\u201310.\n[21] J. J. Godfrey, E. C. Holliman, and J. McDaniel, \u201cSwitchboard:\nTelephone speech corpus for research and development,\u201d in\nAcoustics, speech, and signal processing, ieee international con-\nference on, vol. 1. IEEE Computer Society, 1992, pp. 517\u2013520.\n[22] M. Roddy and N. Harte, \u201cNeural generation of dialogue response\ntimings,\u201d in Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics , D. Jurafsky, J. Chai,\nN. Schluter, and J. Tetreault, Eds., Jul. 2020, pp. 2442\u20132452.\n[23] S. O. Russell, I. Gessinger, A. Krason, G. Vigliocco, and N. Harte,\n\u201cWhat automatic speech recognition can and cannot do for con-\nversational speech transcription,\u201d Research Methods in Applied\nLinguistics, vol. 3, no. 3, p. 100163, 2024.\n[24] S. O\u2019Connor Russell and N. Harte, \u201cTowards multimodal turn-\ntaking for naturalistic human-robot interaction,\u201d in 2nd Interna-\ntional Multimodal Communication Symposium, Frankfurt, Ger-\nmamy, 2024, pp. 91\u201392.\n[25] A. Reece, G. Cooney, P. Bull, C. Chung, B. Dawson, C. Fitz-\npatrick, T. Glazer, D. Knox, A. Liebscher, and S. Marin, \u201cThe can-\ndor corpus: Insights from a large multimodal dataset of naturalis-\ntic conversation,\u201d Science Advances, vol. 9, no. 13, p. eadf3197,\n2023.\n[26] T. Afouras, J. S. Chung, and A. Zisserman, \u201cLrs3-ted: a\nlarge-scale dataset for visual speech recognition,\u201d arXiv preprint\narXiv:1809.00496, 2018.\n[27] D. Snyder, G. Chen, and D. Povey, \u201cMusan: A music, speech, and\nnoise corpus,\u201d arXiv preprint arXiv:1510.08484, 2015.\n[28] I. Loshchilov, \u201cDecoupled weight decay regularization,\u201d arXiv\npreprint arXiv:1711.05101, 2017.\n[29] Z. Lin and N. Harte, \u201cUncovering the visual contribution in audio-\nvisual speech recognition,\u201d in ICASSP 2025 - 2025 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2025.\n[30] Student, \u201cThe probable error of a mean,\u201d Biometrika, vol. 6, no. 1,\npp. 1\u201325, 1908.\n[31] R. Marxer, J. Barker, N. Alghamdi, and S. Maddock, \u201cThe im-\npact of the lombard effect on audio and visual speech recognition\nsystems,\u201d Speech communication, vol. 100, pp. 58\u201368, 2018.\n[32] J. Trujillo, A. \u00a8Ozy\u00a8urek, J. Holler, and L. Drijvers, \u201cSpeakers ex-\nhibit a multimodal lombard effect in noise,\u201d Scientific Reports ,\nvol. 11, no. 1, p. 16721, 2021.\n[33] A. S\u00f8rensen, M. Fereczkowski, and E. MacDonald, \u201cEffects of\nnoise and l2 on the timing of turn taking in conversation,\u201d ser.\nProceedings of the International Symposium on Audiological and\nAuditory Research, vol. 7, 2020, pp. 86\u201392.\n[34] G. Sterpu, C. Saam, and N. Harte, \u201cHow to teach dnns to\npay attention to the visual modality in speech recognition,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-\ncessing, vol. 28, pp. 1052\u20131064, 2020.\n[35] T. Chen, Q. Wang, B. Wu, M. Itani, E. S. Eskimez, T. Yoshioka,\nand S. Gollakota, \u201cTarget conversation extraction: Source separa-\ntion using turn-taking dynamics,\u201d in Interspeech 2024, 2024, pp.\n3550\u20133554.",
      "arXiv:2505.23746v1  [cs.AI]  29 May 2025\nComparative of Genetic Fuzzy regression\ntechniques for aero-acoustic phenomenons\nHugo Henry1[0009\u22120001\u22122530\u22123521] and Dr. Kelly Cohen4[0000\u22120002\u22128655\u22121465]\nUniversity of Cincinnati, Cincinnati OH 45221, USA\nhenryho@mail.uc.edu\ncohenky@ucmail.uc.edu\nAbstract. This study investigates the application of Genetic Fuzzy Sys-\ntems (GFS) to model the self-noise generated by airfoils, a key issue\nin aeroacoustics with significant implications for aerospace, automotive,\nand drone applications. Using the publicly available \"Airfoil Self Noise\"\ndataset, various fuzzy regression strategies are explored and compared.\nThe paper evaluates a brute-force Takagi-Sugeno-Kang (TSK) fuzzy sys-\ntem with high rule density, a cascading Genetic Fuzzy Tree (GFT) ar-\nchitecture, and a novel clustered approach based on Fuzzy C-Means\n(FCM) to reduce the model\u2019s complexity. This highlights the viability\nof clustering-assisted fuzzy inference as an effective regression tool for\ncomplex aero-acoustic phenomena.\nKeywords: Fuzzy logic\u00b7 Regression \u00b7 Cascading systems\u00b7 Clustering \u00b7\nAI.\n1 Introduction\nAirfoils have a lot of application, especially in aerospace but in a more general\nsense in aerodynamics, including aerospace, automotive and naval industry. Even\nthough they have a lot of use and a lot of advantages, there is also drawbacks\nto the extensive use of airfoils. One of the most notable is vibrations, an airfoil\npossesses different vibration modes depending on the geometry and the materi-\nals of the airfoil. Vibration creates two unwanted phenomena, constraints and\nresonance on the structure potentially leading to problems in structural integrity\nand noise for the people near the airfoil.\nThe noise in particular is something that have to be studied in order for the\nfoil to be used in crowded area, notably helicopters, drones or formula cars.\n2 Survey Methodology\nThisstudywillusethedataset\"AirfoilSelfNoise\"[1]foundontheUCIDatabase.2 H. Henry and K. Cohen\n3 analysis of the dataset\nThe dataset used for this regression is composed of 1503 points along 6 features,\n5 inputs and 1 output:\n\u2013 Frequency \u2208 [50 \u2212 20000] Hz\n\u2013 angle of attack\u2208 [0 \u2212 22.2]\u00b0 (discrete set)\n\u2013 chord length\u2208 [0.254 \u2212 0.3048] (discrete set)\n\u2013 free stream Velocity\u2208 [31.7 \u2212 71.3] (discrete set)\n\u2013 suction side displacement thickness\u2208 [0.00040068 \u2212 0.0584]\n\u2013 Noise in dB\u2208 [103.38 \u2212 140.987]\nFor further insight, the dataset underwent a clustering methodology, here we\nused the Fuzzy C-means (FCM) alongside the elbow method to determine the\nnumber of clusters present in this dataset. Referring figure1.\nFig. 1.Clustering of the dataset\nBased on this method, we chose to use 15 clusters in the following regressions\nin part 4.3.Title Suppressed Due to Excessive Length 3\n4 Fuzzy regressions\n4.1 Brute force system\nThis dataset containing 5 inputs create a huge space of rules possibles for the\nsystem and approaches the limits for a simple crossover based Genetic Algorithm\n(GA).\nIndeed, the system used here is composed of 5 membership functions per\ninput and as such a grand total of 3125 rules to be encoded in the chromosome.\nUsing a TSK (Takagi Sugeno Kang) 1st order, and triangular membership\nfunctions, we attain a total of 18825 parameters as shown in equation 1\nnumber_of_parameters = 3125\u2217 6 + 5\u2217 5 \u2217 3 = 18825 (1)\nThis huge chromosome highly constraints the size of the population and the\nnumber of generation in order to keep a reasonable training time. For the rest\nof this section, the population size will be 50 and 100 generation of training.\nFig. 2.Training set (blue) and prediction\n(red)\nFig. 3.Testing set (blue) and prediction\n(red)\nWe can see that the fitting seems quite adequate for the dataset despite\nthe pretty low number of generations and and population size. indeed we can\nconfidently state that the GA didn\u2019t finish optimizing by the few stray training\nprediction and the mere 8 fitness improvement over the duration of training. 2,3\nand 4.4 H. Henry and K. Cohen\nFig. 4.Fitness evolution\n4.2 Cascading systems\nTSK 0 orderAs shown previously, it is crucial to reduce the number of rules\nin order to reduce the number of parameters and improve training efficiency. As\nsuch, we tried a Genetic Fuzzy Tree (GFT) architecture instead of the brute\nforce to reduce the number of rules to train. This GFT will use the architecture\npresented in figure 5.\nFig. 5.Clustering of the dataset\nIn order to compare, this inference method will use 3 membership function\nper input and then 5 membership functions per input. allowing comparison with\npart 4.1.\nWe can see that the prediction is very longitudinal with very few change\nwhich would mean that the different rules do not really intersect with each otherTitle Suppressed Due to Excessive Length 5\nFig. 6.Training set (blue) and prediction\n(red)\nFig. 7.Testing set (blue) and prediction\n(red)\nFig. 8.Fitness evolution\nandimpliesaverygranulardataset.assuchweobservemoreofatrendprediction\nthan a true prediction. Referring figures 6 and 7\nAlthough this prediction do not make physical sense due to the nature of\nthe output being in Decibels and therefore a linear trend line over a logarithmic\nvalue dot make sense we can see the work of the GFT trying to find the best\ndistribution. Referring figure 8.\nThe GFT losing explainability because of the intermediate nodes it is very\ndifficulttounderstandandthereforedebuggsucharesultoutsideoftryingtoadd\nmore membership functions and rules to solve this non-overlapping situation.\nSo did we with 5 membership functions instead of 3.\nThis time we see a bit more overlapping but the prediction is still very much\nlongitudinal and on the test set, there is even null values which shows us that\nthis model using the 0 order TSK cannot be trusted for this dataset. referring\nfigures 9, 10 and 11.6 H. Henry and K. Cohen\nFig. 9.Training set (blue) and prediction\n(red)\nFig. 10.Testing set (blue) and prediction\n(red)\nFig. 11.Fitness evolution\nTSK 1st order In order to escape this longitudinal trending, we then applied\na 1st order to get slopes and try to avoid this behavior.\nIn the same spirit than for a first order, we ran for 3 then 5 membership\nfunctions.Title Suppressed Due to Excessive Length 7\nFig. 12.Training set (blue) and prediction\n(red)\nFig. 13.Testing set (blue) and prediction\n(red)\nFig. 14.Fitness evolution\nWe can see more coverage of the different behaviors in terms of slopes but\nthe behavior seems to continue being blocked by the granularity of the dataset.\nReferring figures 12, 13 and 14.8 H. Henry and K. Cohen\nFig. 15.Training set (blue) and prediction\n(red)\nFig. 16.Testing set (blue) and prediction\n(red)\nFig. 17.Fitness evolution\nWith those 5 membership functions the Genetic algorithm just breaks down\ncompletely and goes back to trend lining even with modifications on the fitness\nfunction and changing structure in the chromosomes, either it creates a trend\nline or it goes down to a null vector. Referring figures 21, 22 and 23.\nwe can conclude this part upon the fact that the cascading system wasn\u2019t cut\nout for this specific dataset and the existence of the intermediate nodes made it\nvery difficult to debugg even with changing the fitness or architecture.\n4.3 Clustered systems\nIn order to find a solution to this regression that can be a good approximation\nlike the brute force technique while having a good computing efficiency like the\ncascading system, we went back to the the analysis done in part 3 and used the\nclustering to reduce the number of parameters based on the clustering.Title Suppressed Due to Excessive Length 9\nfor that 2 different inference method have been tried. first, we created a\ngaussian membership function centered around the center of each cluster and\nused them as input, this yielded comfortable results but with a significantly\nbigger absolute error across the dataset, it made more sense physically but not\ncomputationally. Referring to figures 18, 19 and 20.\nFig. 18.Training set (blue) and prediction\n(red)\nFig. 19.Testing set (blue) and prediction\n(red)\nFig. 20.Fitness evolution\nThen in order to improve this fitness we got rid of the gaussian membership\nfunction and took full advantage of the fuzzy clustering technique by recomput-\ning the membership value of the point to the different centers therefore giving\nthe degree of activation of the corresponding rule.10 H. Henry and K. Cohen\nThis gave a very good approximation that can be scaled in terms of popula-\ntion size and generations, indeed, the number of parameters being reduced from\nthe 18825 to a mere 90.\nFig. 21.Training set (blue) and prediction\n(red)\nFig. 22.Testing set (blue) and prediction\n(red)\nFig. 23.Fitness evolutionTitle Suppressed Due to Excessive Length 11\n5 Comparison and analysis of the results\nWe can clearly see that in this dataset case, the high granularity of the a few\ninputs lead to a collapse of the cascading systems and the intermediate nodes\nmake it near impossible to fix due to their virtual nature and of the lack of\nphysical representation. In this particular case, the effect of increasing the order\nof the TSK inference or the number of membership functions did not change\nmuch on the output.\nFor the non-cascading systems, the brute-force method has good precision,\nbut the problem of scalability is evident and therefore not a good solution com-\npared to the clustered system. The clustered approximation will still be worse\nthat the brute-forcing in terms of precision if the time is given to is but the\ntraining time is incomparable and for added precision we can increase the num-\nber of clusters while keeping a number of parameters very low compared to the\nbrute-force method.\n6 Conclusion\nThrough this project, we showed the problem that can be encountered with cas-\ncading system, being that if it doesn\u2019t work at first, it will be extremely difficult\nto fix especially toward granularity in the dataset, we also demonstrated first\nhand the problem of scalability of basic Genetic Fuzzy Systems while still show-\ning that the prediction keep good results despite the granularity.\nLastly we showed that clustering have a huge impact in the prediction al-\nlowing for the precision of the brute-force while faster than a cascading system\nand more robust toward the type of data. Moreover, the Fuzzy clustering al-\nlow to be less impacted by outliers but still taking them into accounts with the\nmembership values.\nResults demonstrate that while brute-force models offer accurate predictions\nat high computational costs, cascading systems suffer from limited explainability\nand convergence issues. In contrast, the clustered GFS significantly reduces the\nnumber of parameters while maintaining good predictive accuracy, striking a\nbalance between computational efficiency and physical relevance.\nFurther work would include the use of better clustering methods such as VAT\nand IVAT as well as comparing other optimization methods such as gradient\nbased optimization or swarm theory.\nReferences\n1. https://archive.ics.uci.edu/dataset/291/airfoil+self+noise\n2. Stephane,R.,Turzo,B.,Arjit,S.,Larry,K.,B.,L.:Airfoilself-noisepredictionusing\ndeep neural networks. (2023)https://doi.org/10.1016/j.enganabound.2023.11.\n024",
      "arXiv:2505.23590v1  [cs.CV]  29 May 2025\nJigsaw-R1: A Study of Rule-based Visual Reinforcement\nLearning with Jigsaw Puzzles\nZifu Wang\u2217\nESAT-PSI, KU Leuven\nJunyi Zhu\u2217\nESAT-PSI, KU Leuven\nBo Tang\u2217\nUniversity of Science and Technology of China\nInstitute for Advanced Algorithms Research, Shanghai\nZhiyu Li\nMemory Tensor, Shanghai\nFeiyu Xiong\nMemory Tensor, Shanghai\nJiaqian Yu\nSamsung R&D Institute China, Beijing\nMatthew B. Blaschko\nESAT-PSI, KU Leuven\nAbstract\nThe application of rule-based reinforcement learning (RL) to multimodal large language\nmodels (MLLMs) introduces unique challenges and potential deviations from findings in text-\nonly domains, particularly for perception-heavy tasks. This paper provides a comprehensive\nstudy of rule-based visual RL using jigsaw puzzles as a structured experimental framework,\nrevealing several key findings.Firstly, we find that MLLMs, initially performing near to\nrandom guessing on simple puzzles, achieve near-perfect accuracy and generalize to complex,\nunseen configurations through fine-tuning.Secondly, training on jigsaw puzzles can induce\ngeneralization to other visual tasks, with effectiveness tied to specific task configurations.\nThirdly, MLLMs can learn and generalize with or without explicit reasoning, though open-\nsource models often favor direct answering. Consequently, even when trained for step-by-step\nreasoning, they can ignore the thinking process in deriving the final answer.Fourthly, we\nobserve that complex reasoning patterns appear to be pre-existing rather than emergent,\nwith their frequency increasing alongside training and task difficulty.Finally, our results\ndemonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning\n(SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although\nthese observations are based on jigsaw puzzles and may vary across other visual tasks, this\nresearch contributes a valuable piece of jigsaw to the larger puzzle of collective understanding\nrule-based visual RL and its potential in multimodal learning. The code is available at:\nhttps://github.com/zifuwanggg/Jigsaw-R1.\n\u2217Equal contribution.\n11 Introduction\nPost-training has emerged as a critical step for enhancing the performance of large language models (LLMs).\nA significant contribution in this area is DeepSeek-R1 (Guo et al., 2025), which employs a simple yet effective\nrule-based reinforcement learning (RL) strategy. This approach can mitigate reward hacking (Gao et al., 2023)\nwithout relying on traditional scaffolding techniques (Lightman et al., 2024; Wang et al., 2024a; Xie et al.,\n2024; Xin et al., 2025), and has shown robust generalization capabilities in LLMs across various domains\nsuch as mathematics, coding, common-sense reasoning and logic puzzles (Chen et al., 2025b; Guo et al., 2025;\nLiu et al., 2025a; Xie et al., 2025b).\nDespite these advancements, the application of rule-based RL to multimodal contexts is still in its early stages.\nUnlike purely textual environments such as DeepSeek-R1, multimodal large language models (MLLMs) face\nthe complex challenge of integrating and reasoning over both textual and visual information. This introduces\nunique difficulties and potential deviations from findings in purely linguistic domains (Gandhi et al., 2025;\nGuo et al., 2025; Lee et al., 2025; Liu et al., 2025d; Marjanovi\u0107 et al., 2025; Xie et al., 2025b).\nForinstance, akeyinsightfromDeepSeek-R1isthemodel\u2019snaturalachievementoftest-timescaling(Snelletal.,\n2025) through pure RL, evidenced by increased completion lengths and the emergence of complex reasoning\npatterns (Gandhi et al., 2025), a phenomenon termed the aha moment. Nevertheless, perception-heavy tasks,\nsuch as spatial reasoning, often permit concise answers derived directly from visual understanding. This\ncontrasts sharply with reasoning-intensive problems like mathematics and coding that benefit from extended,\nstep-by-step reasoning. In such perceptual domains, an explicit, lengthy thinking process\u2014characteristic of\nsome rule-based RL successes in text\u2014might even prove detrimental (Jiang et al., 2025).\nThis paper presents a comprehensive study of rule-based RL within the visual domain. Rather than relying\non verifiable answers from existing MLLM benchmarks, we revisit a classic pretext task in computer vision:\nsolving jigsaw puzzles (Carlucci et al., 2019; Chen et al., 2023; Doersch et al., 2015; Du et al., 2020; Noroozi &\nFavaro, 2016). This task (a visual illustration is presented Figure 1) offers a compelling testbed for studying\nrule-based visual RL for several reasons:\nFirstly, jigsaw puzzles inherently provide a ground truth. This allows for the direct generation of rule-based\nrewards across various visual domains, eliminating the need for expensive human annotation.\nSecondly, the complexity of these puzzles is readily adjustable by varying the number of pieces, facilitating a\nstructured experimental framework.\nLastly, solving jigsaw puzzles involves an interplay of step-by-step reasoning and visual perception. The\nhuman approach\u2014iteratively placing pieces while considering local and global visual coherence\u2014provides a\nrich analog for the complex decision-making processes we aim to explore in MLLMs.\nUsing jigsaw puzzles as our experimental framework, this research undertakes an in-depth exploration of\nmultifaceted aspects within rule-based visual RL. Our investigation yields findings that address the following\nkey research questions:\n\u2022 Research Question #1: How do contemporary MLLMs perform on the classic pretext\ntask of jigsaw puzzles?\nWithout task-specific training, the performance of contemporary MLLMs on the simplest jigsaw\npuzzles (i.e., 2x1) is comparable to random guessing. However, fine-tuning enables these models\nto effectively solve such puzzles with near-perfect accuracy. Importantly, these learned abilities\ngeneralize to more complex configurations (e.g., 3x1) not encountered during training.\n\u2022 Research Question #2: Can MLLMs trained to solve jigsaw puzzles develop generalizable\nabilities applicable to other visual tasks?\nTraining models on jigsaw puzzles enables generalization to downstream tasks. The effectiveness of\nthis generalization is dependent on specific task configurations, including puzzle size, question type\nand training dataset.\n2\u2022 Research Question #3: Given that extended reasoning may be detrimental for some\nperceptual tasks, is an explicit thinking process still beneficial when employing rule-based\nvisual RL to solve jigsaw puzzles?\nMLLMs can learn and generalize with or without an explicit reasoning process. However, open-source\nMLLMs typically show stronger performance in direct answering. As a result, even when trained\nto employ step-by-step reasoning, they tend to disregard the thinking process in deriving the final\nanswer.\n\u2022 Research Question #4: Considering that many visual tasks can be solved with concise\noutputs, does the aha moment still emerge in MLLMs trained on jigsaw puzzles?\nThe aha moment, characterized by the sudden emergence of complex reasoning patterns, is not\nobserved. Instead, these patterns are pre-existing within MLLMs and are readily elicited by tasks\nwith inherent reasoning structures, like jigsaw puzzles. Furthermore, the frequency of these reasoning\npatterns demonstrably increases throughout training and in response to greater task difficulty.\n\u2022 Research Question #5: How does supervised fine-tuning (SFT) compare with RL in\nterms of generalization?\nSFT generally demonstrates less effective generalization compared to RL. Besides, initiating training\nwith a SFT cold start phase can make later RL optimization less effective.\n2 Related Work\n2.1 Jigsaw Puzzles\nSince their inception, jigsaw puzzles have been closely linked to learning. Around 1760, British cartographer\nJohn Spilsbury created the first dissected map\u2014an early jigsaw puzzle\u2014specifically to teach geography. More\nrecently, the task of solving jigsaw puzzles has gained considerable attention within the computer vision\ncommunity as a pretext task. The central idea is that neural networks, by training to reassemble images from\nshuffled patches, can develop rich feature representations transferable to various downstream applications.\nFor instance, Doersch et al. (2015); Noroozi & Favaro (2016) propose pre-training Convolutional Neural\nNetworks (CNNs) on jigsaw puzzles before fine-tuning them for downstream tasks such as image classification\nand object detection. Subsequently, Carlucci et al. (2019) employ jigsaw puzzles as a self-supervised\nregularization term, demonstrating strong domain generalization. Du et al. (2020) further advance this\narea by incorporating jigsaw puzzles into a progressive training pipeline for fine-grained classification. Most\nrecently, Chen et al. (2023) investigate the efficacy of jigsaw puzzles in the context of vision transformers\n(ViTs), highlighting the importance for architecture-specific modifications.\n2.2 Rule-based Reinforcement Learning\nIn order to mitigate reward hacking (Gao et al., 2023), DeepSeek-R1 (Guo et al., 2025) adopts a simple\nyet effective rule-based RL approach. This method diverges from traditional scaffolding techniques such as\nprocess reward models (Lightman et al., 2024; Wang et al., 2024a) and Monte Carlo Tree Search (MCTS) (Xie\net al., 2024; Xin et al., 2025), and has proven effective in acquiring reasoning skills transferable across diverse\ndomains, including mathematics, coding, common-sense reasoning and logic puzzles (Chen et al., 2025b; Guo\net al., 2025; Liu et al., 2025a; Xie et al., 2025b).\nBeyond the generalization capabilities, DeepSeek-R1 also demonstrates a natural achievement of test-time\nscaling (Snell et al., 2025), evidenced by increased completion lengths and the sudden emergence of complex\nreasoning patterns, a phenomenon termed the aha moment. However, Xie et al. (2024) find that longer\nresponses do not guarantee better reasoning and Liu et al. (2025d) challenge the notion of sudden emergence,\npositing that these complex behaviors might be inherent in the base model rather than appearing abruptly.\nFurthermore, its reasoning processes reportedly exhibit human-like language processing characteristics (Mar-\njanovi\u0107 et al., 2025). Investigating these further, Gandhi et al. (2025) draw connections to human psychology,\n3developing a framework that categorizes these reasoning patterns into four key cognitive behaviors. Their find-\nings suggest that Qwen2.5 (Qwen et al., 2025) naturally exhibits these behaviors, whereas Llama3.2 (Grattafiori\net al., 2024) initially does not. Adding another dimension to this discussion, Lee et al. (2025) observe that\nthe data format has a significantly greater impact on reasoning behavior than the data domain. Their\nanalysis reveals that models trained on multiple-choice data produce concise answers, whereas those trained\non free-form data tend to be more verbose.\n2.3 Rule-based Visual Reinforcement Learning\nApplying rule-based RL to MLLMs is an emerging field. Unlike models that operate solely on text (e.g.,\nDeepSeek-R1), MLLMs operate in a more complex environment by processing both textual and visual\ninformation. This inherent complexity introduces unique difficulties and potential deviations from established\nfindings in purely linguistic domains. Recently, several contemporary studies have emerged, reporting varied\nfindings across diverse task settings.\nA significant line of exploration, motivated by the success of DeepSeek-R1 in mathematical reasoning, involves\nadapting these rule-based RL techniques to multimodal mathematical tasks (Chen et al., 2025a; Deng et al.,\n2025; Liu et al., 2025b; Meng et al., 2025; Peng et al., 2025; Wang et al., 2025b; Yang et al., 2025). These\nefforts have consistently demonstrated strong out-of-domain generalization. Notably, studies by Chen et al.\n(2025a); Meng et al. (2025) have documented the emergence of the aha moment\u2014the spontaneous exhibition\nof sophisticated reasoning patterns like self-correction\u2014arising from end-to-end RL training.\nHowever, the landscape appears different for visual perception tasks, including visual classification, visual\ngrounding and spatial reasoning. While research in this area (Bai et al., 2025b; Chen et al., 2025d; Lai et al.,\n2025; Li et al., 2025b; Liao et al., 2025; Liu et al., 2025e;c; Shen et al., 2025; Yu et al., 2025a; Zhou et al., 2025)\nalso show robust out-of-distribution generalization, the aha moment has not been observed when employing\ninstruction-tuned models. Indeed, the necessity of explicit reasoning steps for these perception-intensive\ntasks has been questioned, as direct answers may often suffice (Jiang et al., 2025). In fact, work by Lai et al.\n(2025); Li et al. (2025b); Yu et al. (2025a) suggest that training models for direct answering often leads to\nsuperior performance compared to models trained for explicit step-by-step reasoning.\n3 Task Design\nThis section outlines the formulation of jigsaw puzzles in a format suitable for processing by MLLMs.\nSubsequently, we introduce a rule-based reward system designed for RL training. All specific prompts are\ndetailed in Appendix E, and Figure 1 provides a conceptual overview.\n3.1 Jigsaw Puzzle Images\nThe creation of jigsaw puzzles begins with an input image. The image is first partitioned into anm\u00d7n grid\nof patches and the task\u2019s difficulty can be readily adjusted by varying the values ofm and n. Optionally,\na masked region can be added between patches to highlight the grid layout. If the image\u2019s height is not\nperfectly divisible bym, or its width byn, the image is trimmed from the bottom or right edges to ensure\nits dimensions are exact multiples of the patch count. Subsequently, these patches are randomly shuffled to\ncreate the jigsaw puzzle images. To uniquely identify each patch\u2019s location within this grid, position indices\nare assigned sequentially in row-major order, from 1 (top-left) tomn (bottom-right).\n3.2 Question Types\nBased on these shuffled images, we formulate distinct question types. These questions either directly assess\nthe MLLM\u2019s ability to reconstruct the original image or require it to reason about the relative positions\nbetween the shuffled patches in the initial image. An additional question type, assessing both spatial reasoning\nand visual grounding, is presented in Appendix B.\nFull. In this task, MLLMs are required to identify the initial position index for each shuffled patch, thereby\nenabling the reconstruction of the original image. The answer is a list ofmn numbers arranged in anm\u00d7n\n4... For patches currently at positions 2 and 4, \ndetermine their relative position in the original \nimage...\nPair Jigsaw PuzzleFull Jigsaw Puzzle\nShuffle\nOriginal Image Jigsaw Image\n2\u00d72\n...For each patch,determine its correct \nposition index in the original image ...\nThinking: \n<think>...\n</think>\n<answer>4 2 \n1 3</answer>\nNon-thinking: \n1 3 2 4\n3\u00d71\nNon-thinking: \n(B) 2 is on the\nupper left of  4\nThinking: \n<think>...\n</think>\n<answer>(C)...\n</answer>\nOther Sizes\n1\u00d722\u00d71\n4\u00d71\n1 2\n3 4\nFigure 1: An illustration of the task design.\ngrid, where each number corresponds to a shuffled patch and indicates its original position index. The\ncomplexity of this task is thereforemn!.\nPair. For this task, two patches are randomly selected, and the MLLM\u2019s objective is to identify their\nrelative positions in the original image. If the image is divided into a single row (m= 1) or a single column\n(n= 1), only two relative positions are possible (e.g. left/right or top/bottom, respectively). Otherwise, eight\ndistinct relative directions are possible (e.g., top-left, directly above, to the right, bottom-right). This task is\nstructured as a multiple-choice question, requiring the model to output a single letter corresponding to the\ncorrect relative position. Consequently, the task complexity is either 2 or 8, depending on whether the image\nis divided into a single row/column or not.\n3.3 Thinking or Non-thinking\nFor any given question, regardless of its type, we investigate two prompting approaches for MLLMs. One\napproach instructs MLLMs to include an explicit thinking process in their response, similar to the format\nused in DeepSeek-R1 (Guo et al., 2025). The inclusion of explicit reasoning has been shown to improve\ngeneralization across diverse downstream tasks (Hu et al., 2025; Xie et al., 2025b) and is considered valuable\nfor enhancing safety and transparency (Chen et al., 2025f; Wang et al., 2025a). Conversely, as explicit\nstep-by-step reasoning might be detrimental for tasks heavily reliant on visual perception (Jiang et al., 2025),\nwe also explore an alternative: prompting the MLLM to provide the final answer directly, without detailing\nintermediate reasoning. In summary, we examine the following two distinct instructions:\nThinking. MLLMs are instructed to first output their thinking process, which should be enclosed within\n<think> and </think> tags. Subsequently, they must provide the final answer, enclosed within <answer>\nand </answer> tags.\nNon-thinking. MLLMs are prompted to directly output the final answer to the posed question.\n53.4 Rule-based Rewards\nThe reward serves as the primary training signal in rule-based RL. Our reward system consists of two\ncomponents: an accuracy reward and a format reward. The total reward is the sum of these two components.\nAccuracy reward. This reward assesses the correctness of the response. For full questions, the reward\nis calculated as the proportion of correctly identified position indices to the total number of indices (mn),\nresulting in a fractional value between 0 and 1. For pair questions, the reward is binary: 1 for a correct choice\nand 0 otherwise.\nFormat reward. The final answer must be extractable in the prescribed format: a list ofmn integers\narranged in anm\u00d7n grid for full questions or a single letter for pair questions. With thinking instructions,\nthe answer is extracted from within the <answer> and </answer> tags. For non-thinking instructions, it is\nextracted directly from the raw output.\nFurthermore, for thinking, the model must adhere to the instruction of enclosing its reasoning process within\n<think> and </think> tags and the final answer within <answer> and </answer> tags. Each tag must\nappear exactly once and in the correct sequence (the thinking process before the final answer).\nThe format reward is 0.5 if the output adheres to all these requirements, and 0 otherwise.\n4 Experimental Setups\n4.1 Datasets\nCOCO (Lin et al., 2014).This dataset serves as the foundation for training and evaluating jigsaw puzzles.\nWe exclusively use the images and randomly generate the ground truth permutations. For training, we employ\nthe train2014 split, and for testing, we randomly select 1,000 images from the test2014 split.\nCV-Bench (Tong et al., 2024a).This benchmark repurposes standard vision datasets such as COCO\nwith a multimodal context, offering 2,638 test examples. It includes four distinct tasks: spatial relationship\nand object counting for 2D understanding, and depth order and relative distance for 3D understanding.\nMMVP (Tong et al., 2024b).Similar to CV-Bench, MMVP adapts classic vision datasets like Ima-\ngeNet (Deng et al., 2009) to create 300 multimodal questions. This benchmark assesses MLLMs on nine\nfundamental visual patterns, such as orientation, perspective, and structural characteristics.\nSAT (Ray et al., 2024).This synthetic dataset features indoor scenes, from which we exclusively use its\nstatic split. We categorize the original questions into the four task types defined in CV-Bench. For testing,\nwe randomly sample 500 questions per task, yielding a total of 2,000 test questions. The remaining 96,924\nquestions constitute the training set.\nSuper-CLEVR (Li et al., 2023).This is another synthetic dataset containing various vehicle models like\ncars and motorcycles. Following (Chen et al., 2025d), we select 200 images from the test split and adapt the\ndataset as counting problems.\n4.2 Models\nProprietary Models,We evaluate GPT-4.1 (OpenAI, 2025a), GPT-4.1-mini (OpenAI, 2025a), and Claude\n3.5 Haiku (Anthropic, 2024).\nOpen-Source Models. We consider Qwen2-VL-2B-Base (Wang et al., 2024b) and several instruction-tuned\nmodels: Qwen2.5-VL-72B/7B/3B (Bai et al., 2025a), Qwen2-VL-2B (Wang et al., 2024b), and InternVL2.5-\n2B (Chen et al., 2024).\n4.3 Implementation Details\nWe use GRPO (Shao et al., 2024) as the reinforcement learning algorithm. The GRPO iteration\u00b5= 1, the\nKL efficient\u03b2 = 0.04 and the clipping value\u03f5= 0.2. The number of training steps is 1,000 for thinking and\n62,000 for non-thinking. In each training step, 64 unique prompts are processed, with each prompt being\nsampled 8 times to calculate the advantages. The sampling temperature is set to 1, and top-k sampling is\nused withk= 50. The learning rate initiates at 1e-6 and linearly decays to 0.\n5 Experiments\nThis section presents results designed to address the proposed research questions. The main paper focuses on\ninstruction-tuned models; for a discussion regarding Qwen2-VL-2B-Base, please refer to Appendix C.\nResearch Question #1: How Do MLLMs Perform on Jigsaw Puzzles?\nTo answer the question, we first train models on 2x1 jigsaw puzzles using the training split of the COCO\ndataset. To introduce task diversity for non-square puzzles, the piece order is randomly shuffled in 50% of\ninstances (e.g. 2x1 becomes 1x2 and vice verse). We then evaluate model performance on the same question\ntype but with varying puzzle sizes, utilizing the test split of the COCO dataset. Evaluation results for pair\nquestions are shown in Table 1, and comprehensive results for full questions are provided in Appendix A.\nThe training dynamics for pair jigsaw puzzles are illustrated in Figure 2.\nFinding 1.1: MLLMs struggle with jigsaw puzzles before fine-tuning.As demonstrated in Tables 1\nand 7, jigsaw puzzles are notably difficult for MLLMs without task-specific training. Prior to fine-tuning,\neven powerful proprietary models perform at levels comparable to random guessing, struggling even with the\nsimplest jigsaw puzzles (i.e., 2x1).\nFinding 1.2: MLLMs exhibit efficient learning and generalization for jigsaw puzzles after fine-\ntuning. Despite the initial difficulty, MLLMs show a strong capacity to learn and solve these puzzles after\nfine-tuning. For example, the reward progression for Qwen2.5-VL-3B, depicted in Figure 2, indicates rapid\nconvergence to near-perfect accuracy. Notably, models trained exclusively on 2x1 jigsaw puzzles successfully\ngeneralize their learned abilities to larger puzzle sizes beyond the training distribution (e.g., 3x1).\nTakeaways #1.Without task-specific training, modern MLLMs perform no better than random\nguessing on the simplest jigsaw puzzles (i.e., 2x1). Nevertheless, after fine-tuning, they can solve these\npuzzles almost perfectly and can generalize the learned abilities to more complex configurations (e.g.,\n3x1) unseen during training.\n0 200 400 600 800 1000\nTraining Step\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Reward\nAccuracy Reward\nFormat Reward\n0 400 800 1200 1600 2000\nTraining Step\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Reward\nAccuracy Reward\nFormat Reward\n0 200 400 600 800 1000\nTraining Step\n20\n40\n60\n80\n100\n120\n140\n160Length Qwen2.5-VL-7B\nQwen2.5-VL-3B\nQwen2-VL-2B\nInternVL2.5-2B\nFigure 2: The training dynamics of Jigsaw-R1.Left: Rewards of Qwen2.5-VL-3B (thinking).Middle:\nRewards of Qwen2.5-VL-3B (non-thinking).Right: The completion length of various models. All curves are\nexponentially smoothed for visualization.\n7Table 1: Evaluation results on pair jigsaw puzzles with different sizes. For thinking and non-thinking of the\nsame model, the better result is underlined.\nThinking\nMethod 2x1 3x1 4x1 2x2 AVG\nRandom 50.00 50.00 50.00 12.50 40.63\nGPT-4.1 54.10 53.40 54.70 20.70 45.73\nGPT-4.1-mini 61.90 54.50 54.80 20.30 47.88\nClaude 3.5 Haiku 61.00 49.10 51.30 15.20 44.15\nQwen2.5-VL-72B 43.40 50.20 52.80 18.00 41.10\nQwen2.5-VL-7B 49.40 48.70 50.60 15.80 41.12\n+ Jigsaw-R1 97.80\u219148.40 61.70\u219113.00 54.80\u21914.20 15.20\u2193\u22120.60 57.38\u219116.26\nQwen2.5-VL-3B 48.50 47.50 48.80 12.20 39.25\n+ Jigsaw-R1 96.80\u219148.30 58.80\u219111.30 52.20\u21913.40 13.10\u21910.90 55.22\u219115.97\nQwen2-VL-2B 32.80 33.90 32.10 10.30 27.27\n+ Jigsaw-R1 70.30\u219137.50 56.50\u219122.60 48.10\u219116.00 10.70\u21910.40 46.40\u219119.13\nInternVL2.5-2B 44.90 41.90 48.60 9.70 36.28\n+ Jigsaw-R1 99.30\u219154.40 63.00\u219121.10 53.00\u21914.40 13.70\u21914.00 57.25\u219120.97\nNon-thinking\nMethod 2x1 3x1 4x1 2x2 AVG\nRandom 50.00 50.00 50.00 12.50 40.63\nGPT-4.1 53.80 49.70 50.90 16.50 42.73\nGPT-4.1-mini 62.50 52.70 53.90 16.20 46.32\nClaude 3.5 Haiku 31.30 42.40 43.00 13.20 32.47\nQwen2.5-VL-72B 52.50 51.60 55.60 14.20 43.48\nQwen2.5-VL-7B 50.40 49.60 54.20 13.20 41.85\n+ Jigsaw-R1 98.90\u219148.50 65.90\u219116.30 53.90\u2193\u22120.30 14.90\u21911.70 58.40\u219116.55\nQwen2.5-VL-3B 52.20 48.30 48.60 13.70 40.70\n+ Jigsaw-R1 98.80\u219146.60 66.00\u219117.70 53.20\u21914.60 16.80\u21913.10 58.70\u219118.00\nQwen2-VL-2B 50.90 53.60 46.90 9.60 40.25\n+ Jigsaw-R1 98.60\u219147.70 65.00\u219111.40 53.50\u21916.60 12.30\u21912.70 57.35\u219117.10\nInternVL2.5-2B 51.00 48.50 53.50 10.90 40.98\n+ Jigsaw-R1 99.30\u219148.30 63.20\u219114.70 53.40\u2193\u22120.10 12.90\u21912.00 57.20\u219116.22\nResearch Question #2: How Do Jigsaw Puzzles Generalize to Downstream Tasks?\nTo address this question, we evaluate the performance of models trained on jigsaw puzzles across several\ndownstream tasks, including CV-Bench, MMVP, SAT, and Super-CLEVR. Further details regarding these\ndatasets can be found in Section 4.1.\nFinding 2.1: Jigsaw puzzles generalize. Our primary investigation (Table 2) reveals that models\ntrained on jigsaw puzzles generally achieve improved performance on downstream tasks, indicating robust\ngeneralization. Notably, despite being trained exclusively on the COCO dataset for jigsaw puzzles, these\nmodels successfully adapt to spatial reasoning tasks on synthetic image datasets such as SAT and Super-\nCLEVR. Nevertheless, the improvements seen in thinking models might be superficial. They learn to neglect\nthe reasoning process (further discussed inResearch Question #3), and their performance after fine-tuning\nremains inferior to that of non-thinking models that have not undergone fine-tuning.\nFinding 2.2: Jigsaw puzzle configuration impacts generalization. To understand the factors\ninfluencing generalization, we analyze the performance of Qwen2.5-VL-3B under various jigsaw puzzle\nconfigurations:\nPuzzle size: The choice of jigsaw puzzle size significantly affects downstream performance (Table 3). For the\nnon-thinking setting, training on a larger, more challenging jigsaw puzzle size leads to better generalization.\n8Table 2: Evaluation results on downstream tasks. For thinking and non-thinking of the same model, the\nbetter result is underlined.\nThinking\nMethod CV-Bench MMVP SAT Super-CLEVR AVG\nGPT-4.1 83.69 88.66 73.70 52.00 74.52\nGPT-4.1-mini 84.42 82.00 72.00 60.50 74.73\nClaude 3.5 Haiku 73.38 71.33 59.30 48.00 63.00\nQwen2.5-VL-72B 82.98 76.33 71.00 72.00 75.57\nQwen2.5-VL-7B 64.89 72.66 65.85 59.00 65.60\n+ Jigsaw-R1 75.97\u219111.08 77.00\u21914.34 69.15\u21913.30 66.00\u21917.00 72.03\u21916.43\nQwen2.5-VL-3B 63.87 61.66 57.05 48.00 57.64\n+ Jigsaw-R1 69.48\u21915.61 65.00\u21913.34 61.95\u21914.90 47.00\u2193\u22121.00 60.86\u21913.22\nQwen2-VL-2B 51.55 63.33 45.75 55.00 53.91\n+ Jigsaw-R1 59.36\u21917.81 61.33\u2193\u22122.00 53.15\u21917.40 66.00\u219111.00 59.96\u21916.05\nInternVL2.5-2B 56.02 54.66 47.60 15.50 43.44\n+ Jigsaw-R1 60.73\u21914.71 63.67\u21919.01 56.25\u21918.65 46.00\u219130.50 56.66\u219113.22\nNon-thinking\nMethod CV-Bench MMVP SAT Super-CLEVR AVG\nGPT-4.1 81.95 86.33 73.30 55.75 74.33\nGPT-4.1-mini 81.46 80.66 69.90 65.50 74.38\nClaude 3.5 Haiku 63.87 67.00 56.90 37.00 56.19\nQwen2.5-VL-72B 82.83 78.00 72.00 97.50 82.58\nQwen2.5-VL-7B 79.87 78.00 69.55 92.50 79.98\n+ Jigsaw-R1 80.44\u21910.57 77.67\u2193\u22120.33 69.80\u21910.25 92.50 80.10\u21910.12\nQwen2.5-VL-3B 70.35 66.00 65.50 76.50 69.59\n+ Jigsaw-R1 73.57\u21913.22 70.00\u21914.00 65.65\u21910.15 83.50\u21917.00 73.18\u21913.59\nQwen2-VL-2B 64.89 66.33 61.65 72.00 66.21\n+ Jigsaw-R1 67.40\u21912.51 66.00\u2193\u22120.33 64.70\u21913.05 72.50\u21910.50 67.65\u21911.44\nInternVL2.5-2B 65.84 66.00 61.50 51.00 61.09\n+ Jigsaw-R1 67.36\u21911.52 72.00\u21916.00 61.30\u2193\u22120.20 83.50\u219132.50 71.03\u21919.94\nFurthermore, employing a curriculum learning approach that mixes different puzzle sizes (e.g. 3x1\u21924x1)\nproves more effective than training exclusively with a single size.\nQuestion type:Pair jigsaw puzzles result in superior generalization on downstream tasks compared to full\njigsaw puzzles (Table 4). We believe that this advantage stems from the analogy of pair jigsaw puzzles to\ndownstream tasks (e.g. requiring models to answer multi-choice questions and explicitly asking them to\nreason about spatial relationships between visual elements).\nTraining dataset:As demonstrated in Table 5, aligning the training dataset with the target domain yields\nimproved performance. For example, training directly on the SAT dataset enhances performance on SAT\ntasks. Given that jigsaw puzzles are label-free, it is even feasible to train on the test set of SAT for further\nperformance gains.\nTakeaways #2.Training on jigsaw puzzles can induce generalization to downstream tasks. The\ndegree of generalization is affected by specific task configurations, including puzzle size, question type\nand training dataset.\n9Table 3: Averaged downstream task performance of Qwen2.5-VL-3B when trained on different jigsaw puzzle\nsizes. 2x1\u21923x1 (3x1\u21924x1): Mixing 2x1 and 3x1 (3x1 and 4x1) jigsaw puzzles in a curriculum setting. For\nthinking and non-thinking of the same model, the better result is underlined.\nThinking\n2x1 3x1 4x1 2x2 2x1 \u21923x1\n60.86 58.86 59.05 58.79 61.92\nNon-thinking\n2x1 3x1 4x1 2x2 3x1 \u21924x1\n73.18 74.95 73.03 72.68 75.29\nTable 4: Evaluation results on downstream tasks when Qwen2.5-VL-3B (non-thinking) is trained on different\nquestion types.\nQuestion Type CV-Bench MMVP SAT Super-CLEVR AVG\nFull 71.76 69.67 65.20 84.00 72.65\nPair 73.57\u21911.81 70.00\u21910.33 65.65\u21910.45 83.50\u2193\u22120.50 73.18\u21910.53\nTable 5: Evaluation results on downstream tasks when Qwen2.5-VL-3B (non-thinking) is trained on different\ndatasets.\nTraining Dataset CV-Bench MMVP SAT Super-CLEVR AVG\nCOCOtrain 73.57 70.00 65.65 83.50 73.18\nSATtrain 72.29\u2193\u22121.28 68.00\u2193\u22122.00 67.00\u21911.35 82.00\u2193\u22121.50 72.32\u2193\u22120.86\nSATtrain + SATtest 72.46\u2193\u22121.11 68.33\u2193\u22121.67 67.40\u21911.75 81.00\u2193\u22122.50 72.29\u2193\u22120.89\nResearch Question #3: Thinking or Non-thinking?\nFinding 3.1: MLLMs can learn from jigsaw puzzles and generalize to downstream tasks with\nor without explicit reasoning.As demonstrated by the jigsaw puzzle results (Tables 1 and 7), models\ncan effectively learn with rule-based visual RL, whether or not an explicit reasoning chain is generated.\nImportantly, these learned capabilities can be generalized to various downstream tasks (Table 2).\nFinding 3.2: Open-source MLLMs often benefit from direct answering, while proprietary\nmodels tend to perform better with explicit reasoning.Consistent with observations in (Li et al.,\n2025b; Jiang et al., 2025), our results on jigsaw puzzles (Tables 1 and 7) and downstream tasks (Table 2)\nconfirm that open-source models tend to achieve stronger results when prompted to output the answer\ndirectly. Indeed, even after fine-tuning, models that adopt a reasoning process show weaker generalization on\ndownstream tasks than direct-answering models that have not undergone fine-tuning. Conversely, we find\nthat proprietary models generally demonstrate improved performance when an explicit reasoning process\nprecedes the final answer. It is important to note, however, that this does not necessarily mean proprietary\nmodels are inherently stronger. For example, Claude 3.5 Haiku performs comparably to Qwen2.5-VL-3B on\ndownstream tasks.\nFinding 3.3: MLLMs can neglect the reasoning process after fine-tuning.As illustrated in Figure 2\n(Right), the completion length of InternVL2.5-2B significantly decreases during training. This occurs because\nthe model increasingly circumvents step-by-step reasoning and often includes only the final answer in its\nthinking process (see examples in Appendix G). Conversely, while Qwen models do present explicit reasoning\nsteps, these steps may not consistently inform the derivation of the final answer (examples are provided\nin Appendix F). To quantify this, we utilized GPT-4.1 to assess the consistency between the reasoning process\nof Qwen2.5-VL-3B and its final answer. As Figure 3 (Left) demonstrates, although the model\u2019s final answer\nbecomes more accurate with training, its reasoning chain becomes progressively more inconsistent.\n10Takeaways #3.MLLMs can learn and generalize, irrespective of whether an explicit reasoning\nprocess is included. Nevertheless, open-source MLLMs usually excel at direct answering. Consequently,\neven when trained to utilize step-by-step reasoning, they may ignore the thinking process when\ndeiriving the final answer.\n0 200 400 600 800 1000\nTraining Step\n50\n60\n70\n80\n90Accuracy\nAccuracy\nConsistency\n0 200 400 600 800 1000\nTraining Step\n0\n10\n20\n30\n40\n50\n60Keyword Frequency\n2x1 3x1 4x1 2x2\nJigsaw Puzzle Size\n3400\n3600\n3800\n4000\n4200\n4400\n4600\n4800Keyword Frequency\n48\n50\n52\n54\n56\n58\n60\n62\n64\nConsistency\nFigure 3: Left: Accuracy of the final answer and consistency of the reasoning process during training.\nMiddle: Evolution of keyword frequency throughout the training process.Right: Comparison of keyword\nfrequency when trained on different jigsaw puzzle sizes. All results are demonstrated using Qwen2.5-VL-3B.\nResearch Question #4: Does the Aha Moment Emerge?\nFinding 4.1: Complex reasoning patterns are pre-existing in MLLMs.The aha moment is often\nassociated with the emergence of complex reasoning patterns Guo et al. (2025). While contemporary studies\nfocusing on perception-heavy tasks like visual classification, visual grounding and spatial reasoning (Bai\net al., 2025b; Chen et al., 2025d; Lai et al., 2025; Li et al., 2025b; Liao et al., 2025; Liu et al., 2025e;c; Shen\net al., 2025; Yu et al., 2025a; Zhou et al., 2025) typically do not observe these patterns in instruction-tuned\nmodels, our investigation into jigsaw puzzles reveals a distinct phenomenon. Although the completion length\nis not increasing, as illustrated in Figure 2 (Right), we find that all these models, including InternVL2.5-2B\nand also Qwen2-VL-2B-Base (see Appendix C), exhibit complex reasoning patterns, such as verification and\nbacktracking, even before training starts. Indeed, throughout the training process, we successfully identify all\nfour cognitive behaviors as defined in (Gandhi et al., 2025).\nExamples of the Four Cognitive Behaviors When Solving Jigsaw Puzzles\nVerification: \"Let me check the numbers ...\"\nBacktracking: \"After re-evaluating the patches, I observe the following ...\"\nSubgoal Setting:\"Let\u2019s try to match the descriptions of the patches to ...\"\nBackward Chaining:\"I can work backwards to find the correct placement ...\"\nFinding 4.2: Complex reasoning patterns evolve during fine-tuning.To monitor the evolution\nof these behaviors, we track the frequency of keywords indicative of backtracking and backward chaining\n(detailed in Appendix D). As depicted in Figure 3 (Middle), the occurrence of these keywords demonstrates a\nsteady and significant increase throughout the training process.\nFinding 4.3: Complex reasoning patterns emerge more frequently with harder jigsaw puzzles.\nTo further investigate these reasoning patterns, we plot the frequency of these keywords as the Qwen2.5-VL-3B\nmodel is trained on jigsaw puzzles of varying sizes, as shown in Figure 3 (Right). Our analysis reveals a clear\ntrend: the frequency of these keywords increases when the model is trained on more challenging (i.e., larger)\njigsaw puzzles.\n11Takeaways #4:Rather than emerging abruptly, complex reasoning patterns are intrinsic within\nMLLMs. Tasks that inherently require structured reasoning, such as jigsaw puzzles, readily activate\nthese pre-existing patterns. Furthermore, they become demonstrably more prominent both throughout\nthe training process and when MLLMs face more challenging jigsaw puzzles.\nResearch Question #5: SFT or RL?\nThis section evaluates the generalization capabilities of SFT in comparison to RL. For these experiments,\nSFT data for thinking is curated via rejection sampling, while non-thinking utilizes ground-truth data.\nFinding 5.1: SFT exhibits weaker generalization than RL.As demonstrated in Table 6, applying\nSFT to either the reasoning chain (thinking) or directly to ground-truth answers (non-thinking) can yield\nsome generalization. However, it is generally less effective than RL.\nFinding 5.2: A cold start phase with SFT preceding RL can be detrimental.Compared to a\nsingle-stage RL process, a two-stage pipeline that incorporates a cold start phase with SFT prior to RL can\nhelp models learn specific output formats (Guo et al., 2025). However, this is not essential in our experimental\nsetting, as indicated by the rapid increase in rewards (Figure 2). More importantly, we observe that this cold\nstart phase can diminish the effectiveness of subsequent RL optimization (Table 6).\nTakeaways #5.SFT typically shows weaker generalization compared to RL. Additionally, initiating\nthe training process with a SFT cold start phase may limit the efficacy of subsequent RL optimization.\nTable 6: Evaluation results of SFT and RL models on downstream tasks. For thinking and non-thinking of\nthe same model, the better result is underlined.\nThinking\nMethod CV-Bench MMVP SAT Super-CLEVR AVG\nQwen2.5-VL-3B 63.87 61.66 57.05 48.00 57.64\n+ Jigsaw-R1 69.48\u21915.61 65.00\u21913.34 61.95\u21914.90 47.00\u2193\u22121.00 60.86\u21913.22\n+ SFT 66.41\u21912.54 64.00\u21912.34 58.85\u21911.80 42.00\u2193\u22126.00 57.81\u21910.17\n+ SFT + Jigsaw-R1 68.96\u21915.09 63.66\u21912.00 60.05\u21913.00 43.00\u2193\u22125.00 58.91\u21911.27\nNon-thinking\nMethod CV-Bench MMVP SAT Super-CLEVR AVG\nQwen2.5-VL-3B 70.35 66.00 65.50 76.50 69.59\n+ Jigsaw-R1 73.57\u21913.22 70.00\u21914.00 65.65\u21910.15 83.50\u21917.00 73.18\u21913.59\n+ SFT 71.27\u21910.92 67.73\u21911.73 62.20\u2193\u22123.30 76.75\u21910.25 69.48\u2193\u22120.11\n+ SFT + Jigsaw-R1 71.04\u21910.69 66.67\u21910.67 62.00\u2193\u22123.50 80.00\u21913.50 69.92\u21910.33\nLimitations and Future Work\nVisual reasoning models. Recent advancements from OpenAI, particularly the o3 and o4-mini mod-\nels (OpenAI, 2025b), have shown significant promise in reasoning with images for enhanced perception. While\nour work does not incorporate these visual reasoning models (Li et al., 2025a; Liu et al., 2025f; Qi et al., 2025;\nSu et al., 2025; Wang et al., 2025c), we believe jigsaw puzzles are an ideal candidate for exploring rule-based\nvisual RL in this context due to their inherent reliance on image-based reasoning. For an early exploration, we\nconduct small-scale experiments using the ChatGPT console, where these models are equipped with tool-use\ncapabilities. Our preliminary experiments indicate that OpenAI o3 can effectively solve 2x2 jigsaw puzzles,\nsubstantially outperforming other models considered in this paper. However, it still faces challenges with\nmore complex puzzles (e.g. 3x3), highlights areas for further investigation.\n12Multimodal generative models.Our study does not consider models capable of both understanding and\ngenerating multimodal content (Chen et al., 2025c;e; Hurst et al., 2024; Team, 2024; Wu et al., 2024; Xie\net al., 2025a). A promising future research direction involves integrating our approach with these advanced\nmodels. Enabling them to generate their own inputs could reduce dependence on external datasets like\nCOCO and create an autonomous environment to learn from experience (Silver & Sutton, 2025).\nTest-time training.We have demonstrated that aligning the training dataset with the target domain can\nyield enhanced performance. Jigsaw puzzles inherently provide readily available annotations, making them\nsuitable for direct training on the test set during test time. Therefore, exploring the use of jigsaw puzzles as\na technique for test-time training (Akyurek et al., 2024; Behrouz et al., 2024; Zhu et al., 2024; Zuo et al.,\n2025) presents an interesting avenue for future work.\nOther pretext tasks.While this work primarily focuses on jigsaw puzzles as the pretext task, numerous\nalternatives exist and worth exploration (Gidaris et al., 2018; Gui et al., 2024). In particular, future research\ncould extend our approach to pretext tasks in other modalities, including text (Lan et al., 2020), video (Ahsan\net al., 2019; Kim et al., 2019; Wang et al., 2020), audio (Carr et al., 2021), point clouds (Poursaeed et al.,\n2020), and tabular data (Lee et al., 2024).\nOther RL algorithms.We exclusively employ GRPO in our current experiments, leaving other promising\nalgorithms such as PPO (Schulman et al., 2017), DPO (Rafailov et al., 2023) and Reinforce++ (Hu, 2025)\nunexplored. Furthermore, investigating recent advancements and variations of GRPO, including DAPO (Yu\net al., 2025b), Dr. GRPO (Liu et al., 2025d), GPG (Chu et al., 2025), and NoisyRollout (Liu et al., 2025b),\ncould offer valuable insights and potential performance gains.\nAcknowledgements\nWe acknowledge support from the Research Foundation - Flanders (FWO) through project numbers G0A1319N\nand S001421N, and funding from the Flemish Government under the Onderzoeksprogramma Artifici\u00eble\nIntelligentie (AI) Vlaanderen programme.\nWe acknowledge LUMI-BE for awarding this project access to the LUMI supercomputer, owned by the\nEuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through a LUMI-BE\nRegular Access call.\n13References\nUnaiza Ahsan, Rishi Madhok, and Irfan Essa. Video jigsaw: Unsupervised learning of spatiotemporal context\nfor video action recognition.WACV, 2019.\nEkin Akyurek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, and Jacob\nAndreas. The surprising effectiveness of test-time training for few-shot learning.arXiv, 2024.\nAnthropic. Introducing computer use, a new claude 3.5 sonnet, and claude 3.5 haiku, 2024. https:\n//www.anthropic.com/news/3-5-models-and-computer-use .\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, et al. Qwen2.5-vl technical report.arXiv, 2025a.\nSule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, and Yansong Tang.\nUnivg-r1: Reasoning guided universal visual grounding with reinforcement learning.arXiv, 2025b.\nAli Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time.arXiv, 2024.\nFabio M Carlucci, Antonio D\u2019Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain\ngeneralization by solving jigsaw puzzles.CVPR, 2019.\nAndrew N Carr, Quentin Berthet, Mathieu Blondel, Olivier Teboul, and Neil Zeghidour. Self-supervised\nlearning of audio representations from permutations with differentiable ranking.SPL, 2021.\nHardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or\nrl? an early investigation into training r1-like reasoning large vision-language models.arXiv, 2025a.\nJiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng\nLi, Jiaze Chen, Hao Zhou, and Mingxuan Wang. Enigmata: Scaling logical reasoning in large language\nmodels with synthetic verifiable puzzles.arXiv, 2025b.\nJiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou,\nSaining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: A family of fully open unified\nmultimodal models-architecture, training and dataset.arXiv, 2025c.\nLiang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in\nvision-language models with less than $3, 2025d.https://github.com/Deep-Agent/R1-V.\nXiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan.\nJanus-pro: Unified multimodal understanding and generation with data and model scaling.arXiv, 2025e.\nYanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato Carson Denison, John Schulman, Arushi\nSomani, Peter Hase, Misha Wagner Fabien Roger Vlad Mikulik, Sam Bowman, Jan Leike Jared Kaplan,\net al. Reasoning models don\u2019t always say what they think.arXiv, 2025f.\nYingyi Chen, Xi Shen, Yahui Liu, Qinghua Tao, and Johan AK Suykens. Jigsaw-vit: Learning jigsaw puzzles\nin vision transformer.PRL, 2023.\nZhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao\nTian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with\nmodel, data, and test-time scaling.arXiv, 2024.\nKanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick:\nHarnessing gui grounding for advanced visual gui agents.ACL, 2024.\nXiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: A simple and strong\nreinforcement learning baseline for model reasoning.arXiv, 2025.\nHuilin Deng, Ding Zou, Rui Ma, Hongchen Luo, Yang Cao, and Yu Kang. Boosting the generalization and\nreasoning of vision language models with curriculum reinforcement learning.arXiv, 2025.\n14Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. CVPR, 2009.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context\nprediction. ICCV, 2015.\nRuoyi Du, Dongliang Chang, Ayan Kumar Bhunia, Jiyang Xie, Zhanyu Ma, Yi-Zhe Song, and Jun Guo.\nFine-grained visual classification via progressive multi-granularity training of jigsaw patches.ECCV, 2020.\nKanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive\nbehaviors that enable self-improving reasoners, or, four habits of highly effective stars.arXiv, 2025.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization.ICML, 2023.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations. ICLR, 2018.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models.arXiv,\n2024.\nJie Gui, Tuo Chen, Jing Zhang, Qiong Cao, Zhenan Sun, Hao Luo, and Dacheng Tao. A survey on\nself-supervised learning: Algorithms, applications, and future trends.TPAMI, 2024.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv, 2025.\nJian Hu. Reinforce++: A simple and efficient approach for aligning large language models.arXiv, 2025.\nZhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, and Junnan Li.\nBeyond \u2019aha!\u2019: Toward systematic meta-abilities alignment in large reasoning models.arXiv, 2025.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card.arXiv, 2024.\nDongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire\nGuo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning\nquality, robustness, and efficiency.ICML, 2025.\nDahun Kim, Donghyeon Cho, and In So Kweon. Self-supervised video representation learning with space-time\ncubic puzzles. AAAI, 2019.\nYuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for\ngeneralizable medical reasoning in vision-language models.arXiv, 2025.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\nAlbert: A lite bert for self-supervised learning of language representations.ICLR, 2020.\nKyungeun Lee, Ye Seul Sim, Hye-Seung Cho, Moonjung Eo, Suhee Yoon, Sanghyu Yoon, and Woohyung\nLim. Binning as a pretext task: Improving self-supervised learning in tabular domains.ICML, 2024.\nSeongyun Lee, Seungone Kim, Minju Seo, Yongrae Jo, Dongyoung Go, Hyeonbin Hwang, Jinho Park, Xiang\nYue, Sean Welleck, Graham Neubig, Moontae Lee, and Minjoon Seo. The cot encyclopedia: Analyzing,\npredicting, and controlling how a reasoning model will think.arXiv, 2025.\nChengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vuli\u0107, and Furu Wei.\nImagine while reasoning in space: Multimodal visualization-of-thought.arXiv, 2025a.\nMing Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, and Kaipeng Zhang. Think or not think: A study of explicit\nthinking in rule-based visual reinforcement fine-tuning.arXiv, 2025b.\n15Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and\nAlan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning.CVPR,\n2023.\nZhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, and Zhijie Deng. Improved\nvisual-spatial reasoning via r1-zero-like training.arXiv, 2025.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step.ICLR, 2024.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco: Common objects in context.ECCV, 2014.\nJunteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili\nChen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, and Junxian He. Synlogic:\nSynthesizing verifiable reasoning data at scale for learning logical reasoning and beyond.arXiv, 2025a.\nXiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe\nShieh. Noisyrollout: Reinforcing visual reasoning with data augmentation.arXiv, 2025b.\nYuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner:\nUnified visual perception and reasoning via reinforcement learning.arXiv, 2025c.\nZichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin.\nUnderstanding r1-zero-like training: A critical perspective.arXiv, 2025d.\nZiyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang.\nVisual-rft: Visual reinforcement fine-tuning.arXiv, 2025e.\nZiyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin,\nand Jiaqi Wang. Visual agentic reinforcement fine-tuning.arXiv, 2025f.\nSara Vera Marjanovi\u0107, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar\nBhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han L\u00f9, Nicholas Meade, Dongchan Shin,\nAmirhossein Kazemnejad, Gaurav Kamath, Marius Mosbach, Karolina Sta\u0144czak, and Siva Reddy. Deepseek-\nr1 thoughtology: Let\u2019s think about llm reasoning.arXiv, 2025.\nFanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han,\nBotian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning\nwith rule-based reinforcement learning.arXiv, 2025.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.\nECCV, 2016.\nOpenAI. Introducing gpt-4.1 in the api, 2025a.https://openai.com/index/gpt-4-1/.\nOpenAI. Introducing openai o3 and o4-mini, 2025b. https://openai.com/index/\nintroducing-o3-and-o4-mini .\nYingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu,\nXin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage\nrule-based rl. arXiv, 2025.\nOmid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, and Vladimir G Kim. Self-supervised learning of\npoint clouds via orientation estimation.3DV, 2020.\nJi Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao\nDong, and Jie Tang. Cogcom: A visual language model with chain-of-manipulations reasoning.ICLR,\n2025.\n16Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang,\nJiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,\nMingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang\nRen, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,\nand Zihan Qiu. Qwen2.5 technical report.arXiv, 2025.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.\nDirect preference optimization: Your language model is secretly a reward model.NeurIPS, 2023.\nArijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi,\nBryan A Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal\nlanguage models. arXiv, 2024.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\nalgorithms. arXiv, 2017.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nYK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\narXiv, 2024.\nHaozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang,\nKangjia Zhao, Qianqian Zhang, et al. Vlm-r1: A stable and generalizable r1-style large vision-language\nmodel. arXiv, 2025.\nDavid Silver and Richard S Sutton. Welcome to the era of experience, 2025.https://storage.googleapis.\ncom/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf .\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be\nmore effective than scaling model parameters.ICLR, 2025.\nZhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei\nGu, Juntao Li, Xiaoye Qu, and Yu Cheng. Openthinkimg: Learning to think with images via visual tool\nreinforcement learning.arXiv, 2025.\nChameleon Team. Chameleon: Mixed-modal early-fusion foundation models.arXiv, 2024.\nPeter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha\nAkula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: A fully open,\nvision-centric exploration of multimodal llms.NeurIPS, 2024a.\nShengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut?\nexploring the visual shortcomings of multimodal llms.CVPR, 2024b.\nCheng Wang, Yue Liu, Baolong Li, Duzhen Zhang, Zhongzhi Li, and Junfeng Fang. Safety in large reasoning\nmodels: A survey.arXiv, 2025a.\nHaozheWang, ChaoQu, ZumingHuang, WeiChu, FangzhenLin, andWenhuChen. Vl-rethinker: Incentivizing\nself-reflection of vision-language models with reinforcement learning.arXiv, 2025b.\nJiangliu Wang, Jianbo Jiao, and Yun-Hui Liu. Self-supervised video representation learning by pace prediction.\nECCV, 2020.\nPeiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui.\nMath-shepherd: Verify and reinforce llms step-by-step without human annotations.ACL, 2024a.\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\nWang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model\u2019s perception of the world at any\nresolution. arXiv, 2024b.\n17Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei.\nMultimodal chain-of-thought reasoning: A comprehensive survey.arXiv, 2025c.\nChengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie,\nXingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding\nand generation. arXiv, 2024.\nJinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu,\nZhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal\nunderstanding and generation.ICLR, 2025a.\nTian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong\nWu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning.arXiv,\n2025b.\nYuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, and Michael\nShieh. Monte carlo tree search boosts reasoning via iterative preference learning.NeurIPS Workshop, 2024.\nHuajian Xin, Z.Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang,\nXuan Lu, Qiushi Du, Wenjun Gao, Haowei Zhang, Qihao Zhu, Dejian Yang, Zhibin Gou, Z.F. Wu, Fuli Luo,\nand Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning\nand monte-carlo tree search.ICLR, 2025.\nYi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin,\nFengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through\ncross-modal formalization. arXiv, 2025.\nEn Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui\nHan, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning.arXiv,\n2025a.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu,\nLingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale.arXiv,\n2025b.\nHengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zero\u2019s \"aha\nmoment\" in visual reasoning on a 2b non-sft model.arXiv, 2025.\nJunyi Zhu, Shuochen Liu, Yu Yu, Bo Tang, Yibo Yan, Zhiyu Li, Feiyu Xiong, Tong Xu, and Matthew B\nBlaschko. Fastmem: fast memorization of prompt improves context awareness of large language models.\nEMNLP Findings, 2024.\nYuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning\nDing, and Bowen Zhou. Ttrl: Test-time reinforcement learning.arXiv, 2025.\n18A Full Jigsaw Puzzles\nTable 7: Evaluation results on full jigsaw puzzles with different sizes. For thinking and non-thinking of the\nsame model, the better result is underlined. Claude 3.5 Haiku\u2020fails to output answers in the required grid\nformat.\nThinking\nMethod 2x1 3x1 4x1 2x2 AVG\nRandom 50.00 16.67 4.17 4.17 18.75\nGPT-4.1 79.00 27.30 8.20 6.40 30.23\nGPT-4.1-mini 60.80 26.60 8.40 6.20 25.50\nClaude 3.5 Haiku 69.40 19.00 5.30 6.50 25.05\nQwen2.5-VL-72B 53.40 20.40 5.10 5.60 21.13\nQwen2.5-VL-7B 25.60 15.80 3.10 4.20 12.18\n+ Jigsaw-R1 97.30\u219171.70 31.20\u219115.40 8.20\u21915.10 7.80\u21913.60 36.12\u219123.94\nQwen2.5-VL-3B 45.90 11.30 2.60 3.40 15.80\n+ Jigsaw-R1 97.20\u219151.30 31.40\u219120.10 7.30\u21914.70 6.50\u21913.10 35.60\u219119.80\nQwen2-VL-2B 46.90 5.30 0.60 4.00 14.20\n+ Jigsaw-R1 98.00\u219151.10 30.60\u219125.30 8.40\u21917.80 4.80\u21910.80 35.45\u219121.25\nInternVL2.5-2B 16.00 8.60 2.90 2.70 7.55\n+ Jigsaw-R1 99.30\u219183.30 30.10\u219121.50 4.50\u21911.60 0.00\u2193\u22122.70 33.48\u219125.93\nNon-thinking\nMethod 2x1 3x1 4x1 2x2 AVG\nRandom 50.00 16.67 4.17 4.17 18.75\nGPT-4.1 68.50 17.30 7.00 7.30 25.03\nGPT-4.1-mini 49.50 18.80 3.80 5.60 19.43\nClaude 3.5 Haiku\u2020 0.00 0.00 0.00 0.00 0.00\nQwen2.5-VL-72B 91.70 26.40 9.20 8.00 33.83\nQwen2.5-VL-7B 30.10 17.40 1.50 0.50 12.38\n+ Jigsaw-R1 99.20\u219169.10 30.40\u219113.00 8.60\u21917.10 9.00\u21918.50 36.80\u219124.42\nQwen2.5-VL-3B 51.60 16.70 4.40 3.70 19.10\n+ Jigsaw-R1 98.80\u219147.20 29.70\u219113.00 9.10\u21914.70 7.70\u21914.00 36.32\u219117.22\nQwen2-VL-2B 11.40 2.00 0.20 0.00 3.40\n+ Jigsaw-R1 99.00\u219187.60 32.10\u219130.10 7.30\u21917.10 3.50\u21913.50 35.47\u219132.07\nInternVL2.5-2B 20.90 11.30 3.40 2.80 9.60\n+ Jigsaw-R1 99.20\u219178.30 31.40\u219120.10 7.10\u21913.70 7.30\u21914.50 36.25\u219126.65\n19B Box Jigsaw Puzzles\nThis task evaluates a MLLM\u2019s ability to reason about spatial relationships between image patches and to\nvisually ground its understanding by identifying the correct bounding box.\nThe process begins by dividing an input image into anm\u00d7n grid of regions. Each region is assigned a unique\nposition index. From each of thesemn regions, a patch of equal size is randomly selected, resulting inmn\nequally-sized patches. Next, a target position index,i (where 1 \u2264i \u2264mn), is randomly chosen. Allmn\npatches are then shuffled. A key constraint during shuffling is that the patch originally from positioni must\nbe relocated to a different position. The MLLM\u2019s task is to provide the bounding box coordinates for the\npatch that originally belonged in positioni. The accuracy reward is measured by the Intersection over Union\n(IoU) between the predicted and ground truth bounding boxes.\nAs shown in Table 8, training MLLMs on this task can promote generalization to ScreenSpot Cheng et al.\n(2024), a downstream task requiring visual grounding within a Graphical User Interface (GUI) environment.\nTable 8: Evaluation results on ScreenSpot. For thinking and non-thinking of the same model, the better\nresult is underlined.\nThinking\nMethod ScreenSpot\nQwen2.5-VL-3B 57.38\n+ Jigsaw-R1 72.48\u219115.10\nNon-thinking\nMethod ScreenSpot\nQwen2.5-VL-3B 79.08\n+ Jigsaw-R1 81.05\u21911.97\n20C Qwen2-VL-2B-Base\nThe training dynamics of Qwen2-VL-2B-Base on 2x1 pair jigsaw puzzles are illustrated in Figure 4. While\nrewards show a rapid initial increase, mirroring trends seen in instruction-tuned models, the improvement in\naccuracy reward is largely superficial. It primarily reflects the model learning to adhere to the specified output\nformat, which allows for the extraction of a final answer, rather than indicating a genuine enhancement in its\ncapabilities to solve jigsaw puzzles. Subsequently, the accuracy reward stagnates, not surpassing 0.5, which is\nequivalent to the performance of random guessing. This leads us to hypothesize that jigsaw puzzles present a\nsignificant challenge for the base model, potentially requiring a substantially extended training period to\nachieve meaningful performance gains.\nFurthermore, the completion length also shows a swift initial growth. This parallels the reward behavior:\nthe model initially tends to provide direct answers but then learns to include the required explicit reasoning\nprocess within the specified format, leading to longer outputs. After this adjustment period, the completion\nlength remains relatively stable.\nSimilar to observations with instruction-tuned models, the keywords are already present in the model before\ntraining begins (at step 0). This aligns with findings in textual domain (Liu et al., 2025d). Besides, while these\nkeywords appear more often as training progresses, their overall occurrence remains relatively infrequent.\n0 200 400 600 800 1000\nTraining Step\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Reward\nAccuracy Reward\nFormat Reward\n0 200 400 600 800 1000\nTraining Step\n20\n40\n60\n80\n100\n120\n140Length\nCompletion Length\n0 200 400 600 800 1000\nTraining Step\n0\n1\n2\n3\n4\n5\n6\n7\n8Keyword Frequency\nFigure 4: The training dynamics of Jigsaw-R1 using Qwen2-VL-2B-Base.\n21D Keywords\nA key characteristic in our setting is that MLLMs often describe image content when answering the question.\nConsequently, many keywords may appear in these descriptions rather than reflecting the cognitive behaviors\n(e.g., the word \"wait\" in \"two people waiting for trains\"). To address this, after carefully examining model\noutputs, we select keywords specifically indicative of backtracking and backward chaining. While this targeted\nselection minimizes false positives, it inherently increases false negatives, resulting in a relatively low observed\nfrequency. Specifically, they might appear only once in hundreds of samples.\nSelected Keywords\nBacktracking: recheck, reverify, reevaluate, reexmamine\nBackward Chaining:work backwards\n22E Prompts\nThis section provides prompts for different variants of jigsaw puzzles, using a 2x2 puzzle as an example.\n2x2 Full Jigsaw Puzzle (thinking)\nThe input image is divided into 2x2 patches that have been randomly permuted from their original\npositions. Your task is to solve this 2x2 jigsaw puzzle and reconstruct the original image.\nConsider a 2x2 grid, where each number represents a position index ranging from 1 (top-\nleft) to 4 (bottom-right):\n1 2\n3 4\nFor each patch, determine its correct position index in the original image. If a patch cur-\nrently at position X should belong at position Y, place \"Y\" at position X.\nFirst, output the thinking process within <think> </think> tags. Then, provide the final\nanswer within <answer> </answer> tags. The final answer should be the position indexes arranged\nin a 2x2 grid.\n2x2 Full Jigsaw Puzzle (non-thinking)\nThe input image is divided into 2x2 patches that have been randomly permuted from their original\npositions. Your task is to solve this 2x2 jigsaw puzzle and reconstruct the original image.\nConsider a 2x2 grid, where each number represents a position index ranging from 1 (top-\nleft) to 4 (bottom-right):\n1 2\n3 4\nFor each patch, determine its correct position index in the original image. If a patch cur-\nrently at position X should belong at position Y, place \"Y\" at position X.\nDirectly output the final answer. The final answer should be the position indexes arranged\nin a 2x2 grid.\n232x2 Pair Jigsaw Puzzle (thinking)\nThe input image is divided into 2x2 patches that have been randomly permuted from their original\npositions. Your task is to solve this 2x2 jigsaw puzzle and reconstruct the original image.\nConsider a 2x2 grid, where each number represents a position index ranging from 1 (top-\nleft) to 4 (bottom-right):\n1 2\n3 4\nFor patches currently at positions 3 and 2, determine their relative position in the original\nimage.\nSelect the correct answer from the following 8 choices:\n(A) 3 is on the upper right of 2\n(B) 3 is on the lower left of 2\n(C) 3 is on the upper left of 2\n(D) 3 is directly to the right of 2\n(E) 3 is directly below 2\n(F) 3 is directly above 2\n(G) 3 is directly to the left of 2\n(H) 3 is on the lower right of 2\nFirst, output the thinking process within <think> </think> tags. Then, provide the final\nanswer within <answer> </answer> tags. The final answer should be a single letter.\n242x2 Pair Jigsaw Puzzle (non-thinking)\nThe input image is divided into 2x2 patches that have been randomly permuted from their original\npositions. Your task is to solve this 2x2 jigsaw puzzle and reconstruct the original image.\nConsider a 2x2 grid, where each number represents a position index ranging from 1 (top-\nleft) to 4 (bottom-right):\n1 2\n3 4\nFor patches currently at positions 3 and 2, determine their relative position in the original\nimage.\nSelect the correct answer from the following 8 choices:\n(A) 3 is on the upper right of 2\n(B) 3 is on the lower left of 2\n(C) 3 is on the upper left of 2\n(D) 3 is directly to the right of 2\n(E) 3 is directly below 2\n(F) 3 is directly above 2\n(G) 3 is directly to the left of 2\n(H) 3 is on the lower right of 2\nDirectly output the final answer. The final answer should be a single letter.\n252x2 Box Jigsaw Puzzle (thinking)\nThe input image is divided into 2x2 regions. Consider a 2x2 grid, where each number represents a\nregion index ranging from 1 (top-left) to 4 (bottom-right):\n1 2\n3 4\nSome patches of equal size have been randomly swapped from their original positions, re-\nsulting in an unnatural appearance. Your task is to find the original location of the patch that\ncurrently belongs in region 4.\nFirst, output the thinking process within <think> </think> tags. Then, provide the final\nanswer within <answer> </answer> tags. The final answer should be bounding box coordinates,\nformatted as integers and separated by comma.\n2x2 Box Jigsaw Puzzle (non-thinking)\nThe input image is divided into 2x2 regions. Consider a 2x2 grid, where each number represents a\nregion index ranging from 1 (top-left) to 4 (bottom-right):\n1 2\n3 4\nSome patches of equal size have been randomly swapped from their original positions, re-\nsulting in an unnatural appearance. Your task is to find the original location of the patch that\ncurrently belongs in region 4.\nDirectly output the final answer. The final answer should be bounding box coordinates,\nformatted as integers and separated by comma.\n26F Qwen2.5-VL-3B Examples\nIn this section, we present two illustrative examples of Qwen2.5-VL-3B where the model\u2019s final answer, while\ncorrect, is inconsistent with its preceding reasoning process. In the first example, the model\u2019s reasoning chain\nindicates that the patch containing rice should be placed on the left, yet it chooses option A, which is indeed\nthe correct outcome. Similarly, in the second instance, the model\u2019s rationale suggests that patch 2 should be\npositioned directly above patch 1; nevertheless, it again selects option A, also the correct answer.\nQwen2.5-VL-3B Example 1\nPrompt: ... Select the correct answer from the following 2 choices:\n(A) 2 is directly to the left of 1\n(B) 2 is directly to the right of 1\n...\nOutput:\n<think>\nThe image on the left shows a bowl of rice with a spoon and fork placed next to it. The image on the\nright shows a plate of mixed vegetables in a sauce. If we consider the typical layout of a meal tray,\nthe rice would typically be served first, followed by the main dish. Therefore, the image on the left\nshould be positioned to the left of the image on the right.\n</think>\n<answer>A</answer>\nGround truth:A\n27Qwen2.5-VL-3B Example 2\nPrompt: ... Select the correct answer from the following 2 choices:\n(A) 2 is directly below 1\n(B) 2 is directly above 1\n...\nOutput:\n<think>\nThe image shows a scene of a game being played on a sandy surface with spectators in the background.\nThe patch labeled \"2\" appears to be the top part of the image, showing the upper body of a player\nand the frisbee in motion. The patch labeled \"1\" seems to be the lower part of the image, showing the\nlower body of the same player and the ground.\nGiven the layout of the image, it is clear that the top part (patch 2) is above the bottom\npart (patch 1). Therefore, the correct answer is that patch 2 is directly above patch 1.\n</think>\n<answer>A</answer>\nGround truth:A\n28G InternVL2.5-2B Examples\nThis section includes two illustrative examples from InternVL2.5-2B. In both cases, the model\u2019s thinking\nprocess merely contains the final answer, instead of providing a detailed step-by-step reasoning.\nInternVL2.5-2B Example 1\nPrompt: ... Select the correct answer from the following 2 choices:\n(A) 2 is directly to the left of 1\n(B) 2 is directly to the right of 1\n...\nOutput:\n<think>\n2 is directly to the left of 1.\n</think>\n<answer>A</answer>\nGround truth:A\n29InternVL2.5-2B Example 2\nPrompt: ... Select the correct answer from the following 2 choices:\n(A) 2 is directly below 1\n(B) 2 is directly above 1\n...\nOutput:\n<think>\n2 is directly below 1.\n</think>\n<answer>A</answer>\nGround truth:A\n30",
      "Does the Forced Van der Pol Oscillator Exhibit Irregular \nBehavior? \n \nSamaira Tibrewal1 and Soumyajit Seth2 \n \n1 NPS International School, Singapore \n2 School of Technology Management and Engineering, NMIMS Hyderabad, Jadcherla Campus, \nPolepally, 509301 India \n \n \nSUMMARY \nIn various physical and engineering systems in nature, we encounter different types of oscillators, many \nof which exhibit nonlinear behavior. One such system, the Van der Pol oscillator, is known for its self-\nsustained limit cycle behavior. However, when subjected to external forcing, its dynamics can change \nabruptly from its self-sustained oscillation. This study investigates the onset of complex behavior in the \nsinusoidally forced Van der Pol oscillator, aiming to understand how the system transitions from regular \nto aperiodic dynamics under varying conditions. The central research question is: Does the forced Van \nder Pol oscillator exhibit irregular dynamics such as quasi-periodicity or chaos, and if so, what are the \npossible conditions? \nWe hypothesize that if we choose different values of the amplitude and ratios of the external forcing \nfrequency with respect to the natural frequency of the oscillator, it can induce a range of dynamic \nregimes, including higher periodic and chaotic ones. Using numerical simulations in Python, we analyze \nthe behavior of the system through time series plots, phase portraits, and bifurcation diagrams. Our \nresults reveal that as the bifurcation parameter, the forcing amplitude is varied, having different \nfrequency ratios, the system transitions through periodic, quasi -periodic, and chaotic states. This type \nof nonlinear interaction has wide applications in real -world systems, including biological rhythms, \nelectrical circuits, and astrophysical processes. \n \nINTRODUCTION \nA Simple Harmonic Oscillator (SHO) exhibits periodic motion where acceleration is directed toward a \nfixed equilibrium point and is proportional to displacement, resulting in a linear relationship. \nApplications of SHO include clocks, musical instruments, and metronomes. However, many real-world \nsystems display nonlinear behavior, such as gravitational forces in astrophysics, heartbeats in biology, \npredator-prey dynamics in ecology, and climate cycles like El Ni\u00f1o and La Ni\u00f1a. Nonlinearity is often \nintroduced into the SHO through damping\u2014a force that dissipates energy, typically as heat, leading to \na gradual reduction in amplitude. In systems like pendulums, damping arises from resistive forces such \nas air resistance or friction, modeled by a velocity -dependent term (e.g., \u2212b\u03b8, where b is the damping coefficient and \u03b8 is the angular velocity). This incorporation of damping makes the model more realistic \nby accounting for energy losses inherent in practical oscillating systems. \nThe Van der Pol oscillator [1] is a simple nonlinear system that exhibits damped harmonic motion, with \ndamping modeled by a nonlinear velocity -dependent term. This nonlinearity regulates the system's \nenergy, enabling self-sustained oscillations known as a limit cycle. The governing equation for the Van \nder Pol oscillator is: d\u00b2x/dt\u00b2 + (x\u00b2 \u2212 1)dx/dt + x = 0 \nWhen the displacement x is less than one, the damping term of the Van der Pol oscillator is negative, \nreducing resistance and increasing velocity. When x exceeds one, the damping becomes positive, \nincreasing resistance and reducing velocity. This self -regulating mechanism drives the system toward \na stable limit cycle [2], resulting in periodic motion independent of initial conditions. Manu [3] \ninvestigated the existence of limit cycles and the role of the parameter \u03bc in system stability. Tsatsos [4] \nand Chu ghtai [5] explored how variations in system parameters \u2014such as damping, frequency, and \namplitude\u2014affect transitions between periodic, quasi -periodic, and chaotic behaviors, using \nbifurcation diagrams and Fourier power spectra. Venkatesan and Lakshmanan [6] investigated a driven \ndouble-well Duffing \u2013Van der Pol oscillator, identifying various attractors and transitions between \nstable and chaotic behavior, including local-global bifurcations of intermittent and blue-sky catastrophe \ntypes. Jiang and Wei [7] studied the Van der Pol oscillator with delayed feedback, revealing transcritical \nand pitchfork bifurcations and changes in stability associated with a simple zero eigenvalue. \nA Van der Pol oscillator becomes forced when subjected to an external periodic force, with its dynamics \ngoverned by parameters such as nonlinear damping and the frequency and amplitude of forcing. \nDepending on these parameters, the system can exhibit synch ronization, amplitude modulation, \nquasiperiodicity, or chaos, as shown by Ku and Sun [8] and Chughtai [5]. Pinto and Machado [9] \nextended this study to higher -order forced Van der Pol oscillators, uncovering diverse oscillatory \npatterns and stability conditions. Bifurcation analyses by Sharma [10] and Arrowsmith and Taha [11] \nrevealed Saddle-node, Supercritical Hopf, and Infinite-period bifurcations , noting that system \nsymmetry simplifies bifurcation structures and transitions between oscillatory and non -oscillatory \nstates. Xu and Jiang [12] and Holmes and Rand [13] further examined bifurcation behavior under \nsinusoidal and periodic forcing, observin g a range of periodic and aperiodic responses. \u0160ev\u010d\u00edk and \nP\u0159ibylov\u00e1 [14] identified the coexistence of a stabl e torus and a stable cycle emerging from a q -fold \nbifurcation within an Arnold tongue. Cooper et al. [15] also applied perturbation methods and numerical \nsimulations to control or stabilize chaotic dynamics in the forced Van der Pol oscillator. \nAs mentioned above, the studies demonstrate that varying the amplitudes of the forced Van der Pol \noscillator can produce inherent complex phenomena, including chaotic orbits and bifurcation. In this \ncontext, chaos refers to an irregular dynamics, where sma ll changes in initial conditions lead to \nsignificantly different long -term outcomes, a property known as sensitive dependence on initial \nconditions. The chaotic behavior arises due to the mismatch of the frequency ratios or the high values \nof the forcing amplitudes, which makes the oscillation unpredictable. An open question is how the dynamics of the forced Van der Pol oscillator change as the forcing \namplitude varies smoothly, particularly for integer and non -integer ratios of forcing to natural \nfrequency. This study investigates how the dynamical behavior of the system and bifurcation structure \nevolve under these conditions. \nIn real -world scenarios, the amplitude of the forcing term often varies. In structural engineering, \nvariable forces from seismic events, wind, or traffic loads influence the dynamics of buildings and \nbridges. Instruments like guitars and violins exhibit fo rce variations in acoustics due to changes in \nplaying techniques. Similarly, wave -induced forces on ships or offshore platforms fluctuate in fluid -\nstructure interactions with environmental conditions such as tides and storms. These variations \nsignificantly impact the oscillatory response of the system. \nSystem Equations: The equation of the forced Van der Pol oscillator is as below [7]: \n\ud835\udc512\ud835\udc65\n\ud835\udc51\ud835\udc612 + \ud835\udf07(\ud835\udc652 \u2212 1) \ud835\udc51\ud835\udc65\n\ud835\udc51\ud835\udc61 + \ud835\udc65 =  \ud835\udc39\ud835\udc50\ud835\udc5c\ud835\udc60(\ud835\udf14\ud835\udc61) \nIn the system equation, x denotes displacement, \u03bc is the nonlinear damping coefficient, \u03c9 is the angular \nfrequency, and F is the forcing amplitude. This form models various real -world systems\u2014mechanical \n(e.g., mass -spring systems), electrical (e.g., RLC ci rcuits with AC sources), and biological (e.g., \nperiodic forces from cardiac or respiratory cycles) \u2014via a forcing term Fcos(\u03c9t). When \u03bc=0 and F=0, \nthe equation reduces to the classical Van der Pol oscillator. This study explores the dynamical behavior \nof the system by varying the forcing amplitude F. \n \nRESULTS \nTo understand how varying the forcing amplitude F influences the dynamics of the forced Van der Pol \noscillator, we performed a numerical bifurcation analysis with fixed parameters \u03bc=1 and \u03c9=1. The state \nvariable was sampled stroboscopically, synchronized with the external periodic forcing, to capture the \nlong-term behavior of the system. \nThe bifurcation diagram (Figure 1(a)) reveals the progression of dynamical states as F is varied. At \nhigher amplitudes (around F\u22484.0), the system exhibits a stable period -1 orbit, shown by a single \ncontinuous branch in the diagram. A transition to a period -2 orbit occurs as the amplitude decreases, \nindicating a bifurcation point. Interestingly, a reappearance of the period-1 orbit is observed in the range \nF=3.0 to F=3.5, followed by further transitions to higher periodicities that suggest a cascade of period-\ndoubling bifurcations. \nAt intermediate forcing amplitudes, the system shows intermittent chaos with periodic windows. For \nF\u22640.25, persistent chaotic dynamics dominate. Phase portraits (Figures 1(b), 1(d), 1(f), 1(h), 1(j)) and \ntime series (Figures 1(c), 1(e), 1(g), 1(i), 1(k)) mainly exhibit period-1 orbits, except at F=0.05, where a \nthickened limit cycle indicates chaotic behavior. \nIn the bifurcation diagram in Figure 2(a), when \u03bc = 2 and \u03c9 = 1, increasing F drives the system from \nchaotic to period -1 behavior, evidenced by a single continuous branch. Compared to Figure 1, this diagram exhibits a stable period -1 orbit more clearly, with fixed -point values oscillating near zero. \nBetween F=2.5 and F=3.0, a period-2 orbit emerges, indicated by bifurcation into two distinct branches. \nAlthough the diagram is dominated by period -1 orbi t, regions of chaos appear intermittently, \ncharacterised by scattered points reflecting the interplay between periodic and chaotic states. The \nchaotic behavior occurs at F=0.5, as shown by the complex loops in the phase portrait and irregular \nperiodicities in the time series (Figures (d) and (e)). At F=0.05 (Figures (b) and (c)), a weaker form of \nchaos is observed. Outside these regions, the system maintains stable period-1 dynamics. \nThe bifurcation diagram (Figure 3(a)) reveals a dominant period-2 orbit, absent in Figures 1 and 2 for \n\u03bc=2 and \u03c9=3, as indicated by two distinct branches. At low forcing (F<2), the period -2 orbit appears \ndensely packed, with the system exhibiting simple pe riodic behavior characterized by a single fixed \npoint for each F. As F approaches zero, the chaotic orbit narrows. Beyond F\u22482, successive bifurcations \ngive rise to increasingly complex dynamics, with pronounced chaotic regions between 2<F<4 marked \nby scattered points. For F>4, the system regains periodicity with more structured branches, and for F>8, \nfewer distinct branches indicate the emergence of simpler periodic attractors under strong forcing. At \nF=10, a period -4 orbit appears, comprising two stable an d two unstable branches. The symmetry of \nfixed points about x=0 reflects the inherent symmetry of the Van der Pol oscillator. \nThe phase portrait and time series at F=0.5 (Figures (b) and (c)) show a thick trajectory, indicating \nchaos within a period-1 orbit. At F=3.0 and F=6.0 (Figures (c) and (d)), the chaotic features diminish, \nconsistent with the bifurcation structure. Finally , at F=9.5 (Figure (e)), the system stabilizes into a \nperiodic attractor, confirming the re-emergence of ordered dynamics. \nThe bifurcation diagram in Figure 4(a) for \u03bc=\u22120.5 and \u03c9=1.0 demonstrates that, as F increases, the \nsystem predominantly maintains a period -1 orbit, reflected by a continuous line formed by sampled \nvalues of the state variable x. The phase portrait and time  series at F=0.2 (Figure (b)) further confirm \nthe existence of a distinct limit cycle, characteristic of period -1 dynamics. At higher forcing values \n(F=0.5,1.0,1.5), the phase portraits and time series indicate the emergence of fixed points from the \nperiod-1 orbit observed in the bifurcation diagram. \nWe analyze the system dynamics through bifurcation diagrams by varying the forcing amplitude F \nunder two conditions: (i) integer and (ii) non-integer ratios between the applied and natural frequencies. \n(i) Integer Condition: Figure 5 presents the bifurcation diagram for the forced Van der Pol oscillator \nwith an integer frequency ratio of 2:1. For low forcing (F<0.5), the system exhibits period-2 behavior, \nevident from two distinct branches. As F increases to the intermediate range (0.5\u2264F\u22641.0), bifurcations \noccur, leading to the emergence of multiple branches indicative of complex periodic or quasi -periodic \ndynamics. At higher forcing amplitudes (F>1.0), the diagram displays densely packed and overlapping branches, characteristic of chaotic or multi -periodic behavior. Notably, for F>1.8, the strong merging \nof branches signifies a transition to chaos, marked by sensitive dependence on initial conditions. \nPhase portraits at F=0.25 and F=0.75 (Figures 5(b) and 5(d)) reflect this transition between attractors. \nAt F=0.25, trajectories are periodic, consistent with stable limit cycle behavior. In contrast, at F=0.75, \nthe phase portrait exhibits a proliferation of trajectories, indicating the onset of chaotic dynamics as the \nforcing amplitude increases. \n(ii) Non -integer Condition: Figure 6 depicts the bifurcation diagram for the forced Van der Pol \noscillator with a non-integer frequency ratio of 5:3. Compared to the integer case (Figure 5), the system \nexhibits a wider chaotic behavior in the parameter space. At F=0.25, the bifurcati on diagram shows a \nsingle continuous branch corresponding to a period -1 orbit with a narrow band chaos. At F=1.75, a \nperiod-2 orbit emerges, marked by two distinct branches, one significantly denser, indicating the \nprobability of occurrence of the branches. Phase portraits and time series (Figures 6(b) and 6(c)) further \nillustrate this behavior: at F=0.25, the system converges to a stable limit cycle with a little irregularities, \nwhile at higher F, particularly in Figure 6(c), the system displays fully developed chaos without a well-\ndefined limit cycle, consistent with the bifurcation structure. \nFigure 7(a) shows that for F<0.75, the system exhibits a predominantly period-1 orbit with slight chaos, \nindicated by a continuous line with scattered points. In the range 0.75<F<1.5, the system alternates \nbetween period -1 and period -2 orbits accompanied b y chaotic behavior. At higher F values, the \ndynamics transition to fully aperiodic, i.e., a chaotic motion, as reflected by the dense scattering of \npoints in the bifurcation diagram. \nThe corresponding phase portraits and time series confirm this progression: at F=0.5, the system \nremains close to a stable limit cycle; at F=1.0, chaotic features become more prominent; and by F=1.5, \nthe system exhibits strongly aperiodic, chaotic behavior . This trend emphasizes the role of increasing \nforcing amplitude in driving the system toward enhanced chaos. \nThe bifurcation diagram in Figure 8(a) reveals the evolution of chaotic behavior with varying F for the \nratio 2.494/1. As F increases, the size of the chaotic attractor expands. Initially, for F<0.85, the system \nexhibits a period -1 orbit, which transitions  to a period -2 orbit for 0.85<F<1.0 and then to a period -4 \norbit for 1.0<F<1.15. Between F=1.0 and F=1.25, a periodic window is observed. Beyond this range, \nthe system alternates between period -4 and period -3 orbits before entering fully chaotic dynamics. \nThus, the system displays intermittency, transitioning from chaos at low F to periodicity at intermediate \nF, and back to chaos at higher F. \nThe phase portraits support this behavior: at F=1.2, the system already shows substantial chaotic \nfeatures with multiple distinct loops, while at F=1.75, the chaotic attractor becomes wider and less structured, indicating stronger divergence and the absence of a stable limit cycle compared to earlier \nfigures (Figures 1\u20134). \nThe above discussion highlights that the system exhibits a broad range of bifurcation behaviors, \ntransitioning between periodic and chaotic dynamics depending on the ratio values. In a typical \ncontinuous system, period -doubling bifurcations are observed; h owever, with the application of the \nforcing function, the system demonstrates more complex dynamics. Thus, nonlinear oscillators \nsubjected to forcing display irregular behaviors, emphasizing the intricate interplay between periodic \nand chaotic motion. \n \nFigures and Figure Captions  \nFigure 1: Bifurcation and dynamical behavior of the forced Van der Pol oscillator at \u03bc=1, \u03c9=1, \nand varying forcing amplitudes F. (a) Bifurcation diagram showing the evolution of system \ndynamics as F varies from 0 to 4. (b, d, f, h, j) Phase portraits of the oscillator at F=0.05,0.5,1.0,2.5, \nand 4.0, respectively. (c, e, g, i, k) Corresponding time series plots for the same values of F. The state \nof the oscillator was sampled stroboscopically and synchronized with the external periodic forcing. \nAll simulations were performed numerically using a fixed time step and standard integration methods. \nThese plots illustrate transitions from periodic to increasingly complex dynamical behavior as the \nforcing amplitude increases. \n \n \nFigure 2: Dynamical response of the forced Van der Pol oscillator at \u03bc=2, \u03c9=1, and varying \nforcing amplitudes. (a) Bifurcation diagram showing the evolution of the system as the forcing \namplitude F varies from 0 to 4. Phase portraits in (b, d, f, h, j) and corresponding time series in (c, e, \ng, i, k) are shown for F=0.05, 0.5, 1.0, 2.5, and 4.0, respectively. Simulations were conducted using \nnumerical integration with fixed time steps, and state variables were sampled stroboscopically. These \nresults reveal transitions from periodic to chaotic behavior as F changes. \n \n \nFigure 3: Bifurcation and phase space dynamics of the forced Van der Pol oscillator at \u03bc=2 and \n\u03c9=3. (a) Bifurcation diagram showing the system\u2019s response as the forcing amplitude F increases \nfrom 0 to 4. Phase portraits in (b\u2013e) illustrate the oscillator\u2019s behavior at F=0.5, 3.0, 6.0, and 9.5, \nrespectively. Results highlight the emergence of complex dynamics under higher frequency forcing \nconditions. \n \nFigure 4: Dynamics of the forced Van der Pol oscillator at \u03bc=\u22120.5, \u03c9=1.0, under varying forcing \namplitudes. (a) Bifurcation diagram showing system behavior as forcing amplitude F varies from 0 to \n4. Phase portraits in (b, d, f, h) and corresponding time series in (c, e, g, i) are shown for F = 0.2, 0.5, \n1.0, and 1.5, respectively. The results reveal the influence of negative damping on the transition \nbetween periodic and aperiodic dynamics. \n \nFigure 5: Influence of frequency ratio 2:1 on the dynamics of the forced van der Pol oscillator at \n\u03bc=1. (a) Bifurcation diagram showing the variation in system dynamics as the forcing amplitude F \nincreases from 0 to 4. Phase portraits in (b) and (d), and corresponding time series in (c) and (e), \nillustrate the response of the oscillator at F=0.25 and F=0.75, respectively. Simulations were \nperformed for a frequency ratio 2:1 between the applied and natural frequencies. Each dataset \nrepresents a single trial. \n \n \nFigure 6: Effect of frequency ratio 5:3 on the dynamics of the forced van der Pol oscillator at \n\u03bc=1. (a) Bifurcation diagram illustrating changes in system behavior as the forcing amplitude F varies \nfrom 0 to 4. Phase portraits in (b) and (c) correspond to F=0.25 and F=1.75, respectively, with a 5:3 \nratio applied to natural frequency. Each simulation represents a single trial. \n \nFigure 7: Influence of frequency ratio 10:3 on the dynamics of the forced van der Pol oscillator \nat \u03bc=1. (a) Bifurcation diagram showing system behavior as the forcing amplitude F varies from 0 to \n4. Phase portraits in (b), (c), and (d) correspond to F=0.5, 1.0, and 1.5, respectively, with an applied-\nto-natural frequency ratio of 10:3. Each dataset is based on a single simulation. \n \nFigure 8: Effect of near-resonant frequency ratio (2.494:1) on the dynamics of the forced van \nder Pol oscillator at \u03bc=1. (a) Bifurcation diagram illustrating system response as forcing amplitude F \nvaries from 0 to 4. Phase portraits in (b) and (c) correspond to F=1.2 and 1.75, respectively, with an \napplied-to-natural frequency ratio of 2.494:1. Each simulation reflects one set of initial conditions. \n \nDISCUSSION \nThe forced Van der Pol oscillator exhibits nonlinear behaviors such as period -doubling and chaos, \nwhich are relevant to real -world systems in astrophysics, biology, and engineering. In celestial \nmechanics, similar transitions from periodic to chaotic orbit s occur in resonant planetary systems due \nto gravitational forcing, akin to the bifurcation patterns observed in Figures 1 \u20133 (Murray and Dermott \n[16]). In biology, the oscillator models phenomena like cardiac arrhythmias and neuronal firing, where \nincreased stimulus leads to transitions from regular to chaotic activity. These dynamics, illustrated in \nFigures 1, 2, and 4, mirror period-doubling bifurcations in heart rhythms and chaotic spiking in neurons \n(Glass and Mackey [17]). \nIn engineering, the periodic and chaotic behaviors of the forced Van der Pol oscillator are pertinent to \nnonlinear dynamics in electrical circuits and power systems. Relaxation oscillators and converters often \nexhibit bifurcations and transitions to chaos under varying input conditions, as reflected in Figures 5 \u2013\n7. In particular, increasing the forcing amplitude leads to a shift from stable periodic to chaotic behavior, \nanalogous to voltage instabilities observed in power grids. These dynamics highlight the  sensitive \ndependence on initial conditions and parameter changes, where multiple coexisting states and irregular \noscillations emerge under strong forcing, paralleling instabilities in real-world electrical systems. \nThis study examines the dynamical behavior of the forced Van der Pol oscillator under varying forcing \namplitudes, damping coefficients, and driving frequencies. The system exhibits a rich spectrum of \nresponses, as evidenced by bifurcation diagrams, phase p ortraits, and time series. At lower forcing \namplitudes, the oscillator often displays chaotic dynamics, characterized by irregular trajectories and \nscattered bifurcation branches. As the forcing amplitude increases, the system transitions to periodic \nbehavior, including stable period -1 and period -2 orbits. Intermediate forcing levels sometimes led to \nintermittent chaos, where periodic and chaotic phases alternated. At higher amplitudes, the dynamics \ngenerally stabilizes, with dominant low-period attractors and diminished chaotic behavior. These results \nunderscore the sensitivity to parameter variations of the system and the emergence of complex \ntransitions between stable and unstable regimes. \nThe dynamics of the forced Van der Pol oscillator are also found to depend strongly on the ratio of the \napplied frequency to the natural frequency of the system. Bifurcation diagrams, phase portraits, and \ntime series reveal that for integer frequency ratio s, the system exhibits a progression from periodic to \nchaotic behavior with increasing forcing amplitude. The period -2 orbits dominate at low amplitudes. \nAs the amplitude increases further, the system undergoes bifurcations, leading to higher periodicities to \nquasi-periodic-like dynamics. At higher amplitudes, densely overlapping branches emerge in the \nbifurcation diagrams, indicating chaotic or multi-periodic regimes. \nFor non-integer frequency ratios, the oscillator exhibits a shift from stable period-1 orbits at low forcing \namplitudes to higher periodicities and chaotic behavior at higher amplitudes. Phase portraits show the \nabsence of stable limit cycles, especially under irrational ratios, where intermittent dynamics emerged. \nPeriodic windows (e.g., period 2, 3, and 4) are interspersed within chaotic regimes, with intermediate \namplitudes revealing transient order before transitioning to fully aperiodic motion. \nThe dynamics of the forced Van der Pol oscillator strongly depend on the ratio of applied to natural \nfrequencies. Integer ratios yield both periodic and chaotic behavior, with chaos emerging at higher forcing amplitudes. Non-integer and irrational ratios produce complex dynamics, featuring intermittent \ntransitions across broader amplitude ranges. As forcing increases, the response of the system grows \nmore sensitive and complex. While the unforced oscill ator remains regular, forcing introduces \nirregularities, making the system suitable for modeling phenomena in astrophysics, biology (e.g., \ncardiac arrhythmias), and engineering (e.g., nonlinear circuits). \nMATERIALS AND METHODS \nIn this study, we investigated the nonlinear dynamics of the forced van der Pol oscillator by analyzing \nits behavior across a range of forcing amplitudes. The system was modeled using a second -order \nordinary differential equation (ODE) that incorporates no nlinear damping and an external periodic \nforcing term. The mathematical form of the equation used is \n\ud835\udc512\ud835\udc65\n\ud835\udc51\ud835\udc612 + \ud835\udf07(\ud835\udc652 \u2212 1)\n\ud835\udc51\ud835\udc65\n\ud835\udc51\ud835\udc61 + \ud835\udc65 =  \ud835\udc39\ud835\udc50\ud835\udc5c\ud835\udc60(\ud835\udf14\ud835\udc61), \nwhere \u03bc is the nonlinearity parameter, F is the forcing amplitude, and \u03c9 is the forcing frequency. \nThe numerical simulations were conducted in Python using the scipy.integrate.odeint routine from the \nSciPy library to solve the coupled system of first -order ODEs. The function odeint was employed to \nintegrate the system with specified initial conditions and parameter values. Specifically, the dynamics \nwere explored over a time range of t=0 to 50 seconds, sampled uniformly with 5000 points. The initial \nconditions for the state variables were set as x(0)=2 and \n\ud835\udc51\ud835\udc65(0)\n\ud835\udc51\ud835\udc61 =0. \nFor each simulation, the parameters \u03bc=1.0, \u03b2=1.0, and \u03c9=2.0 were held constant while the forcing \namplitude F was varied to examine transitions between periodic, quasi-periodic, and chaotic behavior. \nThe time series were obtained by plotting x(t) as a function of time, and phase portraits were generated \nby plotting x(t) against \n\ud835\udc51\ud835\udc65\n\ud835\udc51\ud835\udc61, revealing the nature of the attractors of the system. \nTo construct bifurcation diagrams, the forcing amplitude F was incrementally varied over a defined \nrange (e.g., 0.1 to 2.0) in 200 steps. For each value of F, the ODE was numerically solved, and the final \nsegment of the solution (last 100 time steps) was e xtracted to identify the steady-state behavior of the \nsystem. These values were plotted against their respective F values to visualize bifurcation structures \nand identify transitions between periodic and chaotic regimes. \nAll plots were generated using matplotlib.pyplot (Matplotlib library in Python) and NumPy were used \nfor numerical computations and array manipulations. The analysis enabled the classification of system \nbehavior into particular dynamical regimes, supported by time series, phase portraits, and bifurcation \ndiagrams. \n \nREFERENCES \n \n1. Van der Pol, B. \"On Relaxation Oscillations.\" The London, Edinburgh, and Dublin \nPhilosophical Magazine and Journal of Science, vol. 2, 1926, pp. 978\u2013992. 2. Ginoux, Jean-Marc, and Christophe Letellier. \"Van der Pol and the History of Relaxation \nOscillations: Toward the Emergence of a Concept.\" Chaos: An Interdisciplinary Journal of \nNonlinear Science, vol. 22, no. 2, 2012, 023120. https://doi.org/10.1063/1.3670008. \n3. Girotti, M. \"Van der Pol Oscillator.\" Mathemanu, 2021, pp. 1\u201312. \nhttps://mathemanu.github.io/VanderPol.pdf. \n4. Tsatsos, M. C. \"The Van der Pol Equation.\" ResearchGate, 2008, pp. 1\u201315. \nhttps://www.researchgate.net/publication/1914758_The_Van_der_Pol_Equation. \n5. Chughtai, F. A. \"Examining the Van der Pol Oscillator: Stability and Bifurcation Analysis.\" \narXiv, vol. 2309, 2023, 07010. https://arxiv.org/pdf/2309.07010. \n6. Venkatesan, A., and M. Lakshmanan. \"Bifurcation and Chaos in the Double-Well Duffing\u2013\nVan der Pol Oscillator: Numerical and Analytical Studies.\" Physical Review E, vol. 56, no. 6, \n1997, p. 6321. https://doi.org/10.1103/PhysRevE.56.6321. \n7. Jiang, W., and J. Wei. \"Bifurcation Analysis in Van der Pol's Oscillator with Delayed \nFeedback.\" Journal of Computational and Applied Mathematics, vol. 213, no. 2, 2008, pp. \n604\u2013615. https://doi.org/10.1016/j.cam.2007.01.041. \n8. Ku, Y. H., and X. Sun. \"Chaos in Van der Pol's Equation.\" Journal of the Franklin Institute, \nvol. 327, no. 2, 1990, pp. 197\u2013207. https://doi.org/10.1016/0016-0032(90)90016-C. \n9. Pinto, C. M. A., and J. A. T. Machado. \"Complex-Order Forced Van der Pol Oscillator.\" \nCiteseerX, 2011, pp. 1\u201310. \nhttps://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0a1b78f0160acbfc99a84ea\n560f0d82b4d1288ab. \n10. Sharma, Arun. \"A Bifurcation Analysis of the Forced Van der Pol Oscillator.\" Queen Mary \nUniversity of London, 2014, pp. 1\u201320. https://www.math.uni-\nrostock.de/~wj/projects/Oscillators/Sharma_Arun.pdf. \n11. Arrowsmith, D. K., and K. I. Taha. \"Bifurcations of a Particular Forced Van der Pol \nOscillator.\" Meccanica, vol. 18, 1983, pp. 195\u2013204. https://doi.org/10.1007/BF02128243. \n12. Xu, J.-X., and J. Jiang. \"The Global Bifurcation Characteristics of the Forced Van der Pol \nOscillator.\" Physica D: Nonlinear Phenomena, vol. 85, no. 2, 1996, pp. 199\u2013215. \nhttps://doi.org/10.1016/0960-0779(95)00045-3. \n13. Holmes, P. J., and D. A. Rand. \"Bifurcations of the Forced Van der Pol Oscillator.\" Quarterly \nof Applied Mathematics, vol. 35, no. 4, 1978, pp. 495\u2013509. \nhttps://doi.org/10.1090/qam/492551. \n14. \u0160ev\u010d\u00edk, J., and L. P\u0159ibylov\u00e1. \"Forced Van der Pol Oscillator\u2014Synchronization from the \nBifurcation Theory Point of View.\" Chaos: An Interdisciplinary Journal of Nonlinear \nScience, vol. 22, no. 2, 2022, 023120. https://doi.org/10.1007/978-3-030-96964-6_29. \n15. Cooper, M., P. Heidlauf, and T. Sands. \"Controlling Chaos\u2014Forced Van der Pol Equation.\" \nMathematics, vol. 5, no. 4, 2017, 70. https://doi.org/10.3390/math5040070. 16. Murray, Carl D., and Stanley F. Dermott. Solar System Dynamics. Cambridge University \nPress, 2000. https://doi.org/10.1017/cbo9781139174817. \n17. Jia, B., H.-G. Gu, L. Li, and X. Zhao. \"Dynamics of Period-Doubling Bifurcation to Chaos in \nthe Spontaneous Neural Firing Patterns.\" Cognitive Neurodynamics, vol. 6, no. 1, 2012, pp. \n89\u2013106. https://doi.org/10.1007/s11571-011-9184-7. \n \nACKNOWLEDGEMENTS \nNo acknowledgement. \n \nAPPENDIX \nThe codes have been generated in Python. The codes are available at the link: \nhttps://github.com/Astriek/bifurcation/blob/main/bifrucation.py ",
      "arXiv:2505.22700v1  [math-ph]  28 May 2025\nAggregation of vortex structures in 2D: the blob-wave system\nand its role in zonal flows\nFranco Flandoli*, Matteo Palmieri**, and Milo Viviani*\n*Scuola Normale Superiore, Pisa\nfranco.flandoli@sns.it and milo.viviani@sns.it\n**Max Planck Institute for Mathematics in the Sciences, Leipzig\nmatteo.palmieri@mis.mpg.de\nAbstract\nWe give a rigorous mathematical result, supported by accurate numerical simulations, of the\naggregation of a concentrated vortex blob with an underlying non constant vorticity field: the blob\nmoves in the direction of the gradient of the field. It is a unique example of Lagrangian explanation\nof aggregation of vortex structures of the same sign in 2D inviscid fluids. A conceptual model of\nzonal flow maintenance and modulation based on this idea is then developed, which produces\nshapes in accordance with observations.\n1 Introduction\nA long-standing open problem in 2D fluid dynamics is the Lagrangian explanation of aggregation\nof vortex structures of the same sign. Two examples of aggregation, that are visible in numerical\nsimulations, is when two blob of similar size, sufficiently close each other, start to mix and become a\nsingle blob - with filaments and residual small-scale structures; or when a small blob is absorbed by a\nlarger one, as it happens for instance for the Red Jupiter Spot or for zonal flows. We have gathered a\nlarge amount of information on vorticity dynamics, see for instance [1], [3], [6], [9], [14], [15], [18], but\nthe aggregation of vortex structures remains poorly understood; at least on a dynamical Lagrangian\nlevel - at Fourier level, which however is not based on vortex blobs but waves, there are explanations.\nAn exception is the theory of Onsager on negative temperature ensambles [19], [10], which provides\ninformation on the final vortex structures emerged after several aggregations (see for instance [4], [13]);\nhowever, in spite of its enormous importance, it fails to explain why aggregation occurs in relatively\nshort time between two structures (see a discussion in [12]).\nWe give here a rigorous mathematical result, supported also by detailed numerical simulations,\nconcerning one aspect of the Lagrangian aggregation phenomena: we investigate a small blob with\nrelatively high circulation embedded into a more rarified vorticity field, with a gradient of vorticity.\nThe result is that the blob moves along the gradient of the underlying field , namely it goes in the\ndirection of the highest values of vorticity. Said another way, if the blob at time zero is in the\nboundary region of a vorticity layer, where the vorticity is small, it moves in the direction of the core\nof the layer. It is one aspect of the absorption of small structures by large ones.\nThe numerical simulations are performed by a variant of the Zeitlin model, on the rotating 2D\nsphere, that we call the vortex-wave Zeitlin model. Numerics confirms the theoretical result and\nallows us to observe a certain scaling law for the displacement of the small blob in the direction of the\ngradient of the field, see Figure 2.\nA main motivation for the theoretical and numerical result of this work is its role in zonal flow\ntheory. Zonal flows are observed in the atmosphere of planets [21] and in confined plasma [8]. Usually,\ntheir emergence, maintenance and structure are understood by analyzing the interaction of waves and\nby spectral analysis, see for instance [20]. Their understanding from a Lagrangian viewpoint is less\ndeveloped. We are not sure that the result proved here is an essential step in the understanding of\nthe emergence of zonal flows, but we think it is a key property for the maintenance of zonal flows\n1and their shape-modulation. The latter two properties are related to the inverse energy transfer from\nsmall scales to large ones, of nonlocal type compared to the more local one of the inverse cascade. We\ncomplete this introduction with a short discussion of this topic.\nWe could identify three main ingredients of the process of maintenance and shape-modulation: i)\nthe aggregation of small blobs with layers of the same sign of vorticity - which we could also call a pref-\nerential concentration; ii) the well-known beta-effect [5], namely a tendency for concentrated positive\nvortex structures to move towards the North pole (assuming anti-clockwise rotation of the planet) and\ntowards the South pole for the negative vortex structures; iii) a form of stochasticity, corresponding\nto some degree of turbulence of the small-scale structures. Using these ingredients, we have devised a\nsimplified conceptual Lagrangian model which provides zonal modulation qualitatively in agreement\nwith known observations. For coherence with the general scheme, we also give a Lagrangian rigorous\nproof of the beta-effect, similar to our main proof of aggregation.\nSince our aim is to understand conceptually maintenance and modulation of zonal flows, let us\nexplain conceptually how the three mechanisms act. Small scale structures are produced by some in-\nstability mechanism, external to our model, which pumps energy into the system at small scales, from\nthe reservoir of energy due to the warmer core of the planet\u2019s atmosphere; the convective instabilities\nmanifest on the 2D surface in the form of small-scale perturbations. Thus we start from several con-\ncentrated vortex structures of different signs, more or less uniformly distributed in the 1-dimension of\nthe model; the 1-dimensionality of the model implicitly incorporates an a priori existence of horizontal\nzonal structure, hence this model is not accurate enough to explain the formation of zones, but only\ntheir modulation by further turbulent small-scale eddies. Due to their random motion, small concen-\ntrations of vortex structures with the same sign occur spontaneously, in a statistical way. As they\nemerge, they would disappear at random; but before they disappear the preferential concentration\nenters into play. The small regions with higher concentration of vortex structures of the same sign\nproduce locally a non-zero gradient of background vorticity, attracting other nearby vortices of the\nsame sign and repulse those of opposite sign. This way, the regions of higher concentration stabilize\nand grow. Randomness modulates this growth avoiding pointwise accumulation. Simultaneously, the\nbeta-drift modulates this picture by sharpening the transition point between positive and negative re-\ngions when the positive region is below the negative one (since the positive vortices tend to go upward\nand the negative downward); on the contrary, the transition layer when the positive region is above\nthe negative one looks more spread, less sharp. This is precisely what is observed in real zonal flows.\nIf we modulate beta depending on latitude, we may observe a larger zone at the equator, as it happens\nin reality.\n2 The vortex-wave system\nIn this section, we recall the results of Marchioro and Pulvirenti about the vortex-wave system (see\n[16]). 1 They consider solutions of the 2d Euler equations with initial condition \u03c90 being the sum a\nbounded part \u00af\u03c90(x) \u2208 L\u221e(S2) \u2229L1(S2) and a point vortex of intensity \u03bb located in x0. Then \u03c90 is the\nfinite (possibly signed) measure\n\u03c90(dx) = \u00af\u03c90(x)dx + \u03bb\u03b4x0 (dx).\nIn [16], the time-dependent measure\n\u03c9(t, dx) = \u00af\u03c9(t, x)dx + \u03bb\u03b4xv(t)(dx)\nis said to satisfy the 2d Euler equations if \u00af\u03c9 \u2208 L\u221e([0, +\u221e), L1 \u2229 L\u221e), xv \u2208 C1([0, +\u221e), S2) and they\nsatisfy the following system of equations\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n\u02d9\u03a6t(x) = \u00afut(\u03a6t(x)) + \u03bbK(\u03a6t(x) \u2212 xv(t)), \u03a60(x) = x, x\u0338= x0\n\u02d9xv(t) = \u00afut(xv(t)), xv(0) = x0\n\u00afut = K \u2217 \u00af\u03c9t\n\u00af\u03c9t(dx) = \u00af\u03c90(\u03a6\u22121\nt (x))\n(1)\n1For our later scopes, we give the results on S2, rather than R2. We have not seen any crucial difference in changing\nthe reference domain to extend the results of [16].\n2Here \u00afu is the velocity field given by \u00af\u03c9, the bounded part of the vorticity, and \u03a6 the flow generated\nby \u00afu. The point vortex xv follows only the velocity field \u00afu, discarding the self-interaction, while \u00af \u03c9\nevolves transported by the sum of the divergent velocity field of the vortex and its velocity field \u00af u.\nThe function K is the Biot-Savart kernel on S2\nK(x) = 1\n2\u03c0\nx\u00d7\n|x|2 .\nSome results from [16] are summarised in the following theorem.\nTheorem 1. Given initial conditions \u00af\u03c90 \u2208 L\u221e(S2) \u2229 L1(S2), x0 \u2208 S2 and \u03bb \u2208 R\\{0}, there exists\na global in time solution of system (1). Under the additional assumption that \u00af\u03c90 is constant in a\nneighbourhood of x0, the solution is unique and it is the weak-* limit (in the sense of finite measures)\nof the solutions starting from the bounded initial conditions\n\u03c9\u03b5\n0(x) = \u00af\u03c90(x) + \u03bb\n\u03c0\u03b52 \u03c7B(x0,\u03b5).\nDue to the divergence of the vector field of the vortex, the case when \u00af \u03c90 is not locally constant is\nless understood. In the smooth case, \u00af \u03c90 is not locally constant in particular if \u2207\u00af\u03c90(x0) \u0338= 0. Under\nthis assumption, we provide numerical simulations (Section 3) and a rigorous proof (Section 4) that\nthe vortex perceives a very strong acceleration proportional to \u03bb\u2207\u00af\u03c9(x0).\n3 Direct numerical simulations of the vortex-wave system\nIn this section, we specifiy the vortex-wave equations (1) on a rotating 2-sphere S2 \u2282 R3. These\nequations have been first introduced by Bogomolov [2] and more recently studied by Laurent\u2013Polz in\n[11], in the context of geometric mechanics. Formally, we set the equations in the following notations:\n\u2022 Position of the point vortices: xi : [0, T] \u2192 S2, for i = 1, ..., NV .\n\u2022 Background vorticity: \u03c9 : [0, T] \u00d7 S2 \u2192 R.\n\u2022 Coriolis vorticity : f = f(x) := \u2126 \u00b7 x, for \u2126 \u2208 R3 angular velocity of the rotating sphere.\nThen the vortex-wave equations on a rotating sphere are given by:\n\u02d9xi = 1\n4\u03c0\nP\nj=1,..,NV\ni\u0338=j\n\u0393j\nxi \u00d7 xj\n1 \u2212 xi \u00b7 xj\n+ xi \u00d7 \u2207\u2206\u22121(\u03c9 \u2212 f)(xi) for i = 1, ..., NV\n\u02d9\u03c9 = {\u03c9, \u2206\u22121(\u03c9 \u2212 f) + P\ni=1,..,NV\n\u0393i log(1 \u2212 xi \u00b7 x)}.\n(2)\nWe remark that the Laplace-Beltrami operator on the sphere \u2206 is invertible on the space of functions\nwith zero mean. Equations (2) are a infinite dimensional Hamiltonian system (more precisely Lie\u2013\nPoisson system) with energy\nH = 1\n2\nX\ni\u0338=j\n\u0393i\u0393j log(1 \u2212 xi \u00b7 xj) +\nX\ni\n\u0393i(\u03c9 \u2212 f)(xi) + 1\n2\nZ\nS2\n\u2206\u22121(\u03c9 \u2212 f)(\u03c9 \u2212 f).\nEquations (2) also have additional Casimir invariants given by Cf =\nR\nS2 f(\u03c9), for any real function f\nand the angular momentum M =\nR\nS2 (P\ni \u0393i\u03b4xi + \u03c9)ndS \u00b7 \u2126, for \u2126 \u0338= 0.\nA spatial discretization which retains many conservation laws is known for the 2D Euler equations\nas the Zeitlin model [22]. Applying the same approach to equations (2), we obtain the vortex-wave\u2013\nZeitlin model. Given the expansion of the vorticity field in spherical harmonics Ylm, we take the\ntruncated expansion for l \u2264 N \u2212 1, m= \u2212l, ..., l. Then, for any N \u2265 1, we have an explicit vector\nspace isomorphism between the classical spherical harmonics {Ylm}l=1,..,N\u22121\nm=\u2212l,...,l\nand a basis of su(N)\n(skew-Hermitian matrix with zero trace), usually denoted in literature with {Tlm}l=1,..,N\u22121\nm=\u2212l,...,l\n, such that\n3the structure constants of the su(N) brackets with respect to this basis converge to the ones of the\nspherical harmonics. Finally the vortex-wave\u2013Zeitlin is given by:\n\u02d9xi = 1\n4\u03c0\nP\nj=1,..,NV\ni\u0338=j\n\u0393i\nxi \u00d7 xj\n1 \u2212 xi \u00b7 xj\n\u2212 P\nl=1,..,N\u22121\nm=\u2212l,...,l\nxi \u00d7 \u2207Ylm(xi)\n\u2212l(l + 1) (W \u2212 F)lm for i = 1, ..., NV\n\u02d9W = \u2212[W,\u2206\u22121\u0000\nW + P\ni=1,..,NV\nl=1,..,N\u22121\nm=\u2212l,...,l\n\u0393iYlm(xi)Tlm \u2212 F\n\u0001\n]N ,\n(3)\nwith Hamiltonian:\nH = \u2212 1\n8\u03c0\nP\ni,j=1,..,NV\ni\u0338=j\n\u0393i\u0393j log(1 \u2212 xi \u00b7 xj)+\n\u2212P\ni=1,..,NV\nl=1,..,N\u22121\nm=\u2212l,...,l\n\u0393i\nYlm(xi)\n\u2212l(l + 1)(W \u2212 F)lm+\n\u22121\n2 Tr(\u2206\u22121(W \u2212 F)\u2217(W \u2212 F)).\n(4)\nEquations (3) have the following independent first integrals:\nCn(W) = Tr(Wn), for n = 2, ..., N\nM =\n\uf8eb\n\uf8edPNV\ni=1 \u0393ixi +\n\uf8ee\n\uf8f0\nTr(W\u2217Tx)\nTr(W\u2217Ty)\nTr(W\u2217Tz)\n\uf8f9\n\uf8fb\n\uf8f6\n\uf8f8 \u00b7 \u2126,\n(5)\nwhere Tx = T1\u22121 + iT1\u22121, Tx = T1\u22121 \u2212 iT1\u22121, Tz = T10.\nA conservative time-discretization of equations (3) can be obtained by extending the method used\nin [17] to the coupled vortex-wave system. Therefore, we obtain a Lie\u2013Poisson integrator for (3) that\nexactly conserves the integrals Cn and M and nearly conserves the Hamiltonian H. In Figure 1, we\nillustrate an example of a DNS of (3), solved with the Lie\u2013Poisson integrator developed in [17], in which\n6 identical point-vortices are placed in the vertices of a regular equatorial hexagon and a background\nsmooth vorticity equal to the Coriolis one.\n4 Local dynamics of the blob-wave system\nIn this section, for the sake of clarity of the calculations, we take as reference domain the flat torus T2.\nWe remark that the results can be straightforwardly adapted to the sphere S2 with \u03c90 depending only\non the latitude (cf. Theorem 2). The aim of this section is to rigorously prove some results concerning\nthe dynamics of a concentrated blob of vorticity with respect to a locally stratified wave.\n4.1 Motion along the gradient\nLet \u00af\u03c90(x) be a stationary smooth solution of the Euler equations on the torus T2 and \u00afu0 be the\nassociated velocity field. Stationarity of \u00af \u03c90 is equivalent to saying that at every x \u2208 T2, the vectors\n\u00afu0(x) and \u2207\u00af\u03c90(x) are orthogonal.\nA passive particle immersed in the fluid moves according to the differential equation \u02d9 x = \u00afu0(x).\nA point vortex with non-zero vorticity modifies the underlying vorticity field \u00af \u03c90, so that its motion\ndeviates from that of a passive particle. We want to describe this deviation, namely the motion along\nthe direction perpendicular to \u00afu0(x), so parallel to \u2207\u00af\u03c90(x) (whenever these two vectors are non-zero).\nTo make our calculation rigorous we will not use the exact vortex-wave model, but instead a blob-\nwave system of equations, where we substitute the delta Dirac with a smooth, compactly supported\nfunction with a diameter of the support of order \u03b5 \u226a 1. To be more precise, let x0 \u2208 T2 (the center of\nthe blob), \u03bb \u2208 R (the vorticity the blob) and \u03c10(x) \u2208 C\u221e\nc (R2) be a smooth function (the blob profile)\nsuch that\n\u03c10 \u2265 0\nZ\n\u03c10(x)dx = 1\nZ\nx\u03c10(x)dx = 0 supp \u03c10 \u2282 B(0, 1)\n4Figure 1: From the top-left to the bottom-right, time snapshots of the evolution of the vorticity field \u03c9\nobtained by a DNS of (2) for N = 501, plotted in azimuth-elevation coordinates. The initial (unstable)\nequilibrium of 6 point-vortices, all with the same sign, surrounded by an initial smooth field equal to\nthe Coriolis vorticty \u03c90 = 2\u2126 cos\u03b8, evolves into a coupled vortex-wave dynamics.\nand, for every \u03b5 >0, define its rescalings as\n\u03c1\u03b5(x) = 1\n\u03b52 \u03c10\n\u0012x \u2212 x0\n\u03b5\n\u0013\n.\nWe define the blob-wave dynamics as the solution of the following problem\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n\u2202t \u00af\u03c9 + u \u00b7 \u2207\u00af\u03c9 = 0 \u00af \u03c9(0, x) = \u00af\u03c90(x)\n\u2202t\u03c1 + u \u00b7 \u2207\u03c1 = 0 \u03c1(0, x) = \u03bb\u03c1\u03b5(x)\nu(t, x) =\nZ\nTd\nK(x, y)(\u00af\u03c9(t, y) + \u03c1(t, y))dy\nwhere \u00af\u03c9(t, x) and \u03c1(t, x) describe respectively the evolution of the large-scale vorticity field \u00af \u03c9 and of\nthe vortex \u03c1. The total vorticity \u03c9 = \u00af\u03c9+\u03c1 solves the Euler equations starting from the initial condition\n\u03c9(0, x) = \u00af\u03c90(x) + \u03bb\u03c1\u03f5(x).\nIn [16] it is proven, in a slightly different setting, that the blob-wave dynamics converges to the\nvortex-wave system, possibly allowing for a greater number of blobs, but with the additional assumption\nthat the underlying vorticity field \u00af\u03c90 is locally constant around the initial position of the blobs.\nFollowing [16], we introduce the barycenter of the blob\nT2 \u220b G(t) =\nZ\nx\u03c1(t, x)dx\n.Z\n\u03c1(t, x)dx = 1\n\u03bb\nZ\nx\u03c1(t, x)dx.\nTheorem 2. In the setting described above, assume that \u00af\u03c90(x) depends only on the second coordinate\nx2 and that, without loss of generality, \u03bb >0 and \u22022 \u00af\u03c90(x0) > 0. Let G2(t) = G(t) \u00b7 \u02c6x2 be the second\ncomponent of the barycenter. Then exists a positive \u03b50 = \u03b50(\u00af\u03c90, \u03c10) such that for every \u03b5 \u2208 (0, \u03b50) there\nexists a time T = T(\u03b5) > 0 such that G2(t) is stricly increasing in [0, T].\nProof. Let us think of \u03c1 as a perturbation and look at how the macroscopic vorticity \u00af \u03c9 and the\nmacroscopic velocity \u00afu = K \u2217 \u00af\u03c9 are modified by this perturbation. Call v = K \u2217 \u03c1, then\n\u2202t \u00af\u03c9(0, x) = \u2212\u00afu(0, x) \u00b7 \u2207\u00af\u03c90 \u2212 v(0, x) \u00b7 \u2207\u00af\u03c90 = \u2212v(0, x) \u00b7 \u2207\u00af\u03c90,\n5where the last equality is true since \u00af\u03c90 is a stationary solution. We obtain\n\u2202t\u00afu(0, x) = K \u2217 \u2202t \u00af\u03c9(0, x) = \u2212K \u2217 (v(0, x) \u00b7 \u2207\u00af\u03c90). (6)\nThe velocity of the barycenter is\nG\u2032(t) = 1\n\u03bb\nZ\nx\u2202t\u03c1dx = 1\n\u03bb\nZ\n\u03c1(\u00afu + v)dx = 1\n\u03bb\nZ\n\u03c1\u00afudx,\nwhere in the last equality we used that\nZ\n\u03c1vdx =\nZ\nK(x, y)\u03c1(x)\u03c1(y)dxdy\nwhich is equal to 0 due to the antisymmetry of the kernel K (this corresponds to the fact that the\nvector field generated by the vortex itself does not change its barycenter).\nWe have that\nG\u2032(0) \u00b7 \u02c6x2 = 1\n\u03bb\nZ\n\u03c1\u00afu(0, x) \u00b7 \u02c6x2dx = 0,\nsince the macroscopic vector field at time 0 is a shear flow parallel to the \u02c6x1 axis, due to the choice of\n\u00af\u03c90. The motion of the vortex for small times will be given by the second derivative\nG\u2032\u2032(t) = 1\n\u03bb\nZ\n\u2202t\u03c1\u00afudx + 1\n\u03bb\nZ\n\u03c1\u2202t\u00afudx.\nEvaluating at time 0, using the initial condition \u03c1(0, x) = \u03bb\u03c1\u03b5(x) and Equation (6), we have\nG\u2032\u2032(0) \u00b7 \u02c6x2 = \u02c6x2\n\u03bb \u00b7\nZ\n\u03bb\u03c1\u03b5(x)(\u2212K \u2217 (v(0, x) \u00b7 \u2207\u00af\u03c90))dx =\n= \u02c6x2\n\u03bb \u00b7\nZ\n(K \u2217 \u03bb\u03c1\u03b5)(v(0, x) \u00b7 \u2207\u00af\u03c90)dx = \u02c6x2\n\u03bb \u00b7\nZ\nv(0, x)(v(0, x) \u00b7 \u2207\u00af\u03c90)dx.\nSince \u2207\u00af\u03c90 = \u2202x2 \u00af\u03c90 \u02c6x2, we obtain\nG\u2032\u2032(0) \u00b7 \u02c6x2 =\nZ\n(v(0, x) \u00b7 \u02c6x2)2\u22022 \u00af\u03c90dx.\nAssume without loss of generality that the vortex is centred at time 0 in x0 = 0. Let c, \u03b4be positive\nreal constants such that\n\u2202x2 \u00af\u03c9(x) \u2265 c >0 for every x \u2208 B(0, \u03b4).\nLet K2 = K\u00b7\u02c6x2 be the second component of the Biot-Savart kernel. Splitting the domain of integration\nwe have\nG\u2032\u2032(0) \u00b7 \u02c6x2 = \u03bb\nZ\nB(0,\u03b4)c\n(K2 \u2217 \u03c1\u03b5)2\u2202x2 \u00af\u03c90dx + \u03bb\nZ\nB(0,\u03b4)\n(K2 \u2217 \u03c1\u03b5)2\u2202x2 \u00af\u03c90dx.\nObserve that K2 is not in L2(T2) but\nZ\nB(0,\u03b4)c\n|K2|2dx <+\u221e\nfor every positive \u03b4. We have then that the first integral converges as \u03b5 goes to 0 to\n\u03bb\nZ\nB(0,\u03b4)c\n|K2|2\u2202x2 \u00af\u03c90dx.\nFor the opposite reason, since K2 is not in L2(B(0, \u03b4)), we have\nlim\n\u03b5\u21920\nZ\n(K2 \u2217 \u03c1\u03b5)2\u2202x2 \u00af\u03c90dx \u2265 lim\n\u03b5\u21920\nc\nZ\n(K2 \u2217 \u03c1\u03b5)2dx = +\u221e\nThis implies that exists an \u03b50 = \u03b50(\u00af\u03c90, \u03c10) such that for every \u03b5 \u2208 (0, \u03b50) we have that G\u2032\u2032(0) \u00b7 \u02c6x2 > 0\nand this concludes the proof.\n6Figure 2: Vertical coordinate trajectory y = y(t) of a point-vortex in a zonal flow, in the hypothesis\nof the results above, plotted in loglog scale and obtained by a DNS of (3), for N = 65. The slope of\nthe curve, after an initial short time, is in (1 , 2), denoting the second derivative being infinity. In the\nvery initial time, the slope of the curve is approximately 2. This is due to the finite spatial resolution\nat which (3) is solved. We have empirically observed that increasing the resolution results in a shift\nto the left of the kink between the two slopes.\nRemark 3. From the proof, one can see that the general case (without making assumptions of the\nsigns of \u03bb and \u2202x2 \u00af\u03c90(x0)) is that the map\nt 7\u2192 sign(\u03bb\u2202x2 \u00af\u03c90(x0))G2(t)\nis strictly increasing.\nRemark 4. Following the same lines of the proof above, the theorem can be extended in the following\nway. Assume \u00af\u03c90(x) is a stationary solution of the Euler equations (not necessarily depending only on\nx2). Without loss of generality, assume that \u03bb >0 and that \u2207\u00af\u03c90(x0) = \u22022 \u00af\u03c90(x0)\u02c6x2 with \u22022 \u00af\u03c90(x0) > 0.\nCall G\u03b5(t) the position of the barycenter depending on the parameter \u03b5. Then\nlim\n\u03b5\u21920\nG\u03b5\n2(0)\u2032 = 0 lim\n\u03b5\u21920\nG\u03b5\n2(0)\u2032\u2032 = +\u221e.\n4.2 Northward motion of positive vortices\nA well-known phenomenon is the northward motion of positive vortices in rotating systems (see, for\nexample, [5]). Exploiting the strategy used in Theorem 2, we provide a possible formalization of this\ntendency.\nAs before, let us assume that we are on the two-dimensional torus and let \u03c10 \u2208 C\u221e(T2) be a\npositive smooth function (the profile of the vortex). The overall vorticity field evolves according to the\n\u03b2-plane approximation (\n\u2202t\u03c9 + u \u00b7 \u2207\u03c9 + \u03b2uy = 0 u = K \u2217 \u03c9\n\u03c9(0, x) = \u03c10(x) .\nWe split \u03c9 as \u00af\u03c9 + \u03c1 where \u03c1 is the evolution of the vortex (just by transport of the vector field u)\nand \u00af\u03c9 is the remaining part\n(\n\u2202t\u03c1 + u \u00b7 \u2207\u03c1 = 0 \u03c1(0, x) = \u03c10(x)\n\u2202t \u00af\u03c9 + u \u00b7 \u2207\u00af\u03c9 + \u03b2u2 = 0 \u00af \u03c9(0, x) = 0\n7Let\nG(t) =\nZ\nx\u03c1(t, x)dx\n.Z\n\u03c1(t, x)dx\nbe the barycenter of the vortex as before.\nProposition 5. In the setting above, assuming that \u03b2 is positive and that \u03c10 is a stationary solution\nof the Euler equations, there exists a positive time T such that G2(t) is strictly increasing in [0, T].\nProof. Up to a time change, one can assume that\nR\n\u03c1dx =\nR\n\u03c10dx = 1. With calculations similar to\nTheorem 2\nG\u2032(0) =\nZ\nx\u2202t\u03c1(0, x)dx =\nZ\nxdiv(u\u03c1)dx =\nZ\n(K \u2217 \u03c10)\u03c10dx = 0\nLooking at the second derivative\nG\u2032\u2032(0) =\nZ\nu(0, x)\u2202t\u03c1(0, x)dx +\nZ\n\u03c1(0, x)\u2202tu(0, x)dx =\nZ\n\u03c10(x)\u2202tu(0, x)dx\nwhere we used that \u2202t\u03c1(0, x) = 0 since \u03c10 is a stationary solution of the Euler equations. We have that\n\u2202tu(0, x) = K \u2217 \u2202t\u03c9(0, x) and\n\u2202t\u03c9(0, x) = \u2212u(0, x) \u00b7 \u2207\u03c1(0, x) \u2212 \u03b2u2(0, x) = \u2212(K \u2217 \u03c10) \u00b7 \u2207\u03c10 \u2212 \u03b2K2 \u2217 \u03c10 = \u2212\u03b2K2 \u2217 \u03c10\nSubstituting we obtain\nG\u2032\u2032(0) =\nZ\n\u03c10\u2202tu(0, x)dx = \u2212\u03b2\nZ\n\u03c10K \u2217 (K2 \u2217 \u03c10)dx = \u03b2\nZ\n(K \u2217 \u03c10)(K2 \u2217 \u03c10)dx\nTaking the second component:\nG\u2032\u2032(0) \u00b7 \u02c6x2 = \u03b2\nZ\n|K2 \u2217 \u03c10|2dx >0\nThe function t 7\u2192 G(t) \u00b7 \u02c6x2 has null first derivative and positive second derivative, so it is increasing in\nan interval [0, T].\n5 A 1D Lagrangian phenomenological model of the zonal flows\nstructure\nIn this section, using the results of the previous sections, we propose a 1-dimensional model displaying\nsome of the characteristic features of zonal flows. It is a conceptual model, meaning that the various\nterms of the equation reproduce in a simplified fashion the same effects of the true physical model, as\nunderstood in the sections above. Let N \u2265 1 be a number of particles, Xi for i = 1, . . . , Nbe their\npositions (in the interval [ \u22121, 1]) and \u03bbi their vorticities. The particles Xi describe the latitudinal\npositions of the vortices, discarding the longitudinal motion. Assume that the vortices move inside a\nmacroscopic vorticity field \u00af\u03c9. Given \u03b1, \u03b2 >0, consider the following system of SDEs:\ndXi\nt = \u03bbi\u03b1\u00af\u03c9\u2032(t, Xi\nt)dt + \u03bbi\u03b2(Xi\nt)dt +\n\u221a\n2\u03badWi\nt (7)\nwhere {Wi}N\ni=1 is a collection of independent Brownian motions constructed on a common probability\nspace. The equations are constructed as follows:\n\u2022 the aggregation term \u03b1\u03bbi \u00af\u03c9\u2032(Xi\nt) is designed inspired by Theorem 2. A positive vortex deviates\nfrom a purely latitudinal motion following the gradient of \u00af\u03c9 (in one dimension, just the deriva-\ntive). This deviation is proportional to the intensity \u03bbi. The parameter \u03b1 depends on the shape\nof the vortex.\n\u2022 the term \u03bbi\u03b2(Xi\nt) models the beta-effect, following Theorem 5. \u03b2 : [\u22121, 1] \u2192 [0, +\u221e) may be\nchosen to be constant or dependent on the position of the vortex.\n8Figure 3: From the top-left to the bottom-right, time snapshots of the evolution of the vorticity field\nobtained by a DNS of the Euler\u2013Zeitlin equations, i.e. (2) with no point-vortices, for N = 257, plotted\nin azimuth-elevation coordinates. The simulation displays the northward motion of the of a vorticity\nblob in a spherical domain, under the effect of the Coriolis force.\n\u2022 each vortex experiences a random force\n\u221a\n2\u03badWi\nt modelling the small-scale fluctuations of the\nfluid existing in the system.\nWe finally assume that the background vorticity field \u00af \u03c9 is itself generated by the N vortices,\nobtaining the model (\ndXi\nt = \u03bbi\u03b1\u00af\u03c9\u2032(t, Xi\nt)dt + \u03bbi\u03b2(Xi\nt)dt +\n\u221a\n2\u03badWi\nt\n\u00af\u03c9(t, x) = \u03c6\u03b5(x) \u2217 PN\ni=1 \u03bbi\u03b4Xi\nt\nwhere \u03c6\u03b5(x) = \u03b5\u22121\u03c6(\u03b5\u22121x) is a classical mollifier.\nImagine that particles Xi are initially uniformly spread in the interval [\u22121, 1]. After a while, due to\nthe diffusion term, in some regions the vorticity is slightly unbalanced (either in the positive or negative\ndirection). These fluctuations are amplified by the aggregations term \u03bbi\u2202x\u03c9t(Xi) which keeps together\nvortices of the same sign, allowing for a stronger nucleation phenomenon. The competition between\nthe diffusion and the attraction terms gives stability to the system and we have alternating regions\nof positive and negative vorticity. At this moment, enters the effect of the \u03b2-term. Assume the \u03b2 is\nzero and the profile of \u03c9 has a sinusoidal shape and look at two neighbouring picks of positive and\nnegative vorticity (with the positive one below). If now \u03b2 is increased to a positive value, particles in\nthe positive region will try to go to the right and those in the negative one to the left. Due to the\nrepulsion, if \u03b2 is not very strong, the only effect will be that of bringing closer the two picks. This has\nalready been observed for the complete two-dimensional \u03b2-plane in [7] where the vorticity profile has\nsharp transitions from left to right (which results in a belly-tip profile for the velocity). While partially\nunclear for the true physical system, our model provides a simple explanation of the phenomenon.\n5.1 Difficulties with the mean field limit\nIt is natural to take the limit as the number N of particles goes to infinity and consider, in place of\nthe particle system above, the related mean field Fokker-Planck equation\n(\n\u2202t\u03c9 + \u03b1\u2202x(\u03c9\u2202x\u03c9) + \u2202x(\u03c9\u03b2) = \u03ba\u2206\u03c9\n\u03c9|t=0 = \u03c90\nThe term \u03b1\u2202x(\u03c9\u2202x\u03c9) is originally of the form \u03b1\u2202x(\u03c9\u2202x(\u03c6\u03b5 \u2217\u03c9)) but, taking the limit as N \u2192 \u221e, it\nis natural to take simultaneously also the limit as \u03b5 \u2192 0. Let us remark that here we have written, for\n9(a) Vorticity profile.\n (b) Velocity profile.\nFigure 4: Vorticity and velocity simulated using the SDE model in (7) (parameters: N = 2000, \u03bbi =\nsign(i \u2212 1000), \u03b1= 0.25, \u03b2= 0.35, \u03c3= 1.25 \u00b7 10\u22123, \u03b5= 0.05, t= 1).\nsake of simplicity, the equation only in the case of vortices of the same signs, otherwise we had to write\na system of two equations, with the signs of \u03b1 and \u03b2 chosen accordingly to the sign of the vorticity.\nHowever, this equation presents problems of well posedness, because the second order differential\noperator, written on the right-hand-side, takes the form\n\u2202x((\u03ba \u2212 \u03b1\u03c9)\u2202x\u03c9)\nwhich is elliptic only when \u03c9(x) \u2264 \u03ba/\u03b1 for every x. If we start with an initial condition \u03c90 satisfying\nthis inequality, one can prove that also the solution does, but this prevents the formation of clusters\nwith increasing strength, the solution will simply decay. If we take an initial condition which violates\nthis inequality, we have a problem of lack of ellipticity.\nIt is not clear yet how this problem can be remedied. One possibility is to consider the mean field\nequation without taking the limit \u03b5 \u2192 0\n\u2202t\u03c9 + \u03b1\u2202x(\u03c9\u2202x(\u03c6\u03b5 \u2217 \u03c9)) + \u2202x(\u03c9\u03b2) = \u03ba\u2207\u03c9\nwhich however looks a very ad hoc choice. Deeper perhaps is recognizing that the randomness, at the\nlevel of the particle system, plays a central role, responsible for the formation of clusters and for some\ndegree of smoothing, and reproduce these features on the mean field equation by means of a noise,\nnamely consider a stochastic PDE\nd\u03c9 + \u2202x(\u03c9\u03b2)dt = \u2202x((\u03ba \u2212 \u03b1\u03c9)\u2202x\u03c9)dt + \u03c3(\u03c9)dWt\nwhere Wt is a suitable space-dependent white noise in time. The difficulties about ellipticity remain\nbut perhaps one can choose a noise such that \u03c9(x) \u2264 \u03ba/\u03b1 is always satisfied, but thanks to the noise\nsome aggregation occurs. This research direction will not be discussed here.\nAcknowledgement\nThe research of F.F. is funded by the European Union, ERC NoisyFluid, No. 101053472.\nReferences\n[1] Guido Boffetta and Robert E Ecke. \u201cTwo-dimensional turbulence\u201d. In: Annual review of fluid\nmechanics 44.1 (2012), pp. 427\u2013451.\n[2] V. Bogomolov. \u201cDynamics of vorticity at a sphere\u201d. In: Fluid Dyn. 6 (1977), pp. 863\u2013870.\n[3] Paolo Butt` a and Carlo Marchioro. \u201cLong time evolution of concentrated Euler flows with planar\nsymmetry\u201d. In: SIAM Journal on Mathematical Analysis 50.1 (2018), pp. 735\u2013760.\n10[4] Emanuele Caglioti, Pierre-Louis Lions, Carlo Marchioro, and Mario Pulvirenti. \u201cA special class\nof stationary flows for two-dimensional Euler equations: a statistical mechanics description. Part\nII\u201d. In: Communications in Mathematical Physics 174 (1995), pp. 229\u2013260.\n[5] George F. Carnevale, Rudolf C. Kloosterziel, and Gertjan J. F. Van Heijst. \u201cPropagation of\nbarotropic vortices over topography in a rotating tank\u201d. In: Journal of Fluid Mechanics 233\n(1991), pp. 119\u2013139.\n[6] Pierre-Henri Chavanis. \u201cStatistical mechanics of two-dimensional point vortices: relaxation equa-\ntions and strong mixing limit\u201d. In: The European Physical Journal B 87 (2014), pp. 1\u201324.\n[7] Sergey Danilov and David Gurarie. \u201cScaling, spectra and zonal jets in beta-plane turbulence\u201d.\nIn: Physics of fluids 16.7 (2004), pp. 2592\u20132603.\n[8] Patrick H. Diamond, S. I. Itoh, K. Itoh, and T. S. Hahm. \u201cZonal flows in plasma\u2014a review\u201d. In:\nPlasma Physics and Controlled Fusion 47.5 (2005), R35.\n[9] GL Eyink and H Spohn. \u201cNegative-temperature states and large-scale, long-lived vortices in\ntwo-dimensional turbulence\u201d. In: Journal of statistical physics 70 (1993), pp. 833\u2013886.\n[10] Gregory L Eyink and Katepalli R Sreenivasan. \u201cOnsager and the theory of hydrodynamic tur-\nbulence\u201d. In: Reviews of modern physics 78.1 (2006), pp. 87\u2013135.\n[11] Laurent\u2013Polz F. \u201cPoint vortices on a rotating sphere\u201d. In: Regular and Chaotic Dynamics 10.1\n(2005), pp. 39\u201358.\n[12] Franco Flandoli. \u201cRenormalized Onsager functions and merging of vortex clusters\u201d. In: Stochas-\ntics and Dynamics 20.06 (2020), p. 2040010.\n[13] Pierre-Louis Lions. On Euler equations and Statistical Physics . Scuola Normale Superiore (Edi-\nzioni della Normale, 1997).\n[14] Andrew J. Majda and Andrea L. Bertozzi. Vorticity and Incompressible Flow. Cambridge Texts\nin Applied Mathematics. Cambridge University Press, 2001.\n[15] Carlo Marchioro and Mario Pulvirenti. Mathematical theory of incompressible nonviscous fluids .\nVol. 96. Springer Science & Business Media, 2012.\n[16] Carlo Marchioro and Mario Pulvirenti. \u201cOn the vortex\u2013wave system\u201d. In: (1991), pp. 79\u201395.\n[17] Klas Modin and Milo Viviani. \u201cA Casimir preserving scheme for long-time simulation of spherical\nideal hydrodynamics\u201d. In: Journal of Fluid Mechanics 884.22A (2020). doi: 10.1017/jfm.2019.\n944.\n[18] David Montgomery and Glenn Joyce. \u201cStatistical mechanics of negative temperature states\u201d. In:\n(1973).\n[19] Lars Onsager. \u201cStatistical hydrodynamics\u201d. In: Il Nuovo Cimento (1943-1954) 6.Suppl 2 (1949),\npp. 279\u2013287.\n[20] Peter B Rhines. \u201cWaves and turbulence on a beta-plane\u201d. In: Journal of Fluid Mechanics 69.3\n(1975), pp. 417\u2013443.\n[21] Ashwin R Vasavada and Adam P Showman. \u201cJovian atmospheric dynamics: An update after\nGalileo and Cassini\u201d. In: Reports on Progress in Physics 68.8 (2005), p. 1935.\n[22] Vladimir Zeitlin. \u201cSelf-Consistent Finite-Mode Approximations for the Hydrodynamics of an\nIncompressible Fluid on Nonrotating and Rotating Spheres\u201d. In: Phys. Rev. Lett. 93.26 (2004),\npp. 264\u2013501. issn: 0031-9007,1079-7114. doi: 10.1103/PhysRevLett.93.264501 . url: https:\n//doi.org/10.1103/PhysRevLett.93.264501.\n11",
      "Provably Efficient Quantum Thermal State Preparation via Local Driving\nDominik Hahn, S. A. Parameswaran, and Benedikt Placke\nRudolf Peierls Centre for Theoretical Physics,\nUniversity of Oxford, Oxford OX1 3PU, United Kingdom\nPreparing the thermal density matrix\u03c1\u03b2 \u221d e\u2212\u03b2H corresponding to a given HamiltonianH is a task\nof central interest across quantum many-body physics, and is particularly salient when attempting to\nstudy it with quantum computers. Although solved in principle by recent constructions of efficiently\nsimulable Lindblad master equations \u2013 that provably have \u03c1\u03b2 as a steady state [C.-F. Chen et al,\narXiv:2311.09207] \u2013 their implementation requires large-scale quantum computational resources and\nis hence challenging in practice on current or even near-term quantum devices. Here, we propose\na scheme for approximately preparing quantum thermal states that only requires the [repeated]\nimplementation of three readily available ingredients: (a) analog simulation of H; (b) strictly local\nbut time-dependent couplings to ancilla qubits; and (c) reset of the ancillas. We give rigorous\nperformance guarantees independent of detailed physical knowledge of H beyond its locality.\nI. INTRODUCTION\nPreparing thermal states of many-body systems is\na key goal for a wide range of quantum devices, since\nit enables the exploration of such systems through\n\u201cquantum numerical experiments\u201d. However, the\nability of analog quantum simulators to access low-\ntemperature regimes of target systems such as the\nHubbard model is often limited by the absence of\nscalable, problem-agnostic techniques for removing\nentropy in a controlled manner so as to reach the\ntarget energy density. Similarly, while a proposed\nnear-term use-case for digital quantum computers is\nto attack problems in quantum chemistry, this goal\nis often obstructed by the absence of efficient ways\nto prepare a thermal density matrix. While vari-\nous physically-motivated approaches have been pro-\nposed to meet this challenge [1\u201312], these are typi-\ncally of a heuristic and case-by-case nature. As such,\ntheir systematic error is poorly understood: in other\nwords, it is often unclear how exactly one has to scale\nthe available resources to obtain a close approxima-\ntion to a given target state.\nIn counterpoint to this are recent rigorous results\nin the quantum computer science literature [13\u201323].\nA subset of particular interest to this work con-\ncerns quantum Gibbs sampling [24\u201329]: the prob-\nlem of engineering a dissipative quantum dynam-\nics whose steady state corresponds to a specified\nGibbs density matrix. Although a formal solution\nhas long been available in terms of so-called Davies\ngenerators [30\u201332], the corresponding dynamics in-\nvolves a purely dissipative Lindblad evolution under\nhighly nonlocal and hence unphysical \u2018jump opera-\ntors\u2019, making it unfeasible in practice. The recent\nwork upends this conventional wisdom by demon-\nstrating that exact Gibbs sampling is possible with\ncontrollable accuracy (in the sense above) while in-\nducing dissipation using only quasi-local jump op-\nerators, but at the cost of introducing a specially\ntailored coherent evolution. The steady state of the\nresulting Lindblad evolution is exactly given by \u03c1\u03b2,\nwhich follows from the fact that the Lindbladian\nsatisfies a certain [quantum] detailed balance prop-\nerty [33\u201336]. The sole remaining unknown is the\nmixing time \u03c4mix of the Lindbladian, which controls\nthe approach to the steady state and can be long for\nphysically meaningful reasons much as in the clas-\nsical case. Thus, although Gibbs samplers are un-\nlikely to speed up problems that are classically hard\nbecause of glassy landscapes [37\u201343] (such as generic\noptimization problems), they are not limited by the\nsign problem or entanglement growth which are the\nusual \u201cintrinsically quantum\u201d obstacles to simulat-\ning many-body physics.\nDespite this remarkable breakthrough, convenient\nprotocols to implement these exact Gibbs samplers\n(or their close approximations) in physically realis-\ntic settings relevant to current and near-term quan-\ntum hardware remain largely unexplored (but see\ne.g. Refs. 44 and 45 for concurrent work). Pro-\nposed approaches rely on block-encoding Lindbladi-\nans [27, 29]; doing so at appreciable scale is likely out\nof the question for the near term. It is this gap be-\ntween formal theory and practical implementation\nthat we bridge here, while bolstering a physically\nmotivated picture with mathematical rigor.\nSpecifically, we devise a protocol which approx-\nimately prepares thermal states of an arbitrary\nHamiltonian, with controlled and arbitrarily small\nerror \u03f5, using a total Hamiltonian simulation time\nscaling as \u02dcO(\u03b2\u03c4 3\nmix\u03f5\u22122). Crucially, we only require\nthree relatively standard features of the current gen-\neration of quantum devices: (a) \u201canalog\u201d simula-\narXiv:2505.22816v1  [quant-ph]  28 May 2025i j\nsystem\nbath\noptional\nFIG. 1. Schematic of one time step of the local driving\nsampler. In each step, the \u201cbath\u201d qubits are initialized\nin the all-zero state, followed by an entangling step con-\nsisting of time evolution with the system Hamiltonian\nH together with a dynamically driven system-bath cou-\npling V (t) = P\ni Vi(t) [cf. Eq. (16)]. Finally, the bath\nqubits are measured and the outcomes discarded. The\nfinal unitary \u201crewinding\u201d eiHT can be dropped with only\na modest increase to the error bound, and can be ignored\nwith no penalty if a quasiparticle picture applies.\ntion of the nS-qubit Hamiltonian; (b) a dynami-\ncally tunable coupling to an (arbitrary) number of\nnB = O(nS) ancilla qubits; and (c) the ability to\nreset the ancillas (see Fig. 1).\nThe proposed protocol implements a controlled\ndiscrete-time approximation of a certain Lindblad\nevolution. The corresponding Lindbladian has ex-\nactly the same dissipative part as the exact sampler,\nbut the \u201cwrong\u201d coherent part: a Lamb shift correc-\ntion rather than the coherent dynamics necessary\nfor exact thermal state preparation. However, us-\ning the so-called secular approximation [5, 46, 47],\nwe show that this difference in coherent evolution\nonly changes the fixed point of the Lindbladian up\nto an error parametrically small in the Hamiltonian\nsimulation time between resets.\nAficionados of near-term quantum algorithms will\nrecognize a resemblance between our proposal and\nthe \u201cquasiparticle cooling\u201d approach of Refs. 9 and\n10. Formally, the difference between the two pro-\ntocols is that ours implements a certain additional\nunitary \u201crewinding\u201d procedure. However, this step\nis unnecessary if one assumes the validity of a quasi-\nparticle picture. We note that our motivation here\nis also distinct: we aim to engineer a specific time\nevolution of the system-bath coupling, such that the\ndissipative portion of our Lindbladian coincides with\nthat of an exact Gibbs sampler. This then admits an\nanalysis that does not rely on a quasi-particle pic-\nture, allowing us to arrive at rigorous error bounds\nat arbitrary temperature and for arbitrary Hamil-\ntonians; inter alia , our results also provide a rigor-\nous (but less tight) error bound for the quasiparticle\ncooling protocol. Nevertheless, the link to Refs. 9\nand 10 simultaneously highlights the feasibility of\nour protocol, and suggests that their approach to\nthermal state preparation, with minor modifactions,\nis likely more versatile than previously thought.\nThe remainder of this paper is organized as fol-\nlows. In Sec. II, we provide some relevant back-\nground on the history of quantum thermal state\npreparation, with an emphasis on recent work on\nexact Gibbs samplers. (The latter may also serve\nas a useful physicists\u2019 guide to recent results in the\nquantum computer science literature.) We then in-\ntroduce our local driving protocol in Sec. III, and\nboth state and motivate bounds on its efficiency, in\nterms of the scaling of resources required to achieve\na specified proximity (in 1-norm distance) to the tar-\nget density matrix. We then proceed to give a rigor-\nous proof of these bounds in Sec. IV, which contains\nthe bulk of the technical material. Readers primarily\ninterested in physical intuition can skip directly to\nSec. IV C that gives the resource estimate implied by\nthese bounds. Sec. V simulates the protocol numer-\nically on small systems, demonstrating agreement\nwith various aspects of the error bounds. We close\nwith a discussion in Sec. VI, where we also outline\npromising directions for future study.\nII. STATE PREPARATION, DETAILED\nBALANCE, AND EXACT GIBBS SAMPLING\nIn order to introduce concepts and place our work\nin context, we first give some relevant background\nthermal state preparation. After a brief historical\norientation, our main emphasis is on recent construc-\ntions of Lindblad operators whose steady state cor-\nresponds to a specified density matrix \u03c1, by virtue of\ntheir satisfying a so-called Kubo\u2013Martin\u2013Schwinger\n(KMS) detailed balance condition with respect to \u03c1.\nA. Historical Background\nAs noted in the introduction, the use of quan-\ntum computers to study many body systems in ther-\nmal equilibrium usually requires the preparation of\na thermal (Gibbs) density matrix \u03c1\u03b2 = Z\u22121e\u2212\u03b2H\n(where Z \u2261 tr e\u2212\u03b2H ) of a given Hamiltonian H. A\nconceptually significant effort to meet this challenge\n2was made by Terhal and DiVincenzo [2], who pro-\nposed adapting the physical picture of equilibration\nunder system-bath dynamics to an algorithmic, com-\nputational setting. They argued that repeated cycles\nof initializing a bath of ancilla qubits, evolving its in-\nteraction with the system for finite time (e.g. using\nHamiltonian simulation) and performing a reset of\nthe ancillas, asymptotically prepare a thermal state\nof the system. However, this result is exact only in\nthe limit of infinite bath size and vanishing system\nbath coupling familiar in the derivation of master\nequations. Thus, their analysis does not address the\nfeasibility of the protocol in more practical settings,\nwhere making general statements in the absence of\nany foreknowledge of H is challenging.\nRef. 15 instead proposed an algorithm of a\nvery different spirit, by lifting the technique of\nimportance-sampling configurations from the clas-\nsical to the quantum setting. Such \u201cquan-\ntum Metropolis sampling\u201d involves a Metropolis-\nweighted random walk on eigenstates, using quan-\ntum phase estimation to determine energies. It has\nrecently been claimed that this algorithm is not\nprovably efficient due to the finite energy resolution:\nthe error in estimating the energy is argued to propa-\ngate to the error (e.g. in norm distance) between the\nactual and desired fixed points of the sampling algo-\nrithm, viewed as a quantum channel on the Hilbert\nspace. (For a recent claimed resolution via weak\nmeasurements, see Ref. 23.)\nA similar issue also applies to system-bath mod-\nels. One way of ensuring that the Lindbladian gov-\nerning the evolution of the system has \u03c1\u03b2 as a fixed\npoint is to demand that it satisfy quantum detailed\nbalance1; while we will make this notion precise be-\nlow, a lucid discussion oriented towards physicists\nmay be found in Ref. 32. A formal solution satisfy-\ning detailed balance, the so-called \u201cDavies\u2019 Lindbla-\ndian\u201d, requires non-local jump operators that distin-\nguish individual eigenstates, so that its construction\nagain requires exponentially fine energy resolution\n(or equivalently, exponentially long time evolution).\nQuantum Metropolis sampling relies on access to the\neigenbasis to ensure detailed balance. Thus, both\napproaches fail due to the well-known [48] unphysi-\ncality of eigenstates in many-body systems.\nAlternative approaches have employed classical\ndetailed balance together with the eigenstate ther-\nmalization hypothesis to construct Gibbs sam-\nplers [7, 45]. However, this assumption may limit\n1 As in the classical case, this is sufficient but not necessary.\ntheir applicability in low-temperature regimes or in\nsystems that fail to act as their own bath.\nA recent burst of activity has provided an ele-\ngant exact solution to the longstanding challenge of\npreparing thermal states. First, Ref. 26 observed\nthat for any geometrically local Hamiltonian H, the\nDavies\u2019 jump operators can be replaced by \u2018filtered\u2019\nanalogues that are quasilocal (in essence, generated\nby finite time evolution of local operators) yet lead-\ning to a Lindbladian whose fixed point is only poly-\nnomially far from the thermal state \u03c1\u03b2. Shortly\nthereafter, a subset of those authors showed [27] that\nadding a suitably tailored coherent evolution leads\nto a Lindbladian with \u03c1\u03b2 as an exact fixed point.\nThe central insight in both works is that one should\nimpose a suitable form of \u201cKubo-Martin-Schwinger\u201d\n(KMS) detailed balance condition on the Lindbla-\ndian, and adjust the filter function and correspond-\ning coherent evolution to ensure that this is satisfied.\nThis construction was then generalized [28, 29], lead-\ning to the identification of the most general family of\nLindbladians satisfying the KMS condition. We turn\nnext to a summary of these recent results, which will\nserve as essential background to our work.\nB. Exact Gibbs Samplers\nSince KMS detailed balance and exact Gibbs sam-\npling play a central role in our work, we give a brief\noverview of these topics to establish notation and\nto make this paper self-contained. Since we take a\nsomewhat abstract perspective when introducing the\nnecessary formalism, readers may wish to initially\nskip or just skim this section, and return to it after\nthe more concrete derivations of Sec. III, but before\nstudying the derivation of error bounds in Sec. IV.\nMuch of what follows, including our notation, is\nadapted from Refs. 27 and 29, to which the reader\nis referred for details. While we attempt to give as\nmuch detail as feasible, more laborious computations\nare relegated to App. A.\n1. Kubo-Martin-Schwinger Detailed Balance\nThe Lindbladian superoperator implements time\nevolution of states (density matrices) in the\nSchr\u00a8 odinger picture, according to [49]\nd\u03c1\ndt = L[\u03c1], (1)\nso that formally we have \u03c1(t) = etL[\u03c1(0)]. For L\nto represent a sensible time evolution, we require\n3that etL is a completely positive trace-preserving\n(CPTP) map on density matrices. Trace preserva-\ntion tr etL[\u03c1] = tr \u03c1 in turn requires that tr L[\u03c1] = 0.\nThe corresponding Heisenberg evolution of opera-\ntors is implemented by the adjoint Lindbladian L\u2020,\ntaken with respect to the Frobenius inner product\n\u27e8A, B\u27e9 \u2261tr[A\u2020B]: for any state \u03c1 and operator O,\nwe have tr[L[\u03c1]O] = tr\n\u0002\n\u03c1L\u2020[O]\n\u0003\n. Requiring that the\nSchr\u00a8 odinger evolution preserves the trace is equiv-\nalent to requiring that the Heisenberg evolution be\nunital, i.e. preserves the identity, etL\u2020\n[1] = 1, which\nin turn requires L\u2020[1] = 0.\nGiven any full-rank density matrix \u03c1, we also de-\nfine a self-adjoint \u201cweighting\u201d superoperator \u0393 \u03c1:\n\u0393\u03c1[\u00b7 ] := \u03c11/2(\u00b7 )\u03c11/2 = \u0393\u2020\n\u03c1[\u00b7 ]. (2)\nA Lindbladian L satisifies KMS detailed balance\nwith respect to a full-rank density matrix \u03c1 if\nL\u2020 = \u0393\u22121\n\u03c1 \u25e6 L \u25e6\u0393\u03c1. (3)\nAn immediate corollary of KMS detailed balance is\nthat \u03c1 is a fixed point of L. To see this, observe that\nEq. (3) is equivalent to the condition L = \u0393\u03c1 \u25e6 L\u2020 \u25e6\n\u0393\u22121\n\u03c1 , so that we have\nL[\u03c1] = \u03c11/2L\u2020[\u03c1\u22121/2(\u03c1)\u03c1\u22121/2]\u03c11/2 = 0, (4)\nwhere the second equality follows from L\u2020[1] = 0.\nIn its stated form [Eq. (3)], the KMS condition does\nnot immediately resemble the classical notion of de-\ntailed balance, which is often formulated in terms\nof the \u201creversibility\u201d of the transition probabilities.\nThe connection can be sharpened by defining the\nKMS inner product with respect to \u03c1:\n\u27e8A, B\u27e9\u03c1 \u2261 tr\u221a\u03c1A\u221a\u03c1B. (5)\nThe KMS detailed balance condition [Eq. (3)] is\nthen the statement that L\u2020 is self-adjoint with re-\nspect to the inner product in Eq. (5). Equivalently,\nwe may also say that L \u25e6\u0393\u03c1 = \u0393\u03c1 \u25e6 L\u2020 is self-adjoint\nwith respect to the Frobenius inner product. Ei-\nther perspective brings the KMS condition in con-\nsonance with classical detailed balance, which can\nbe defined as the transition matrix being symmet-\nric when weighted appropriately by the Boltzmann\nweights.\n2. Structure of KMS-Detailed Balanced Lindbladians\nTo understand the structure imposed on L by\nKMS detailed balance, it is convenient to pass to a\nrepresentation in terms of the Bohr frequencies (i.e.\nenergy differences) of the Hamiltonian H that speci-\nfies the target thermal density matrix \u03c1\u03b2; we denote\nthe set of such frequencies \u03bd \u2208 B. We may then\nwrite A(t) \u2261 eiHtAe\u2212iHt = P\n\u03bd\u2208B A\u03bdei\u03bdt, where\nA\u03bd \u2261\nX\n\u03c91,\u03c92\u2208Spec(H)\n\u03c91\u2212\u03c92=\u03bd\n|\u03c91\u27e9\u27e8\u03c91|A |\u03c92\u27e9\u27e8\u03c92|, (6)\nso that (A\u2212\u03bd)\u2020 = (A\u2020)\u03bd.\nWe will first write L in terms of the Bohr fre-\nquency representation of some set of jump operators\nAa \u2208 A. In this representation, a generic Lindbla-\ndian L = G + T + R can be decomposed into \u201ctran-\nsition\u201d, \u201cdecay\u201d, and \u201ccoherent parts\u201d given by\nT [\u00b7 ] =\nX\na\u2208A\nX\n\u03bd1,\u03bd2\u2208B\n\u03b1\u03bd1,\u03bd2 Aa\n\u03bd1 (\u00b7 )(Aa\n\u03bd2 )\u2020, (7a)\nR[\u00b7 ] = \u22121\n2\nX\na\u2208A\nX\n\u03bd1,\u03bd2\n\u03b1\u03bd1,\u03bd2\n\b\n(Aa\n\u03bd2 )\u2020Aa\n\u03bd1 , \u00b7\n\t\n, (7b)\nG[\u00b7 ] =\nX\na\u2208A\nX\n\u03bd1,\u03bd2\ng\u03bd1,\u03bd2\n\u0002\n(Aa\n\u03bd2 )\u2020Aa\n\u03bd1 , \u00b7\n\u0003\n, (7c)\nwhere P\na\u2208A . . .\u2261 P|A|\na=1, and we require that\n\u03b1\u03bd1,\u03bd2 = (\u03b1\u03bd2,\u03bd1 )\u2217 and g\u03bd1,\u03bd2 = (g\u03bd2,\u03bd1 )\u2217 and \u03b1\u03bd1,\u03bd2 \u2265\n0 for L to be CPTP. The potentially counterintuitive\nordering of the frequency labels in T relative to R\nand G is required by the Lindbladian structure.\nNote that while the kernel \u03b1\u03bd1,\u03bd2 of T is fixed to\nthat of R by trace preservation, that of the coherent\npart g\u03bd1,\u03bd2 is arbitrary. However, as we will see, this\nis linked to \u03b1\u03bd1,\u03bd2 by detailed balance.\nWe now impose KMS detailed balance [Eq. (3)]\nwith respect to a thermal density matrix \u03c1\u03b2 =\n1\nZ e\u2212\u03b2H and ask how this constrains L.\nFirst, we consider the transition part. From a\nstraightforward calculation (App. A), we have\n(\u0393\u22121\n\u03c1\u03b2 \u25e6 T \u25e6\u0393\u03c1\u03b2 )[\u00b7 ] =\nX\na\u2208A\nX\n\u03bd1,\u03bd2\u2208B\n\u03b1\u03bd1,\u03bd2 e\n\u03b2(\u03bd1+\u03bd2)\n2\n\u00d7 Aa\n\u03bd1 (\u00b7 )(Aa\n\u03bd2 )\u2020, (8)\nMeanwhile, under the assumption that the set of\njump operators A is closed under Hermitian conju-\ngation (i.e. Aa \u2208 Aiff A\u2020\na \u2208 A), a slightly more\ninvolved computation (detailed in App. A) yields\nT \u2020[\u00b7 ] =\nX\na\u2208A\nX\n\u03bd1,\u03bd2\u2208B\n\u03b1\u2212\u03bd2,\u2212\u03bd1 Aa\n\u03bd1 (\u00b7 )(Aa\n\u03bd2 )\u2020. (9)\nComparing Eq. (8) and Eq. (9), we see that T is\nKMS detailed balanced, i.e. T \u2020 = \u0393\u22121\n\u03c1 \u25e6 T \u25e6\u0393\u03c1, if\n\u03b1\u2212\u03bd2,\u2212\u03bd1 = \u03b1\u03bd1,\u03bd2 e\n\u03b2(\u03bd1+\u03bd2)\n2 . (10)\n4While T satisfies detailed balance on its own, for\ngeneric choices of \u03b1\u03bd1,\u03bd2 satisfying Eq. (10) the\ndecay and coherent parts R and G do not satisfy\ndetailed balance individually, but instead are inter-\ntwined by the action of \u0393 \u03c1. However, as first noted\nby Ref. 27 (and here summarized in App. A), R\nand G together satisfy KMS detailed balance, i.e.\n(G + R)\u2020 = \u0393\u22121\n\u03c1 \u25e6 (G + R) \u25e6 \u0393\u03c1, if we choose\ng\u03bd1,\u03bd2 = tanh \u03b2(\u03bd1 \u2212 \u03bd2)\n4 \u03b1\u03bd1,\u03bd2 . (11)\nAny set of coefficients\u03b1\u03bd1,\u03bd2 and g\u03bd1,\u03bd2 that satisfy\nthe conditions in Eq. (10) and Eq. (11) generates a\nLindbladian via Eq. (7) that exactly satisfies KMS\ndetailed balance, and hence has the thermal state \u03c1\u03b2\nas a fixed point. Two choices are especially salient.\nThe first is to take\n\u03b1\u03bd1,\u03bd2 = \u03b4\u03bd1,\u03bd2 \u03b3(\u03bd1) with \u03b3(\u2212\u03bd) = \u03b3(\u03bd)e\u03b2\u03bd ,\n(12)\nin which case G = 0. This corresponds to the well-\nknown Davies Lindbladian. The drawback, as noted\npreviously, is that generating such Lindblad dynam-\nics by evolving local jump operators under a local\nH requires a time exponentially long in system size\n(since in order to resolve the smallest Bohr frequen-\ncies, we need to evolve until the Heisenberg time).\nA different choice, and our main focus, is [29]\n\u03b1\u03bd1,\u03bd2 = \u02c6f(\u2212\u03bd1)[ \u02c6f(\u2212\u03bd2)]\u2217 (13)\nwith\n\u02c6f(\u03bd) = e+\u03b2\u03bd/4q(\u03bd), and q(\u2212\u03bd) = q\u2217(\u03bd). (14a)\nUsing this form of filter function, the transition and\ndecay parts in Eq. (7) can be brought into the form\nL\u03b2[\u00b7 ] = \u2212i[G, \u00b7 ] +\nX\na\u2208A\nLa(\u00b7 )L\u2020\na \u2212 1\n2\n\b\nL\u2020\naLa, \u00b7\n\t\n,\n(14b)\nwhere\nG = i\n2\nX\na\u2208A\nX\n\u03bd\u2208B\ntanh\n\u0012\u03b2\u03bd\n4\n\u0013\n(L\u2020\naLa)\u03bd (14c)\nand\nLa =\nX\n\u03bd\u2208B\n\u02c6f(\u2212\u03bd)(Aa)\u03bd =\nZ \u221e\n\u2212\u221e\nf(t)Aa(t)dt, (14d)\nwith f(t) = 1\n2\u03c0\nR\u221e\n\u2212\u221e\n\u02c6f(\u03bd)ei\u03bdtd\u03bd [as can be seen using\nEq. (6)]. (We have added a subscript \u03b2 to emphasize\nthat Eq. (14b) is detailed balanced with respect to\n\u03c1\u03b2.) For a suitable choice of q(\u03bd) (e.g., a Gaussian),\nthe time-domain filter function decays quickly, so we\ncan view Eq. (14d) as a \u201csmoothing\u201d of the time-\nevolved jump operator. As such, for local H, the\nLa obtained in this way will be quasi-local due to\nLieb-Robinson bounds.\nThe KMS condition [Eq. (3)] with respect to \u03c1\u03b2\nis then conveniently stated as\n\u03c1\u22121/2\n\u03b2 La\u03c11/2\n\u03b2 = L\u2020\na. (15)\nRef. 29 shows that Lindbladians that satisfy KMS\ndetailed balance (i.e., exact Gibbs samplers) can al-\nways be written in the form of Eq. (14).\nIII. THE LOCAL DRIVING SAMPLER\nThe form of the exact Gibbs sampler, where quasi-\nlocal jump operators are generated by time evolution\nof strictly local operators, suggests that one might\nbe able to generate such a Lindbladian by leveraging\nthe operator spreading that naturally occurs under\nHamiltonian dynamics. Combining this observation\nwith known results on Lindbladian simulation via\nHamiltonian simulation, we show below that we can\nimplement, efficiently and with controlled error, a\nLindbladian that reproduces an exact Gibbs sampler\nup to the coherent part . While the latter is impor-\ntant to exactly satisfy KMS detailed balance, this is\ninessential if we require only that the thermal state is\nan approximate steady state [26, 47]. We show below\nin Sec. IV that the Lindbladian that we implement\nis indeed such an approximate Gibbs sampler with\ncontrolled fixed-point error. (To sequester the tech-\nnical portion of the work to a single section, here\nwe only discuss the high-level picture and relegate\ndetailed derivations to where we prove error bounds\nin Sec. IV and Appendices B and C.)\nOur setup, sketched in Fig. 1, consists of a sys-\ntem with Hamiltonian H acting on nS system qubits,\ncoupled to a bath of nB ancilla qubits, one per jump\noperator Aa, a = 1 , 2 . . .|A| = nB. The Hamilto-\nnian for the coupled system is:\nHSB = IB \u2297 H + HB \u2297 Is\n+\nX\na\nJ{f(t)B\u2020\naAa + f\u2217(t)BaA\u2020\na} (16)\nwhere f(t) is taken to coincide with the filter func-\ntion appearing in the exact sampler [cf. Eq. (14a)\nand below Eq. (14d)] and B = 1\n2 (XB \u2212 iYB),\nB\u2020 = 1\n2 (XB + iYB) are lowering and raising oper-\nators for the ancilla, which satisfy B2 = (B\u2020)2 = 0\n5and B\u2020 |0\u27e9 = |1\u27e9, B|0\u27e9 = 0. We will often be in-\nterested in the case where the bath Hamiltonian is\ntrivial, but for now allow there to be some nontriv-\nial bath dynamics governed by HB; note however\nthat we will only consider the case where the bath is\nnoninteracting, i.e. the bath Hamiltonian HB does\nnot involve coupling between the ancillas. (The case\nwhere the bath interactions are purely diagonal can\nbe shown to be equivalent to this, but may prove\nuseful e.g. in engineering appropriate f(t)\u2019s.)\nWe now move to the interaction picture with re-\nspect to H + HB: the corresponding interaction-\npicture Hamiltonian is then denoted by a tilde, and\nconsists purely of a system-bath interaction\n\u02dcHSB(t) = J\nX\na\n\b\nf(t)B\u2020\na(t) \u2297 Aa(t)\n+ f\u2217(t)Ba(t) \u2297 A\u2020\na(t)\n\t\n(17)\nwith OB(t) \u2261 eiHBtOBe\u2212iHBt for O = X, Y, and\nA(t) \u2261 eiHtAe\u2212iHt as above.\nWe assume that the f(t) remains operational over\nsome finite time interval [ \u2212T/2, T/2], chosen to\nbe symmetric for convenience. Returning to the\nSchr\u00a8 odinger picture, we see that this implements the\nunitary e\u2212iHT \u02dcV . We complete the cycle by \u2018reset-\nting\u2019 the bath to \u03c10\nB = |0\u27e9\u27e80|B, while implement-\ning a \u201crewinding step\u201d only on the system: a back-\nwards time evolution with H for a period T. In\nother words, a single cycle of the time evolution im-\nplements the following channel on the Schr\u00a8 odinger\npicture density matrix \u03c1:\nK[\u03c1] = trB\nh\n\u02dcV\n\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001 \u02dcV \u2020\ni\n. (18)\nWe emphasize that here,\n\u02dcV \u2261 Tt exp\n \n\u2212i\nZ T/2\n\u2212T/2\ndt \u02dcHSB(t)\n!\n(19)\nis the interaction-picture time evolution operator for\nthe composite system comprising the target system\nand the bath of ancillas, even though Eq. (18) de-\nscribes the evolution of the Schr\u00a8 odingerdensity ma-\ntrix. This explains the point of the \u201crewinding\u201d step:\nto allow a repeated application of the channel cor-\nresponding to evolution under \u02dcV followed by a bath\nreset, which is not equivalent to simply evolving un-\nder H + V (t) for time T followed by a reset.\nThe idea is that such a channel implements a Li-\nouvillian time evolution over an controllably small\ninterval \u2206\u03c4 = J2 up to an error O(J4). To see this,\nexpand \u02dcV in powers of J up to third order via the\nMagnus expansion: we may write\n\u02dcV = e\u2212i P3\nn=1\n\u02dcH(n)\nM + O(nBJ4) (20)\nwhere the H(n)\nM represents the nth-order term in the\nMagnus expansion, see Sec. IV A for details.\nWe now insert the final expression Eq. (20) for \u02dcV\ninto Eq. (18) and expand the exponential. Owing to\nthe assumption that the ancilla qubit is reset after\neach cycle, we can restrict our attention to terms in\nthe expansion that involve H(m)\nF \u00b7 H(n)\nF with m + n\neven, which enter atO(Jm+n). Accordingly, we have\nK[\u03c1] = \u03c1 + trB\nh\n\u02dcH(1)\nM\n\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001 \u02dcH(1)\nM\ni\n\u2212 1\n2 trB\n\u0014\u001a\u0010\n\u02dcH(1)\nM\n\u00112\n,\n\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001\u001b\u0015\n\u2212 i trB[ \u02dcH(2)\nM , \u03c10\nB \u2297 \u03c1] + O(nBJ4). (21)\nA straightforward but tedious calculation, pre-\nsented in App. B, now yields the fact that\nK[\u03c1] = eJ2LT [\u03c1] + O(nBJ4) (22)\nwhere the effective Lindbladian LT can be written\nin a very appealing form: we have\nLT [\u00b7 ] = \u2212i[HLS, \u00b7 ] + Ldiss;T [\u00b7 ], (23a)\nwhere we have separated the f-smoothed Lindblad\ndynamics into a purely dissipative evolution\nLdiss;T [\u00b7 ] =\nX\na\nLa;T \u00b7 L\u2020\na;T \u2212 1\n2{L\u2020\na;T La;T , \u00b7 } (23b)\nwith the jump operator\nLa;T \u2261\nZ T/2\n\u2212T/2\nf(t)Aa(t)dt, (23c)\nand a coherent Lamb shift contribution governed by\nthe Hamiltonian\nHLS;T = \u2212 1\n2i\nZ T/2\n\u2212T/2\ndt1\nZ T/2\n\u2212T/2\ndt2 f\u2217(t2)f(t1)\n\u00d7 sgn(t1 \u2212 t2)\nX\na\nA\u2020\na(t2)Aa(t1) (23d)\n=\nX\na\u2208A\nX\n\u03bd1,\u03bd2\nh(T)\n\u03bd1,\u03bd2 (Aa\n\u03bd2 )\u2020Aa\n\u03bd1 (23e)\nwhere\nh(T)\n\u03bd1,\u03bd2 = \u2212 1\n2i\nZ T/2\n\u2212T/2\ndt1\nZ T/2\n\u2212T/2\ndt2 f(t1)f\u2217(t2)\n\u00d7 sgn(t1 \u2212 t2)ei\u03bd1t1\u2212i\u03bd2t2 (23f)\n6Note that the transition, decay, and coherent (Lamb\nshift) contributions each respectively arise from the\nthree different traces over the bath in Eq. (21).\nObserve that LT has almost precisely the form\nof the exact Gibbs sampler in Eq. (14), up to a\nfinite-time truncation of the f-filtered Heisenberg\nevolution, but with a distinct coherent term: instead\nof the form G required by the exact sampler [Eq.\n(14c)], the coherent evolution generated by the local\ndrive is a \u201cphysical\u201d Lamb shift [47].\nThis second distinction between the exact and lo-\ncal driving samplers is most easily seen by taking\nT \u2192 \u221ein the latter; the resulting Lindbladian has\ntransition and decay parts that take the the form\nof Eq. (7a) and Eq. (7b) with \u03b1\u03bd1,\u03bd2 given by Eq.\n(13), but with a different coherent part; instead of a\nkernel g\u03bd1,\u03bd2 in Eq. (7c) that satisfies the condition\nEq. (11) for exact detailed balance, we instead have\ng\u03bd1,\u03bd2 \u2192 h\u03bd1,\u03bd2 \u2261 lim\nT\u2192\u221e\nh(T)\n\u03bd1,\u03bd2 . (24)\nWe have so far made no specific choice of filter\nfunction f(t). Henceforth, we make the convenient\nand specific choice of a shifted Gaussian [29],\nf(t) =\nr\n2\n\u03c0\u03c32 e\u2212 2\n\u03c32 (t\u2212i\u03b2\n4 )\n2\n, (25)\ncorresponding to taking q(\u03bd) = e\u2212(\u03c3\u03bd)2\n8 in Eq. (14a).\nThis fixes both the exact KMS-detailed balanced\nand the local-driving Lindbladians that we consider\nin the remainder of this paper.\nIn the next section, we bound the error in approx-\nimating the former by the latter. We will show that\nalso LT , and hence K have an approximately ther-\nmal steady state.\nIV. BOUNDS ON THE ACCURACY\nIn this section, we summarize the different er-\nror sources and their contributions to the final state\nwhen using the setup described in Sec. III to prepare\nan approximate thermal state of a Hamiltonian H.\nWe use these results to derive an upper bound on the\nHamiltonian simulation time necessary to prepare\nthe thermal \u03c1\u03b2 up to some error \u03f5 in trace norm.\nWe summarize the different approximations made\nin Fig. 2. The total error of implementing the lo-\ncal driving protocol as opposed to the exact sam-\npler in Eq. (14) stems, roughly speaking, from two\nsources. First, the physical protocol implements an\nfinite time evolution with a Lindbladian LT , up to\nan error which is controlled by the truncation error\nof the third-order Magnus expansion in the interac-\ntion picture [50]. As explained below, this in turn\nis governed by the time step \u2206 \u03c4 in the discretiza-\ntion of the target Lindblad evolution, which in ef-\nfect scales as the second power of the system-bath\ncoupling J. The second source of error is the fact\nthat even if we could perfectly realize the Lindbla-\ndian LT , it has a steady state \u03c1SS (i.e. LT [\u03c1SS] = 0)\nwhich is only approximately equal to the thermal\nstate, i.e. \u03c1SS \u2248 \u03c1\u03b2. This fixed point error is ulti-\nmately controlled by the parameter T of LT and the\nwidth \u03c3 of the filter function. Note that these enter\nthe error bound in two ways. Clearly, T affects the\nextent to which the finite- T smoothing of jump op-\nerators in Eq. (23c) approximates that of the exact\nsampler in Eq. (14d). Less obviously, \u03c3, Ttogether\ngovern the modification of the fixed point due to the\n\u201cwrong\u201d coherent evolution, via their influence on\nthe \u201csecular approximation\u201d discussed below.\nA. Lindbladian Simulation Error\nAs we advertised, our protocol implements a Lind-\nbladian LT approximately by performing a time-\ndependent Hamiltonian simulation followed by a re-\nset; the key is that for a suitable choice of f(t), the\ndrive approximates the jump operators that enter\nthe exact sampler. This approximation stems from\nreplacing the time-ordered integral \u02dcV in Eq. (19)\nby its third-order Magnus expansion (cf. Sec. III).\nTherefore, to bound this error we can leverage recent\nresults on Hamiltonian simulation in the interaction\npicture [50] which show that\n\r\r\r\r\r\n\u02dcV \u2212 exp\n\"\n\u2212i\n 3X\nk=1\n\u02dcH(k)\nSB\n!#\r\r\r\r\r\n1\n= O\n\u0010\nnB (dJT )4\n\u0011\n(26)\nwhere \u02dcH(k)\nSB is the kth-order term in the Magnus ex-\npansion of \u02dcHSB, and d = maxt |f(t)|. Note that for\nthe choice of filter function f [Eq. (25)] considered\nin this work dT = O(1) in Eq. (26). We use this, in\nApp. B, to show that for all \u03c1,\n\r\r\reJ2LT [\u03c1] \u2212 K[\u03c1]\n\r\r\r\n1\n= O\n\u0000\nnBJ4\u0001\n(27)\nwhere LT is given exactly by Eq. (23).\nIn summary, our protocol implements a Lindbla-\ndian time step of size \u2206 \u03c4 = J2 up to an error\nO((\u2206\u03c4)2), which allows one in principle to imple-\nment evolution with controlled error for arbitrary\nlong times.\n7Fixed point Lindbladian/ Channel Parameter choices\nDifferent coherent \npart\nFinite time \nevolution\nImplementation \nby local driving\n+ reset\n1\n1\n1\nFIG. 2. Summary of different approximations made during the derivation of the local driving sampler protocol.\nOverall, the setup in Fig. 1, with the parameter choices indicated in the rightmost column above, implements a\nchannel K that approximates an exact Gibbs sampler and hence has an approximately thermal fixed point.\nB. Fixed-point Error\nAccepting the fact that our protocol implements\nLT up to controllable error, it remains to be shown\nthat LT is indeed an approximate Gibbs sampler,\ni.e. that the fixed point of the dynamics generated\nby LT is close to the thermal state. This \u2018fixed-\npoint error\u2019 again has two distinct sources, that is\n(i) the fact that we implement the dissipative part\nof the exact Gibbs sampler only up to some finite\nevolution time T, and (ii) the fact that LT even for\nT \u2192 \u221eimplements the \u201cwrong\u201d coherent part of\nthe evolution. In turns out that both of these lead\nto bounded errors.\nFor a choice of filter function f(t) that decays\nquickly on some finite time scale \u03c3, restricting LT\nto finite T incurs only a minor error in the Lind-\nbladian evolution in the sense that L\u221e and LT are\nclose in superoperator norm for T \u226b \u03c3. This then\nbounds the difference between their respective fixed\npoints,\n\u2225\u03c1T \u2212 \u03c1\u221e\u22251 \u2264 4 \u03c4mix(LT ) \u2225LT \u2212 L\u221e\u22251\u22121\n\u2264 24\n\u03c0 nB\n\u0010\u03c3\nT\n\u0011\ne\u2212(T\n\u03c3 )\n2\n\u03c4mix(LT ) (28)\nwhere the first inequality is Lemma II.1 of Ref. 26,\nand the second, derived in App. C 1, follows for the\nspecific choice of a Gaussian filter function with stan-\ndard deviation \u03c3. Here and below, we define the\nmixing time of a Lindbladian as the smallest time\n\u03c4mix for which, given any \u03c11, \u03c12, we have\n\r\re\u03c4mixL[\u03c11 \u2212 \u03c12]\n\r\r\n1 \u2264 1\n2\u2225\u03c11 \u2212 \u03c12\u22251. (29)\nGiven the error bound in Eq. (28), what is left\nto show is that the fixed point \u03c1\u221e of L\u221e is close to\nthe thermal state. While this is technically the most\ninvolved step of the whole derivation it follows from\na clear intuition. Compared to the exact sampler\nin Eq. (14), the non-truncated local driving sampler\nL\u221e has an \u2018added\u2019 coherent partB = HLS\u2212G. Now,\nwriting B in terms of energy transfers corresponding\nto the Bohr frequencies of H,\nB =\nX\n\u03bd1,\u03bd1\u2208B\nb\u03bd1,\u03bd2 (A\u03bd2 )\u2020A\u03bd1 (30)\nwe can show that if the matrix b\u03bd1,\u03bd2 is fast decaying\nfor |\u03bd1 \u2212 \u03bd2| \u226b\u00b5 and some \u00b5 >0, then the operator\nB almost commutes with \u03c1\u03b2. Therefore, adding it to\nL\u03b2 as a coherent evolution should leave fixed point\nalmost unchanged. The challenge is to quantify the\n\u201calmost\u201d, which technically requires employing the\nso-called \u201csecular approximation\u201d [5, 26, 46, 47]. In\nour case, we show in App. C 2, that\n\u2225\u03c1\u221e \u2212 \u03c1\u03b2\u22251 = \u02dcO\n\u0012\nnB\n\u03b2\n\u03c3 max(\u03c4mix(L\u221e), \u03c4mix(Ldiss)\n\u0013\n(31)\nwhere \u02dcO denotes an upper bound up to\n(poly)logarithmic corrections. This part of the\nfixed-point error is hence controlled by the width of\nthe filter function in the time domain \u03c3 compared\nto the inverse temperature \u03b2.\nCombining the two sources of fixed-point error, we\nfind\n\u2225\u03c1T \u2212 \u03c1\u03b2\u22251 = \u02dcO\n\u0012\nnB\n\u0012\u03c3\nT e\u2212(T\n\u03c3 )\n2\n+ \u03b2\n\u03c3\n\u0013\n\u03c4\u2217\nmix\n\u0013\n(32)\n8where \u03c4\u2217\nmix \u2261 max(\u03c4mix(LT ), \u03c4mix(L\u221e), \u03c4mix(Ldiss)).\nThis informs the choice of filter function and finite-\ntime truncation: we want to choose T \u226b \u03c3 \u226b \u03b2 [cf.\nEq. (33)].\nC. Resource Estimate\nUsing the error bounds derived above, we can es-\ntimate the total cost of preparing a thermal state \u03c1\u03b2\nup to some precision \u03f5. For that, we want both the\nfixed-point error Eq. (32), as well as the total accu-\nmulated simulation error Eq. (27) to be \u02dcO(nB\u03f5).\nFor a given target accuracy \u03f5, we choose\n\u03c3 = \u03b2\u03c4 \u2217\nmix\n\u03f5 (33a)\nT = \u03c3\nr\nlog \u03c4\u2217\nmix\n\u03f5 = \u03b2\u03c4 \u2217\nmix\n\u03f5\nr\nlog \u03c4\u2217\nmix\n\u03f5 (33b)\nwhich, upon substitution into Eq. (32), yields\n\u2225\u03c1T \u2212 \u03c1\u03b2\u22251 = \u02dcO\n\u0010\nnB\u03f5 (1 +\np\nlog \u03f5\u22121)\n\u0011\n(34)\n= \u02dcO(nB\u03f5). (35)\nFurther, the total accumulated error during a\nLindbladian time evolution of time \u221d \u03c4mix \u2261\n\u03c4mix(LT ) is O(nB\u03c4mixJ2) = O(nB\u03f5) if J2 = O(\u03f5\u03c4\u22121\nmix)\nand hence we need Nstep = O(\u03c42\nmix\u03f5\u22121) elementary\nsteps of the protocol to reach the steady state. The\ntotal Hamiltonian evolution time necessary to pre-\npare the thermal state\u03c1\u03b2 up to error \u02dcO(nB\u03f5) in trace\nnorm then scales as\nTtot = O(NstepT) (36a)\n= \u02dcO\n \n\u03b2 \u03c42\nmix \u03c4\u2217\nmix\n\u03f52\nr\nlog \u03c4\u2217\nmix\n\u03f5\n!\n(36b)\n= \u02dcO\n\u0012\u03b2 \u03c42\nmix \u03c4\u2217\nmix\n\u03f52\n\u0013\n. (36c)\nThis remains polynomial in system size even if we de-\nmand that \u03c1\u03b2 is accurate up to an O(1) error [such\nthat \u03f5\u22121 \u223c O(nB) \u223c O(nS)], assuming a polynomial-\nin-system size mixing time. In practice, the method\nis most effective if the mixing time has a mild (e.g.\nlogarithmic) scaling with nS, and \u03f5 can be chosen to\nbe some O(1) number, which we expect to be suffi-\ncient in many cases to obtain e.g. local observables\nin the thermal state up to an \u02dcO(\u03f5) error.\nThe mixing time is in general controlled by spec-\ntral properties of the Lindbladian [51, 52]. Although\nthere exist Hamiltonians for which any dynamics\nwith (quasi-)local jump operators can be shown to\nhave a mixing time that is exponentially slow in sys-\ntem size [40\u201343], fast mixing has been established\nfor many models and temperature regimes of inter-\nest [53\u201360]. Note that while we do not write this\nexplicitly in Eq. (36), the mixing time in general\ndoes depend on the other parameter choices of the\nprotocol, in particular the width of the filter func-\ntion \u03c3. This means in particular that the choice\nof parameters in Eq. (33) may be nontrivial, or, if\n\u03c4mix scales very unfavorably with \u03c3, even impossi-\nble for some \u03f5. Note that while such scaling would\nalso have a significant impact on the resources nec-\nessary to implement the exact sampler, in this case\nthe problem is much less severe since the only error\nin practice comes from finite-time truncation of the\njump operators. We do not expect unfavorable scal-\ning with \u03c3 in the general case, and we demonstrate\nthe effectiveness of our protocol in a simple exam-\nple numerically below. Still, we expect the question\nof the optimal choice of filter functions and jump\noperators that minimize such dependencies will be\nof significant importance when implementing these\nprotocols in practice.\nD. Reduction to Quasiparticle Cooling and the\n\u201cRewinding\u201d Step\nWe now comment on the precise link between\nour work, and the quasiparticle cooling protocol of\nRefs. 9 and 10. Relative to the latter, our protocol\n[see Fig. 1] implements an additional backwards time\nevolution after each step. As noted above, this step\nis what allows us to build a channel that involves\nrepeated application of the interaction-picture uni-\ntary \u02dcV . However, evidently this step is superfluous\nif for each cycle [\u03c1(t), H] = 0 at a time t after the ap-\nplication of \u02dcV . This in essence is the \u2018quasiparticle\nassumption\u2019 at the heart of Refs. 9 and 10.\nHowever, we note that, since it involves forward\ntime evolution under \u2212H, the rewinding step is po-\ntentially tricky to implement in practice for analog\nsimulators, and may be costly in any case as it dou-\nbles the duration of the protocol. However, we show\n(in App. D) that even in the absence of any quasipar-\nticle assumption, dropping the rewinding step only\nincreases the fixed-point error by an additional fac-\ntor of \u03c4mix, viz.\n\u2225\u03c1\u2032\nT \u2212 \u03c1\u03b2\u2225 \u22644\u03b4\u03c4mix (37)\nwhere \u03b4 is the error of the protocol with rewinding\nin Eq. (32). This leads to an additional factor of\n90 5 10 15 20 25 30\nResets\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n\u2206e(t)\n(a)\nK[\u03c1]\neLT \u2206 \u03c4[\u03c1]\neL\u03b2\u2206 \u03c4[\u03c1]\n0 5 10 15 20 25 30\nResets\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nTrace distance\n(b)\nFIG. 3. Comparison of the time evolution of thermal state preparation for the mixed-field Ising model Eq. (38) under\nthe exact Gibbs sampler L [cf. Eq. (14b)] (black), the Lindbladian LT [cf. Eq. (23a)] (red) and the local driving\nprotocol K[\u03c1] [cf. Eq. (18)] (blue), starting from the maximally mixed state \u03c10 = 2 \u2212nS 1. Parameters: \u03b2 = 1 .0,\n\u03c3 = 0.5, J = 0.5 , nS = 8, T = 6\u03c3. (a) Time evolution of the error in the average energy density \u2206 e(t) =\n\u27e8H\u2212H\u03b2\u27e9\n\u27e8H\u03b2\u27e9 .\nThe energy density approaches the exact result within less than two percent. (b) Evolution of the trace distance to\nthe Gibbs state \u03c1\u03b2. The trace distance decays, indicating convergence toward a state close to the Gibbs state.\n\u03c4\u2217\nmix in the cost estimate in Eq. (36). While this can\nbe severe if the mixing time scaling is particularly\nadverse, the total Hamiltonian time evolution cost\nnevertheless remains polynomial in system size for\nany H for which the full protocol has poly(n) scaling.\nV. NUMERICAL CASE STUDY\nTo illustrate the feasibility of our protocol, we\nnumerically simulate our protocal as discrete-time\nquantum channel, and compare the resulting evolu-\ntion to the dissipative evolution under the Lindbla-\ndians LT [cf. Eq. (23a)] (to confirm that we approx-\nimately simulate it) and the exact Gibbs sampler L\u03b2\n[Eq. (14b)] (whose simulation is our objective).\nAs a concrete example, we consider thermal state\npreparation for the mixed-field Ising model:\nH =\nX\ni\nZiZi+1 + gXi + hZi. (38)\nWe set g = 0.9045 and h = 0.809 , parameters for\nwhich the model is known to thermalize rapidly [61].\nTo simulate the Lindbladian dynamics, we numer-\nically solve the Lindblad equation using the adaptive\nRunge\u2013Kutta method of order 5(4) [62]. The initial\ndensity matrix \u03c1 is taken to be the maximally mixed\nensemble.\nThe results are shown in Fig. 3, where we present\ndata for \u03b2 = 0 .5, \u03c3 = 0 .5, J = 0 .5, T = 6 \u03c3 and\na system size of nS = 8 qubits. The evolution un-\nder the exact Gibbs sampler is shown in black, while\nthe results for the Lindbladian L\u221e [cf. Eq. (23)] are\nshown in red and under the local driving protocol\nK are shown in blue [cf. Eq. (18)]. Results corre-\nsponding to evolving with the Lindbladian L or L\u221e\nonly are shown in dashed lines. In the Schroedinger\npicture, this corresponds to the case where reverse\ntime evolution U = eiHt is applied after the time-\ndependent system-bath interaction.\nIn Fig. 3(a), we display the dynamics of the av-\nerage energy, divided by the energy density of the\nthermal state\n\u2206e(t) = \u27e8H \u2212 H\u03b2\u27e9\nH\u03b2\n, (39)\nwith H\u03b2 = Tr(H\u03c1\u03b2) being the average energy of the\nGibbs state.\nIn both cases, the energy rapidly converges to the\nthermal value within less than 2 percent.\nDue to the deviations from an exact Gibbs sam-\npler, the results converge to a state close to the\nGibbs state. This is shown in Figure 3 (b), where\n10the trace distance with respect to the thermal state\n\u03c1\u03b2 is displayed. The trace distance quickly decays,\ndemonstrating rapid convergence to the steady state.\nThese results indicate that the local driving sam-\npler yields a good approximation of the exact Gibbs\nsampler and provides evidence for the effectiveness\nof our protocol for thermal state preparation.\nVI. CONCLUDING REMARKS\nIn this work, we have demonstrated that it is in\nprinciple possible to prepare approximate thermal\nstates with rigorously controlled error bounds using\ningredients commonly available in the current gener-\nation of quantum devices: implementation of a local\nHamiltonian, time-dependent local driving, and the\nability to reset ancillas. The local driving protocol\nwe introduce and study exactly simulates the dissi-\npative portions of an exact Gibbs sampler (whose\nsteady state is the thermal density matrix \u03c1\u03b2), but\nrather than the specifically tailored coherent evolu-\ntion of the latter, involves a physical Lamb shift.\nDespite this, we are able to show that the proto-\ncol prepares \u03c1\u03b2 with an accuracy that is polynomial\nin system size, assuming that the modifications in-\nvolved do not lead to an insurmountable blow-up\nof mixing times. The last is a challenging question\nthat will need to be addressed on case-by-case ba-\nsis for specific systems, although existing results in\nthe theory of mixing times [53\u201360] offer grounds for\ncautious optimism.\nA notable feature of our proposal is that it does\nnot require complicated \u201cquantum programming\u201d\nsuch as the block-encoding of operators. As such,\nit can be readily adapted to analog quantum simu-\nlators [63], such as those involving ultracold atoms,\nfor which the most nontrivial obstacles will likely be\na convenient implementation of the ancilla reset step\n(essential) and the backward time evolution (desir-\nable).\nThe ready applicability of our protocol is un-\nderscored by the fact that for a suitable choice of\njump operators, it resembles \u201cquasiparticle cooling\u201d\nRefs. [9, 10]; this also indicates that (modulo mixing\ntime considerations) the existence of an approximate\nquasiparticle description is not a necessary condition\nfor such state preparation methods.\nThe key distinction from the protocols in Ref. [9,\n10] is the requirement of backward time evolution,\nwhich is essential in order that the dissipation match\nthat of the exact Gibbs sampler. From a practical\nstandpoint, eliminating this rewinding step is highly\ndesirable, as it doubles the simulation time and\nmay be challenging to implement in experimental\nsetups. Formally, this loosens the accuracy bound\nby a power of the mixing time, but future work [64]\nwill explore whether this can be done without com-\npromising the performance of the protocol.\nClearly, it would be highly desirable to develop\nsimilar readily-implementable protocols that more\nclosely approximate the exact coherent term of\nRef. [27, 29]. Possible routes to this might involve a\njudicious choice of forward and backward time evo-\nlution to \u201cecho out\u201d spurious Lamb shift terms, and\nsimilar techniques of quantum control. A surprising\npreliminary result in this regard is that theneglect of\nthe rewinding step, despite loosening the formal er-\nror bound, appears to improve the convergence and\naccuracy of the resulting protocol. A more careful\nstudy of the optimal parameter choice for efficient\nstate preparation \u2014 especially one organized around\nphysical properties of the target state \u2014 as well as\na more principled understanding of mixing times for\nlocal driving samplers remain important directions\nfor future work. Finally, it would be interesting to\nask how these methods compare operationally with\nothers for controlled approximation of thermal ob-\nservables on quantum or even classical [65\u201368] com-\nputers.\nACKNOWLEDGMENTS\nWe thank C.-F. Chen for useful discussions on the\nsecular approximation and Minh Tran for helpful\ncorrespondence on Ref. 50. We further thank Pieter\nClaeys for helpful discussions. We acknowledge sup-\nport from a Leverhulme Trust International Profes-\nsorship grant (Award Number: LIP-2020-014, for\na Leverhulme-Peierls Fellowship at Oxford to DH,\nBP), the UKRI under a Frontier Research Guaran-\ntee (for an ERC Consolidator Grant) EP/Z002419/1\n(SAP), and the Alexander von Humboldt foundation\nthrough a Feodor-Lynen fellowship (BP).\n11Appendix A: Structure of the Exact Gibbs Sampler from KMS Detailed Balance\nIn this section, we provide a detailed derivation of the constraints imposed on the detailed balance Lind-\nbladian by detailed balance, following Ref. 27. Our starting point is the Bohr-frequency representation Eq.\n(7). We will assume (as in the main text) that L consists either of Hermitian jump operators, or else is\n\u2018paired\u2019 such that Aa appears in the set of jump operators A if and only if A\u2020\na also appears in A. The reason\nfor this will become apparent below.\nBefore proceeding, recall that we have already introduced the \u201cweighting\u201d operator \u0393 \u03c1 involving conjuga-\ntion with powers of any full-rank state \u03c1:\n\u0393\u03c1[\u00b7 ] := \u03c11/2(\u00b7 )\u03c11/2, (A1)\nIt is convenient to also define a second such operator,\n\u039b\u03c1[\u00b7 ] := \u03c1\u22121/2(\u00b7 )\u03c11/2, (A2)\nsuch that for a Hermitian operator X we have \u0393\u03c1[X]\u2020 = \u0393\u03c1[X] and \u039b\u03c1[X]\u2020 = \u039b\u22121\n\u03c1 [X].\nFirst, we consider the detailed balance condition on the transition part T ; a short calculation gives us that\n(\u0393\u22121\n\u03c1\u03b2 \u25e6 T \u25e6\u0393\u03c1\u03b2 )[\u00b7 ] =\nX\na\u2208A\nX\n\u03bd1,\u03bd2\u2208B\n\u03b1\u03bd1,\u03bd2 \u03c1\u03b2\n\u22121/2Aa\n\u03bd1 \u03c1\u03b2\n1/2(\u00b7 )\u03c1\u03b2\n1/2(Aa\n\u03bd2 )\u2020\u03c1\u03b2\n\u22121/2\n=\nX\na\u2208A\nX\n\u03bd1,\u03bd2\u2208B\n\u03b1\u03bd1,\u03bd2 e\n\u03b2(\u03bd1+\u03bd2)\n2 Aa\n\u03bd1 (\u00b7 )(Aa\n\u03bd2 )\u2020. (A3)\nwhere we used the fact that \u03c1\u2212sA\u03bd\u03c1s = e\u03b2s\u03bd A\u03bd and hence \u03c1s(A\u03bd)\u2020\u03c1\u2212s = e\u03b2s\u03bd (A\u03bd)\u2020, which follow from the\nBohr frequency representation Eq. (6).\nOn the other hand, by using tr {OT [\u03c1]} = tr{\u03c1T \u2020[O]} to determine T \u2020, we have\nT \u2020[\u00b7 ] =\nX\na\u2208A\nX\n\u03bd1,\u03bd2\u2208B\n\u03b1\u03bd1,\u03bd2 (Aa\n\u03bd2 )\u2020(\u00b7 )Aa\n\u03bd1\n=\nX\na\u2208A\nX\n\u03bd1,\u03bd2\u2208B\n\u03b1\u03bd2,\u03bd1 (Aa\n\u03bd1 )\u2020(\u00b7 )Aa\n\u03bd2 (Relabeling \u03bd1 \u2194 \u03bd2)\n=\nX\na\u2208A\nX\n\u03bd1,\u03bd2\u2208B\n\u03b1\u2212\u03bd2,\u2212\u03bd1 (Aa\n\u2212\u03bd1 )\u2020(\u00b7 )Aa\n\u2212\u03bd2 (Since if \u03bd \u2208 B then \u2212\u03bd \u2208 B)\n=\nX\na\u2208A\nX\n\u03bd1,\u03bd2\u2208B\n\u03b1\u2212\u03bd2,\u2212\u03bd1 ((Aa\u2020)\u2212\u03bd1 )\u2020(\u00b7 )(Aa\u2020)\u2212\u03bd2 (Since if Aa \u2208 Athen Aa\u2020 \u2208 A)\n=\nX\na\u2208A\nX\n\u03bd1,\u03bd2\u2208B\n\u03b1\u2212\u03bd2,\u2212\u03bd1 Aa\n\u03bd1 (\u00b7 )(Aa\n\u03bd2 )\u2020 (Since (A\u03bd)\u2020 = (A\u2020)\u2212\u03bd) (A4)\nComparing Eq. (A3) and Eq. (A4), we see that in order for T to satisfy the detailed balance condition\nT \u2020 = \u0393\u22121\n\u03c1 \u25e6 T \u25e6\u0393\u03c1, we require that the kernel \u03b1\u03bd1,\u03bd2 satisfies the condition\n\u03b1\u2212\u03bd2,\u2212\u03bd1 = \u03b1\u03bd1,\u03bd2 e\n\u03b2(\u03bd1+\u03bd2)\n2 . (A5)\nWe highlight the crucial role played by the requirement that every jump operator in T is \u2018paired\u2019 with\nits Hermitian conjugate. Without this, since the left/right action of A and A\u2020 are exchanged under the\ntransformation from T to T \u2020, it would not be possible to relate T and T \u2020. Physically, the detailed balance\nparametrized by Eq. (3) (or Eq. (A5)) relates \u2018downhill\u2019 transitions driven by A\u03bd with \u2018uphill\u2019 transitions\ndriven by (A\u2020)\u03bd. Note that this is related to the observation (Appendix B of Ref. 10) that the rate equation\nfor \u2018quasiparticle cooling\u2019 is detailed-balanced only if the jump operators are chosen to be Hermitian.\n12We now define K = G \u2212 iR\n2 , so that ( G + R)[\u00b7 ] = \u2212iK(\u00b7 ) + i(\u00b7 )K\u2020, and ( G + R)\u2020[\u00b7 ] = iK\u2020(\u00b7 ) \u2212 i(\u00b7 )K.\nWe can then write\n(G + R)\u2020[\u00b7 ] \u2212 [\u0393\u22121\n\u03c1\u03b2 \u25e6 (G + R) \u25e6 \u0393\u03c1\u03b2 ][\u00b7 ] = i\n\u0000\nK\u2020(\u00b7 ) \u2212 (\u00b7 )K\n\u0001\n+ i\n\u0010\n\u039b\u03c1\u03b2 [K](\u00b7 ) \u2212 (\u00b7 )\u039b\u22121\n\u03c1\u03b2 [K]\n\u0011\n= i(K\u2020 + \u039b\u03c1\u03b2 [K])(\u00b7 ) \u2212 i(\u00b7 )(K + \u039b\u03c1\u03b2 [K]\u2020), (A6)\nwhere we have used the fact that if Q[\u00b7 ] = Q1(\u00b7 ) \u00b1 (\u00b7 )Q2, then\n\u0393\u22121\n\u03c1 \u25e6 Q \u25e6\u0393\u03c1[\u00b7 ] = \u03c1\u22121/2\n\u0010\nQ1\n\u0010\n\u03c11/2(\u00b7 )\u03c11/2\n\u0011\n\u00b1\n\u0010\n\u03c11/2(\u00b7 )\u03c11/2\n\u0011\nQ2\n\u0011\n\u03c1\u22121/2 = \u039b\u03c1[Q1](\u00b7 ) \u00b1 (\u00b7 )\u039b\u22121\n\u03c1 [Q2], (A7)\nand that \u039b\u22121\n\u03c1 [K\u2020] = \u039b\u22121\n\u03c1 [G] + i\n2 \u039b\u22121\n\u03c1 [R] = \u039b\u03c1[G]\u2020 + i\n2 \u039b\u03c1[R]\u2020 =\n\u0000\n\u039b\u03c1[G] \u2212 i\n2 \u039b\u03c1[R]\n\u0001\u2020\n= \u039b\u03c1[K]\u2020.\nSince G + R must satisfy detailed balance Eq. (3) independently of T , we require that the RHS of Eq.\n(A6) vanishes. This in term requires that\nK\u2020 + \u039b\u03c1\u03b2 [K] = 0 + \u03bbI (A8)\nwhere \u03bb \u2208 R. For simplicity, we will set \u03bb = 0. Now, since B, Rare Hermitian, we can write\nK\u2020 + \u039b\u03c1\u03b2 [K] = B + i\n2R + \u039b\u03c1\u03b2 [B] \u2212 i\n2\u039b\u03c1\u03b2 [R]. (A9)\nUsing the explicit forms of B and R in Eq. (7) and\n\u039b\u03c1\u03b2 [(Aa\n\u03bd2 )\u2020Aa\n\u03bd1 ] = \u03c1\u22121/2\n\u03b2 (Aa\n\u03bd2 )\u2020Aa\n\u03bd1 \u03c11/2\n\u03b2\n= e\n\u03b2(\u03bd1\u2212\u03bd2)\n2 (Aa\n\u03bd2 )\u2020Aa\n\u03bd1 , (A10)\nwe find that\nK\u2020 + \u039b\u03c1\u03b2 [K] =\nX\na\u2208A\nX\n\u03bd1,\u03bd2\n\u0014\u0010\n1 + e\n\u03b2(\u03bd1\u2212\u03bd2)\n2\n\u0011\ng\u03bd1,\u03bd2 + i\n2\n\u0010\n1 \u2212 e\n\u03b2(\u03bd1\u2212\u03bd2)\n2\n\u0011\n\u03b1\u03bd1,\u03bd2\n\u0015\n(Aa\n\u03bd2 )\u2020Aa\n\u03bd1 (A11)\nSetting this to zero gives us a condition on kernels of the decay and coherent parts such that they together\nsatisfy detailed balance:\ng\u03bd1,\u03bd2 = i\n2\n1 \u2212 e\n\u03b2(\u03bd1\u2212\u03bd2)\n2\n1 + e\n\u03b2(\u03bd1\u2212\u03bd2)\n2\n\u03b1\u03bd1,\u03bd2 = \u2212 1\n2i tanh \u03b2(\u03bd1 \u2212 \u03bd2)\n4 \u03b1\u03bd1,\u03bd2 .. (A12)\nAppendix B: Detailed Derivation of Error Bounds I: Lindbladian Evolution and Magnus Expansion\nIn this section, we provide a detailed derivation of the first of the two error bounds used in the main text\nto derive the upper bound on the total Hamiltonian evolution time needed to prepare a thermal state up so\nsome error nB\u03f5 in trace distance.\nHere, we bound the errors incurred by the Lindbladian evolution and the Magnus expansion: we show\nthat that given K as in Eq. (18), and with LT given in Eq. (23a), that\n\r\r\reJ2LT [\u03c1] \u2212 K[\u03c1]\n\r\r\r\n1\n= O\n\u0000\nnBJ4\u0001\n. (B1)\n1. Bounding the Magnus Approximation Error\nRecall that in the interaction picture, we implement the Hamiltonian\n\u02dcHSB(t) =\nX\na\nJ{f(t)B\u2020\na(t) \u2297 Aa(t) + f\u2217(t)Ba(t) \u2297 A\u2020\na(t)}. (B2)\n13Here B = 1\n2 (XB \u2212 iYB), and B\u2020 = 1\n2 (XB + iYB) are lowering and raising operators for the ancilla, and f(t)\nis given by Eq. (25).\nUsing the Magnus expansion, we may express the exact time evolution operator \u02dcV (see Eq. (19), repro-\nduced here) for convenience\n\u02dcV = T exp\n \n\u2212iJ\nZ T/2\n\u2212T/2\ndt \u02dcHSB(t)\n!\n(B3)\nup to third order as\n\u02dcV = e\u2212i P3\nn=1\n\u02dcH(n)\nM + O(nBJ4) (B4)\nwith\n\u02dcH(1)\nM =\nZ T/2\n\u2212T/2\ndt1 \u02dcHSB(t1), (B5)\n\u02dcH(2)\nM = 1\n2i\nZ T/2\n\u2212T/2\ndt1\nZ t\n\u2212T/2\ndt2 [ \u02dcHSB(t1), \u02dcHSB(t2)]\n\u02dcH(3)\nM = \u2212 i\n6\nZ T/2\n\u2212T/2\ndt1\nZ t1\n\u2212T/2\ndt2\nZ t2\n\u2212T/2\ndt3\n\u0010\n[ \u02dcHSB(t1), [ \u02dcHSB(t2), \u02dcHSB(t3)]] + [ \u02dcHSB(t3), [ \u02dcHSB(t2), \u02dcHSB(t1)]]\n\u0011\n.\n(B6)\nNote that the higher-order terms includes commutators between different jump operators (and hence, given\nour construction, different ancillas a, a\u2032).\nThe error incurred can be bounded by using (a mild generalization of) Theorem 1 of Ref. 50. Adapting\ntheir notation to the present setting, Ref. 50 considers the question of approximating the time evolution\noperator corresponding to evolution under a (potentially time-dependent) k-local Hamiltonian HSB, each\nterm of which is supported on a subset X of at most k qubits, in the interaction picture of a geometrically\nlocal Hamiltonian H. We then have\n\r\r\r\r\r\n\u02dcV \u2212 exp\n \n\u2212i\nqX\nn=1\n\u02dcH(n)\nM\n!\r\r\r\r\r \u2264 O(nB(JdT )q+1) (B7)\nwhere d = maxi\nP\nX:i\u2208X \u2225HSB\u2225 is the interaction degree (where the maximum is taken over all sites in the\nsystem and bath) of BX, and \u02dcHSB(t) = eiHtHSBe\u2212iHt is the time-evolved interaction-picture operator. In\nour case, X consists of a single site and its ancilla, andHSB(t) = P\na f(t)A\u2020\na(t)\u2297Ba(t)+h.c. is explicitly time-\ndependent. Note that Ref. 50 explicitly considers time-independent BX, but observe that the generalization\nto time-dependent BX is immediate given the nature of their proof as long as d can be bounded at all times.\nA second comment is in order regarding the scaling with nB in (B7); Ref. 50 do not have a separation of\nsystem and bath and so the operator that plays the role \u02dcHSB simply scales with the system size. Analysing\nthe derivation of their bound (or equivalently, by applying it to each term in the sum within \u02dcHSB separately)\nit is clear that the appropriate scaling in our case is with nB rather than the total size of system and bath\ntogether, nS + nB. In practice of course for effective cooling, nB \u221d nS so the distinction is not fundamental,\nbut may be relevant for practical considerations where prefactors matter.\nWe see that given the choice of filter function, since\n\r\rA\u2020\na(t) \u2297 Ba(t)\n\r\r = 1, we have\nd = max\nt\n|f(t)| =\nr\n2\n\u03c0\u03c32 e\n\u03b22\n8\u03c32 . (B8)\nElsewhere in the proof, we have that \u03b2 \u2272 \u03c3, so that we see that we can always take max t |f(t)| = O\n\u00001\n\u03c3\n\u0001\n.\nNow, the drive is applied for duration T = \u02dcO(\u03c3), characterized by the width of the filter function, so in other\nwords d = O\n\u0000 1\nT\n\u0001\n. Combining this with Eq. (B7), we arrive at Eq. (20).\n142. Bounding the Error of the Reset Protocol\nIt remains to bound the error of the reset protocol, i.e. the fact that we implement a Lindbladian\nby effectively \u201cTrotterizing\u201d: the drive followed by reset results in a quantum channel that approximates\nevolution with the Lindbladian for a short time.\nRecall that each time step of our evolution implements the quantum channel\nK[\u03c1] = trB\nh\n\u02dcV\n\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001 \u02dcV \u2020\ni\n, (B9)\nwith \u03c10\nB = NnB\na=1 |0a\u27e9\u27e80a|B. Consider LT as defined in Eq. (23). We argue that if we take J =\n\u221a\n\u2206\u03c4, then\n\r\re\u2206\u03c4LT [\u03c1] \u2212 K[\u03c1]\n\r\r\n1 \u2264 O(nBJ4). (B10)\nNote that the scaling is one order in J better than naively expected; this is due to the trace over the bath\ndegrees of freedom. We now derive this in detail.\nWe begin by defining the Magnus-approximated channel\nKM [\u03c1] = trB\n\"\nexp\n \n\u2212i\nqX\nn=1\n\u02dcH(n)\nM\n!\n\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001\nexp\n \ni\nqX\nn=1\n\u02dcH(n)\nM\n!#\n(B11)\nUsing the triangle inequality,\n\r\re\u2206\u03c4LT [\u03c1] \u2212 K[\u03c1]\n\r\r\n1 \u2264\n\r\re\u2206\u03c4LT \u03c1 \u2212 KM [\u03c1]\n\r\r\n1 + \u2225KM [\u03c1] \u2212 K[\u03c1]\u22251 (B12)\nThe second term has the scaling\n\u2225KM [\u03c1] \u2212 K[\u03c1]\u2225 \u2264 O\n\u0000\nnBJ4\u0001\n, (B13)\ndue to the Magnus expansion error bound Eq. (B4) combined with the fact that the partial trace (over the\nancillas) is a contractive operation.\nIt remains to bound the first term. To do so, we show that e\u2206\u03c4LT \u03c1 and KM [\u03c1] agree up to O(nBJ4). On\nthe one hand, the identification J2 = \u03c4 gives\ne\u2206\u03c4LT [\u03c1] = \u2206\u03c4LT [\u03c1] + O(nBJ4) (B14)\nOn the other hand, we consider the expansion of KM [\u03c1]. Due to the trace over ancilla qubits at the end\nof the protocol and the structure of the system-bath interactions, all odd orders in J cancel. It follows that\nKM [\u03c1] = trB\nh\n\u02dcH(1)\nM\n\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001 \u02dcH(1)\nM\ni\n\u2212 1\n2 trB\n\u0014\u0010\n\u02dcH(1)\nM\n\u00112 \u0000\n\u03c10\nB \u2297 \u03c1\n\u0001\n+\n\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001\u0010\n\u02dcH(1)\nM\n\u00112\u0015\n\u2212 i trB\nh\n\u02dcH(2)\nM\n\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001\n\u2212\n\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001 \u02dcH(2)\nM\ni\n+ O(nBJ4).\n(B15)\nWe consider the first term in detail; the treatment of the other terms proceeds analogously. We have\ntrB\nh\n\u02dcH(1)\nM\n\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001 \u02dcH(1)\nM\ni\n= J2 X\na,a\u2032\nZ T/2\n\u2212T/2\ndt1\nZ T/2\n\u2212T/2\ndt2 trB\n\u0002\u0000\nf(t1)B\u2020\na(t1) \u2297 Aa(t1) + f\u2217(t1)Ba(t1) \u2297 A\u2020\na(t1)\n\u0001\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001\n\u0010\nf(t2)B\u2020\na\u2032(t2) \u2297 Aa\u2032(t2) + f\u2217(t2)Ba\u2032(t2) \u2297 A\u2020\na\u2032(t2)\n\u0011i\n(B16)\n15Tracing out the bath degrees of freedom forces a = a\u2032. Meanwhile, placing the the bath degree of freedom in\nthe |0\u27e9 state (the reset step) forces the application of Ba(t1) on the left and B\u2020\na(t2) on the right. Together,\nthese give\ntrB\nh\n\u02dcH(1)\nM\n\u0000\n\u03c10\nB \u2297 \u03c1\n\u0001 \u02dcH(1)\nM\ni\n= J2 X\na\nZ T/2\n\u2212T/2\ndt1\nZ T/2\n\u2212T/2\ndt2 f(t1)f\u2217(t2)Aa(t1)(\u00b7)A\u2020\na(t2). (B17)\nIdentifying \u2206 \u03c4 = J2, this gives the transition part in Eq. (23). Similarly, the remaining terms in the\nexpansion Eq. (B15) of KM [\u03c1] can be respectively identified with the decay and coherent parts of LT ,\ncompleting the proof.\nAppendix C: Detailed Derivation of Error Bounds II: Fixed-Point Error\nIn this section, we consider the second of the two error bounds used in the main text. Namely, we obtain\nan upper bound on the overall fixed point error, i.e. the difference between the fixed point of the local driving\nsampler LT in Eq. (23a), (that we denote \u03c1T ), and the thermal state \u03c1\u03b2.\nWe split the total error into two parts using the triangle inequality, viz.\n\u2225\u03c1T \u2212 \u03c1\u03b2\u22251 \u2264 \u2225\u03c1T \u2212 \u03c1\u221e\u22251 + \u2225\u03c1\u221e \u2212 \u03c1\u03b2\u22251 (C1)\nwhere \u03c1\u221e is the fixed point of is the fixed point of L \u2261limT\u2192\u221e LT , the infinite time evolution limit of the\nlocal driving sampler, i.e. L[\u03c1\u221e] = 0. The two terms on the RHS of the above equation capture different\npieces of physics. The first is the error incurred by the fact that the exact sampler involves an infinite-time\nsmoothing, while the local driving sampler is truncated at time T. The the second essentially comes from\nthe fact that even as T \u2192 \u221e, the local driving sampler has the \u201cwrong\u201d coherent part, given by the Lamb\nshift, which we tackle via the so-called \u201csecular approximation\u201d. We address these in turn.\nNote that throughout this section to streamline notation we consider a single jump operator A; the\ngeneralization to multiple jump operators Aa is straightforward and simply involves a sum over a, and all\nthe errors simply pick up an overall factor of nB = |A|.\n1. Error from finite-time evolution\nHere, we prove that\n\u2225\u03c1T \u2212 \u03c1\u221e\u22251 \u2264 4 \u03c4mix \u2225LT \u2212 L\u221e\u22251\u22121 \u2264 24\n\u03c0 nB\n\u0010\u03c3\nT\n\u0011\ne\u2212(T\n\u03c3 )\n2\n\u03c4mix(LT ) (C2)\nWe can use Lemma II.1 of [26] to write\n\u2225\u03c1T \u2212 \u03c1\u22251 \u2264 4 tmix(LT )\u2225LT \u2212 L\u221e\u22251\u22121. (C3)\nRecall that\nLT [\u00b7] =\nZ T/2\n\u2212T/2\ndt1\nZ T/2\n\u2212T/2\ndt2 f(t1)f\u2217(t2)\n\u0012\n\u2212i\n\u0014\n\u2212sgn(t1 \u2212 t2)\n2i A\u2020(t2)A(t1), \u00b7\n\u0015\n+ A(t1)(\u00b7)A\u2020(t2) \u2212 1\n2\n\b\nA\u2020(t2)A(t1), \u00b7\n\t\u0013\n.\n(C4)\nSince L\u221e = limT\u2192\u221e LT , using \u2225A(t)\u2225 \u22641 and |sgn(t1\u2212t2)\n2 | < 1\n2 , |\u0393(t2 \u2212 t1)| = 1 and the H\u00a8 older inequality,\nwe obtain\n\u2225LT \u2212 L\u221e\u22251\u22121 \u2264 3\nZ\nR[T]\ndt1\nZ\nR[T]\ndt2 |f(t1)f\u2217(t2)| = 3\n\"Z\nR[T]\ndt1|f(t1)|\n#2\n(C5)\n16where we have defined R[T] = R\\[\u2212T/2, T/2]. For our specific choice of filter function\nf(t) =\nr\n2\n\u03c0\u03c32 e\u2212 2\n\u03c32 (t\u2212i\u03b2\n4 )\n2\n, (C6)\nwe obtain\nZ\nR[T]\ndt1|f(t1)| = erfc\n\u0012 T\n\u03c3\n\u221a\n2\n\u0013\n\u2264\n\u221a\n2\u03c3\u221a\u03c0T e\u2212 T2\n2\u03c32 , (C7)\nwhere the last bound holds for T2\n2\u03c32 > 1, which is guaranteed by the choice in Eq. (33) as \u03f5 \u2192 0. This leads\nto the final bound\n\u2225\u03c1T \u2212 \u03c1\u22251 \u2264 24\n\u03c0\n\u0010\u03c3\nT\n\u00112\ne\u2212(T\n\u03c3 )\n2\ntmix(LT ). (C8)\n2. Secular Approximation for the Lamb Shift\nWe next derive the inequality\n\u2225\u03c1\u221e \u2212 \u03c1\u03b2\u22251 = \u02dcO\n\u0012\nnB\n\u03b2\n\u03c3 max(\u03c4mix(L\u221e), \u03c4mix(Ldiss)\n\u0013\n(C9)\nThe derivation ultimately proceeds by means of implementing the so-called \u201csecular approximation\u201d, but\nfirst we need to perform some preliminary manipulations involving eigenvector perturbation theory. Much of\nour treatment is agnostic to specific choices of the filter function, except the very last portions of the proof\nwhich impose some requirements of the time-domain asymptotics of f(t). Accordingly we will only specific\nour choice of f(t) where necessary.\na. Preliminaries.\nWe first establish some notation. We denote by L\u221e, Lsec, and L\u03b2 the Lindbladians that respectively\ncorrespond to the T \u2192 \u221elimit of the local-driving sampler LT , its secular approximation (to be explained\nbelow), and the exact sampler. These all have identical dissipative parts, but differ in their coherent parts:\nL\u03b2[\u00b7 ] = \u2212i[G, \u00b7 ] + Ldiss[\u00b7 ]\nL\u221e[\u00b7 ] = \u2212i[HLS, \u00b7 ] + Ldiss[\u00b7 ] = \u2212i[B, \u00b7 ] + L\u03b2[\u00b7 ]\nLsec[\u00b7 ] = \u2212i[Bsec, \u00b7 ] + L\u03b2[\u00b7 ] = \u2212i[Bsec + G, \u00b7 ] + Ldiss[\u00b7 ] (C10)\nwhere we have defined B = HLS \u2212 G, the difference between the coherent parts of the local-driving and\nexact samplers, and its secular approximation Bsec (to be specified below). Note that in contrast to Ref.\n26 (Appendix D), we do not make a secular approximation for the dissipative part of the sampler. This\nis because we are bounding the difference between the local-driving sampler and the exact Gibbs sampler,\nwhereas Ref. 26 were bounding the difference between a physical system-bath problem and a suitably\nsmoothed approximation of a Davies Lindbladian without the coherent part necessary to make the latter an\nexact Gibbs sampler. This difference considerably streamlines our proof relative to that of Ref. 26.\nThe Lindbladians introduced above satisfy the following fixed-point relations:\nL\u03b2[\u03c1\u03b2] = 0; L\u221e[\u03c1\u221e] = 0; Lsec[\u03c1sec] = 0, (C11)\nwhere the final equation can be viewed as a definition of \u03c1sec.\nWe now have by the triangle inequality that\n\u2225\u03c1\u221e \u2212 \u03c1\u03b2\u22251 \u2264 \u2225\u03c1\u221e \u2212 \u03c1sec\u22251 + \u2225\u03c1sec \u2212 \u03c1\u03b2\u22251 (C12)\n17Of the remaining two expressions in Eq. (C12), \u2225\u03c1\u221e \u2212 \u03c1sec\u22251 can be bounded using Lemma II.1 of Ref.\n26, yielding\n\u2225\u03c1\u221e \u2212 \u03c1sec\u22251 \u2264 4\u2225L\u221e \u2212 Lsec\u22251\u22121 \u00b7 tmix(L\u221e). (C13)\nAs we will frequently do, we can use H\u00a8 older\u2019s inequality to relate p \u2212 p norms of superoperators Q[\u00b7] =\n\u2212i[Q, \u00b7]\u00b1 whose sole action is commutation ( \u2212) or anticommutation (+) with some operator Q to the\noperator norm of Q, yielding 2\n\u2225\u03c1\u221e \u2212 \u03c1sec\u22251 \u2264 8\u2225B \u2212 Bsec\u2225\u221e \u00b7 tmix(L\u221e). (C14)\nThe second piece is trickier to bound, and requires a use of eigenvector perturbation theory. Specifically,\nwe use the following identity from Ref. 26, Appendix E: given two linear [super]operators 3 M, M\u2032 (such\nthat M\u2032 is a \u2018small\u2019 perturbation of M), with eigenvectors |v\u27e9, |v\u2032\u27e9 corresponding to eigenvalues \u03bb, \u03bb\u2032, we\nhave\n\u2225|v\u27e9 \u2212 |v\u2032\u27e9\u22251 \u2264 2\n\u221a\n2\u2225M\u2032 \u2212 M\u22251\u22121 + |\u03bb \u2212 \u03bb\u2032|\n\u03b6\u22122(M \u2212\u03bbI) , (C15)\nwhere \u03b6\u22122(Q) is the second smallest singular value of Q.\nAlthough in general eigenvector perturbation theory is poorly behaved for generic non-Hermitian operators\n(such as M, M\u2032), it is possible to choose both to share a common eigenvalue \u03bb = \u03bb\u2032 = 0: simply choose\nsuperoperators M, M\u2032 which have \u03c1sec and \u03c1\u03b2 as fixed points (eigenstates with eigenvalue 0). We can then\nrelate the 1-norm distance between the two density matrices to the difference of the superoperators. There is\na natural choice for M = Lsec. However, there is a wide range of possible superoperators that can be chosen\nto have \u03c1\u03b2 as a fixed point, due to the latter\u2019s central role in defining detailed balance. The nontrivial idea\nintroduced in Ref. 26 is that this flexibility can be leveraged to obtain a tight bound. An elegant choice is\nM\u2032[\u00b7 ] = \u03c11/2\n\u03b2 L\u2020\n2[\u03c1\u22121/2\n\u03b2 \u00b7 \u03c1\u22121/2\n\u03b2 ]\u03c11/2\n\u03b2 , (C16)\nwith L\u2020\n2[\u00b7 ] = \u2212i[Bsec, \u00b7 ] +L\u2020\n\u03b2[\u00b7 ]. Observe that M\u2032[\u03c1\u03b2] = \u03c11/2\n\u03b2 L\u2020\n2[1]\u03c11/2\n\u03b2 = 0, by using the fact that L\u2020\n2[1] = 0\nas long as L2 is a valid Lindbladian.\nWe then have, using the definition Eq. (C10) of Lsec and by rewriting the detailed balance condition as\nL\u03b2[\u00b7 ] \u2212 \u03c11/2\n\u03b2 L\u2020\n\u03b2[\u03c1\u22121/2\n\u03b2 (\u00b7 )\u03c1\u22121/2\n\u03b2 ]\u03c11/2\n\u03b2 = 0 that\n(M\u2032 \u2212 M)[\u00b7 ] = \u03c11/2\n\u03b2 L\u2020\n2[\u03c1\u22121/2\n\u03b2 (\u00b7 )\u03c1\u22121/2\n\u03b2 ]\u03c11/2\n\u03b2 \u2212 Lsec[\u00b7 ]\n= \u2212i\u03c11/2\n\u03b2 [Bsec, \u03c1\u22121/2\n\u03b2 (\u00b7 )\u03c1\u22121/2\n\u03b2 ]\u03c11/2\n\u03b2 + \u03c11/2\n\u03b2 L\u2020\n\u03b2[\u03c1\u22121/2\n\u03b2 (\u00b7 )\u03c1\u22121/2\n\u03b2 ]\u03c11/2\n\u03b2 + i[Bsec, \u00b7 ] \u2212 L\u03b2[\u00b7 ]\n= i\nh\u0010\nBsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2\n\u0011\n(\u00b7 ) \u2212 (\u00b7 )\n\u0010\nBsec \u2212 \u03c1\u22121/2\n\u03b2 Bsec\u03c11/2\n\u03b2\n\u0011i\n(C17)\nUsing the triangle and H\u00a8 older inequalities again, we then have\n\u2225M\u2032 \u2212 M\u22251\u22121 \u2264\n\r\r\rBsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2\n\r\r\r\n\u221e\n+\n\r\r\rBsec \u2212 \u03c1\u22121/2\n\u03b2 Bsec\u03c11/2\n\u03b2\n\r\r\r\n\u221e\n= 2\n\r\r\rBsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2\n\r\r\r\n\u221e\n,\n(C18)\nwhere in the second line we have used \u2225Q\u2225\u2217 =\n\r\rQ\u2020\r\r\n\u2217, with Q = Bsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2 .\n2 This follows since \u2225Q\u2225p\u2212p \u2261 supX\u0338=0\n\u2225Q[X]\u2225p\n\u2225X\u2225p\n=\n\u2225\u2212i[Q,X]\u00b1\u2225p\n\u2225X\u2225p\n\u2264 supX\u0338=0\n2\u2225Q\u2225\u221e\u2225X\u2225p\n\u2225X\u2225p\n= \u2225Q\u2225\u221e where we\nused the triangle inequality to rewrite the commutator in\nterms of a product and then applied H\u00a8 older\u2019s inequality to\nrewrite the numerator.\n3 The original argument in Ref. 26 is in terms of matrices, but\nwe can view these as superoperators written in the doubled\nspace where density matrices are vectorized, so we give the\nargument in terms of superoperators here for brevity.\n18Using the Fan-Hoffmann inequality [69, Proposition III.5.1] on the second-smallest singular value 4 of M\n(not necessarily Hermitian) we have\n\u03b6\u22122(M \u2212\u03bbI) \u2265 \u2212\u03bb2\n\u0012M + M\u2020\n2 \u2212 Re(\u03bb)I\n\u0013\n= \u2212\u03bb2 (Ldiss \u2212 Re(\u03bb)I) (C19)\nwhere \u03bb2(Q) denotes the second largest eigenvalue of Q. Note that in the final step we have simply used Eq.\n(C10) with M = Lsec and recognized that the coherent parts of any Lindbladian L cancels in L + L\u2020.\nFinally, we use the results of Ref. 26, Appendix E, to relate \u03bb2(Lsec) to the Lindbladian gap \u2206( Ldiss),\nand therefore the mixing time, via\n\u2212\u03bb2(Ldiss) \u2265 4\n\u221a\n2\n4\n\u221a\n2 + 1\u2206(Ldiss) \u2265 4\n\u221a\n2\n4\n\u221a\n2 + 1\nln(2)\ntmix(Ldiss). (C20)\nWe can now assemble a bound on the second term in Eq. (C12), by using Eq. (C15) with \u03bb = \u03bb\u2032 = 0 and\nthe results in Eq. (C18), Eq. (C19), and Eq. (C20):\n\u2225\u03c1sec \u2212 \u03c1\u03b2\u22251 \u2264 4\n\u221a\n2 + 1\nln 2\n\r\r\rBsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2\n\r\r\r\n\u221e\n\u00b7 tmix(Ldiss). (C21)\nPutting together Eq. (C14) and Eq. (C21), we finally arrive at a bound on the error of the fixed point of\nlocal-driving sampler in terms of secular-approximable quantities and mixing times\n\u2225\u03c1\u221e \u2212 \u03c1\u03b2\u22251 \u2264 8\u2225B \u2212 Bsec\u2225\u221e \u00b7 tmix(L\u221e) + 4\n\u221a\n2 + 1\nln 2\n\r\r\rBsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2\n\r\r\r\n\u221e\n\u00b7 tmix(Ldiss)\n\u2264 10\n\u0010\n\u2225B \u2212 Bsec\u2225\u221e +\n\r\r\rBsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2\n\r\r\r\n\u221e\n\u0011\n\u00b7 max(tmix(L\u221e), tmix(Ldiss)). (C22)\nWe now turn to computing these norms using the secular approximation, which will allow us to bound the\nterms in the RHS of Eq. (C22).\nb. Implementing the Secular Approximation.\nThe secular approximation is most transparently phrased in frequency space (although bounding var-\nious contributions is often easier in the time domain). Consider a generic operator of the form B =P\n\u03bd1,\u03bd2\u2208B b\u03bd1,\u03bd2 (A\u03bd2 )\u2020A\u03bd1 . The secular approximation of B is obtained by restricting the sum to the \u201calmost\ndiagonal\u201d pieces, such that |\u03bd1 \u2212 \u03bd2| \u2272 \u00b5, where \u00b5 is some suitably chosen cutoff. Formally, we do this by\nmultiplying the kernel b\u03bd1,\u03bd2 by a suitably chosen \u201cbump function\u201d,\nB =\nX\n\u03bd1,\u03bd2\u2208B\nb\u03bd1,\u03bd2 (A\u03bd2 )\u2020A\u03bd1 =\u21d2 Bsec =\nX\n\u03bd1,\u03bd2\u2208B\nb\u03bd1,\u03bd2 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\n(A\u03bd2 )\u2020A\u03bd1 (C23)\nwhere \u03bd\u2212 \u2261 \u03bd1 \u2212 \u03bd2 and w(x) is a smooth (i.e., infinitely differentiable) function with\nw(x) =\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\n1, x = 0\n0, |x| > 1\n< 1, else\n. (C24)\n4 Note that for any n \u00d7 n matrix A, \u03b6j(A) = \u03b6j(\u2212A) \u2265\n\u03bbj\n\u0000\n\u22121\n2 (A + A\u2020)\n\u0001\n= \u2212\u03bbn\u2212j+1\n\u0000 1\n2 (A + A\u2020)\n\u0001\n, which yields\nthe identity in Eq. (C19) if indexing is understood to be\nmodulo n.\n19For our purposes, it is convenient to choose a bump function such that it remains close to 1 except in some\nfinite interval near the boundaries at \u00b11. While a specific choice will not be essential to our argument\n(modulo this requirement) a concrete choice is\nw(x) = w1\n\u0012x + 1\n1 \u2212 \u03bb\n\u0013\nw1\n\u00121 \u2212 x\n1 \u2212 \u03bb\n\u0013\n. (C25)\nwhere we define w1(x) in terms of another auxiliary function\nw1(x) = w2(x)\nw2(x) + w2(1 \u2212 x), w 2(x) =\n(\nexp\n\u0000\n\u22121\nx\n\u0001\n, x > 0\n0, else. (C26)\nNote that w is zero outside of [ \u22121, 1] and one in the interval [ \u2212\u03bb, \u03bb]. By choosing \u03bb close to one, we can\nmake the function arbitrarily sharp.\nThe reason to implement the secular approximation is that in the limit where we take \u00b5 \u2192 0, the secular\npiece is purely diagonal in Bohr frequencies and hence commutes with \u03c1\u03b2. An important point is that,\ndepending on the specific function we wish to approximate, a sharp cutoff in frequencies may prove difficult\nto bound; implementing the secular approximation then requires a judicious use of bump functions (smooth\nfunctions that are strictly vanishing outside a compact domain) in order to remove high-frequency terms.\nThis will be the case for our coherent parts.\nEvidently, in order to use the secular approximation we must bound the two separate terms in Eq. (C22):\nthe \u201calmost-commuting\u201d piece Bsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2 and the non-secular piece B \u2212 Bsec. The strategy of\nproving these bounds is distinct, so we tackle them in turn.\nc. Bounding the Almost-Commuting Part.\nFirst, we consider the \u2018almost commuting\u2019 piece\n\r\r\rBsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2\n\r\r\r\n\u221e\nNote that we can always choose to write any operator in terms of its Bohr frequency expansion as B =P\n\u03bd\u2208B B\u03bd. Using the secular approximation, it then follows that\n\r\r\rBsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2\n\r\r\r\n\u221e\n=\n\r\r\r\r\r\nX\n\u03bd\u2208B\nw\n\u0012\u03bd\n\u00b5\n\u0013\u0010\n1 \u2212 e\u2212\u03b2\u03bd\n2\n\u0011\nB\u03bd\n\r\r\r\r\r\n\u221e\n. (C27)\nWe now exploit the fact that w\n\u0010\n\u03bd\n\u00b5\n\u0011\u0010\n1 \u2212 e\u2212\u03b2\u03bd\n2\n\u0011\n< \u03b2\u00b5for \u03b2\u00b5 \u2264 1, which places a restriction on the choice\nof \u00b5. From this, it follows that\n\r\r\rBsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2\n\r\r\r\n\u221e\n\u2264 O\n \r\r\r\r\r\nX\n\u03bd\u2208B\nB\u03bd\n\r\r\r\r\r\n\u221e\n\u03b2\u00b5\n!\n= O (\u2225B\u2225\u221e\u03b2\u00b5) . (C28)\nSince B = HLS \u2212 G, the final step in bounding the almost-commuting piece is therefore to bound the norms\nof the operators that appear in the coherent evolution of the exact sampler and the local driving sampler\n(the latter being the Lamb shift), which we may reduce to bounding the norm of each individually. For\nboth cases, we can show that this is O(1), by writing the relevant operator as a time domain integral of an\nexpression quadratic in the jump operators multiplied by some weight function that depends quadratically\non the smoothing functions. Since the jump operators can always be fixed to have unit norm by a suitable\nchoice of normalization, we can use triangle inequalities to simplify the bound to one in terms of time domain\nintegrals over the weight functions. For the Lamb shift the time domain representation is shown in Eq. (23d);\nthe weight functions are just a product of filter functions and so resulting bound on the norm is clearly O(1)\ndue to the normalization of the filter functions. For the exact coherent part, the introduction of the tanh \u03b2\u03bd\u2212\n4\ncomplicates the weight functions. It is straightforward but tedious to show [cf. Ref. 27, Appendix A] that\nG =\nZ \u221e\n\u2212\u221e\n\u02dcg\u2212(t\u2212)e\u2212iHt\u2212\n\u0012Z \u221e\n\u2212\u221e\n\u02dcg+(t+)A(t+)A(\u2212t+)dt+\n\u0013\neiHt\u2212dt\u2212 (C29)\n20with\n\u02dcg\u2212(t) = 1\n2i\nZ \u221e\n\u2212\u221e\nd\u03bd\u2212 ei\u03bd\u2212t\u2212g\u2212(\u03bd\u2212) =\n\uf8ee\n\uf8f0 2\n\u221a\n2\u03c0\n\u03b2 cosh\n\u0010\n2\u03c0t\u2212\n\u03b2\n\u0011\n\uf8f9\n\uf8fb \u2217t\u2212\n\"\u221a\n2\n\u03c3 e\n\u03b22\u221216t2\n\u2212\n4\u03c32 sin\n\u00122\u03b2t\u2212\n\u03c32\n\u0013#\n(C30)\nand\n\u02dcg+(t) =\nZ \u221e\n\u2212\u221e\nd\u03bd+ ei\u03bd+t+ g+(\u03bd+) = 4\u221a\u03c0e\u2212 2\n\u03c32 (t+\u2212i\u03b2\n4 )\n2\n\u03c3 . (C31)\nWith this in hand, we can use triangle inequalities to write\n\u2225G\u2225\u221e \u2264\n\u0012Z \u221e\n\u2212\u221e\ndt+|\u02dcg+(t+)|\n\u0013\u0012Z \u221e\n\u2212\u221e\ndt\u2212|\u02dcg\u2212(t\u2212)|\n\u0013\n. (C32)\nSince g+(t+) is a shifted Gaussian, its integral is bounded. Now, g\u2212(t\u2212) is the convolution of two bounded\nfunctions, and we can use Young\u2019s convolution inequality for the L1 norm to bound the norm of this convo-\nlution terms of the product of the norms. In each case due to the choice of overall normalization the bound\nis O(1). Combining the result above, we find that for a single jump operator,\n\r\r\rBsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2\n\r\r\r\n\u221e\n= O(\u03b2\u00b5) (C33)\nFinally, for multiple jump operators nB = |A| the errors add independently, so that we finally have\n\r\r\rBsec \u2212 \u03c11/2\n\u03b2 Bsec\u03c1\u22121/2\n\u03b2\n\r\r\r\n\u221e\n= O(nB\u03b2\u00b5). (C34)\nd. Bounding the Non-Secular Part.\nThe operator B that appears in the error bounds involves the difference between the coherent parts of the\nlocal driving sampler (the Lamb shift) and the exact sampler. A sufficient bound obtains by simply using\nthe triangle inequality to bound these pieces separately. Therefore will proceed initially being agnostic to\nthe form of B, making general arguments as to the form of the kernel such that the non-secular part B \u2212Bsec\nis bounded, before we specialize to the form dictated by our choice of filter functions.\nConsider the non-secular piece\nB \u2212 Bsec =\nX\n\u03bd1,\u03bd2\u2208B\nb\u03bd1,\u03bd2\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\n(A\u03bd2 )\u2020A\u03bd1 (C35)\nNow, let us define a time-domain kernel W(t1, t2) implicitly via\nb\u03bd1,\u03bd2\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\n\u2261\nZ \u221e\n\u2212\u221e\ndt1\nZ \u221e\n\u2212\u221e\ndt2 W(t1, t2)ei\u03bd1t1\u2212i\u03bd2t2 . (C36)\nUsing Eq. (C36) in our expression for the non-secular part, we have\nB \u2212 Bsec =\nX\n\u03bd1,\u03bd2\u2208B\nZ \u221e\n\u2212\u221e\ndt1\nZ \u221e\n\u2212\u221e\ndt2 W(t1, t2)ei\u03bd1t1\u2212i\u03bd2t2 (A\u03bd2 )\u2020A\u03bd1\n=\nZ \u221e\n\u2212\u221e\ndt1\nZ \u221e\n\u2212\u221e\ndt2 W(t1, t2)A\u2020(t2)A(t1) (C37)\nwhere we have simply used the representation of time evolution in terms of Bohr frequencies. The reason for\nthis step is that the A(t) are obtained by time-evolving bounded local operators, and therefore their norm\n21has some O(1) bound (which can be set to be 1 by a suitable choice of normalization, which we henceforth\nassume). We then find, using Eq. (C37) and the triangle inequality, that the non-secular part can be\nbounded by a certain two-dimensional time domain integral:\n\u2225B \u2212 Bsec\u2225\u221e \u2264\n\f\f\f\f\nZ \u221e\n\u2212\u221e\ndt1\nZ \u221e\n\u2212\u221e\ndt2 W(t1, t2)\n\f\f\f\f. (C38)\nNow, we can invert Eq. (C36) by recognizing that it is just a Fourier transformation, so that\nW(t1, t2) =\nZ \u221e\n\u2212\u221e\nd\u03bd1\n2\u03c0\nZ \u221e\n\u2212\u221e\nd\u03bd2\n2\u03c0 b\u03bd1,\u03bd2\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\ne\u2212i\u03bd1t1+i\u03bd2t2 (C39)\nTo proceed, we need to consider the form of b\u03bd1,\u03bd2 . For now, we will simply assume that this has the form\nb\u03bd1,\u03bd2 = b+(\u03bd1 + \u03bd2)b\u2212(\u03bd1 \u2212 \u03bd2); this will be the form of the kernel for both HLS,f and G. We then see, by\ncombining Eq. (C38) (with a judicious sign change) with Eq. (C39), and defining t\u00b1 = t1\u00b1t2\n2 , \u03bd\u00b1 = \u03bd1 \u00b1 \u03bd2,\nthat\n\u2225B \u2212 Bsec\u2225\u221e \u2264\n\f\f\f\f\nZ \u221e\n\u2212\u221e\ndt1\nZ \u221e\n\u2212\u221e\ndt2 W(t1, t2)\n\f\f\f\f\n=\n\f\f\f\f\nZ \u221e\n\u2212\u221e\ndt1\nZ \u221e\n\u2212\u221e\ndt2 W(\u2212t1, t2)\n\f\f\f\f\n=\n\f\f\f\f\nZ \u221e\n\u2212\u221e\ndt1\nZ \u221e\n\u2212\u221e\ndt2\nZ \u221e\n\u2212\u221e\nd\u03bd1\n2\u03c0\nZ \u221e\n\u2212\u221e\nd\u03bd2\n2\u03c0 b+(\u03bd+)b\u2212(\u03bd\u2212)\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\nei\u03bd1t1+i\u03bd2t2\n\f\f\f\f\n\u2264\nZ \u221e\n\u2212\u221e\ndt+\n\f\f\f\f\nZ \u221e\n\u2212\u221e\nd\u03bd+\n2\u03c0 b+(\u03bd+)ei\u03bd+t+\n\f\f\f\f \u00d7\nZ \u221e\n\u2212\u221e\ndt\u2212\n\f\f\f\f\nZ \u221e\n\u2212\u221e\nd\u03bd\u2212\n2\u03c0 b\u2212(\u03bd\u2212)\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\nei\u03bd\u2212t\u2212\n\f\f\f\f\n\u2264\nZ \u221e\n\u2212\u221e\ndt+|cb+(t+)| \u00d7\nZ \u221e\n\u2212\u221e\ndt\u2212\n\f\f\f\f\nZ \u221e\n\u2212\u221e\nd\u03bd\u2212\n2\u03c0 b\u2212(\u03bd\u2212)\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\nei\u03bd\u2212t\u2212\n\f\f\f\f (C40)\nwhere cb+(t+) is the inverse Fourier transform of b+(\u03bd+).\nWe find that in the two cases of interest to us, |cb+(t+)| takes the simple form\n|cb+(t+)| = |ch+(t+)| = |cg+(t+)| = 1\u221a\u03c0\u03c3 e\u03b22/4\u03c32\ne\u22124t2\n+/\u03c32\n(C41)\nTherefore, we can express the first term in the product on the RHS of the final inequality Eq. (C40) above\nas\nZ \u221e\n\u2212\u221e\ndt+|cb+(t+)| = e\u03b22/4\u03c32\n2 (C42)\nin both the coherent and exact cases, which will only give an overall constant prefactor.\nPerhaps unsurprisingly, the nontrivial bound involves the integration over t\u2212 in Eq. (C40). It is useful to\nsplit this into a short-time contribution I1 with t \u2208 [\u2212t0, t0] and a long-time contribution I2 with |t| > t0,\nwith t0 to be specified below. The short-time piece is given by\nI1 \u2261\nZ t0\n\u2212t0\ndt\u2212\n\f\f\f\f\nZ \u221e\n\u2212\u221e\nd\u03bd\u2212\n2\u03c0 b\u2212(\u03bd\u2212)\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\nei\u03bd\u2212t\u2212\n\f\f\f\f \u2264 2t0\nZ \u221e\n\u2212\u221e\nd\u03bd\u2212\n2\u03c0 |b\u2212(\u03bd\u2212)|\n\f\f\f\f1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\f\f\f\f. (C43)\nGiven our choice of bump function, 1 \u2212 w\n\u0010\n\u03bd\u2212\n\u00b5\n\u0011\nis only nonzero for \u03bd\u2212 \u2273 \u00b5, where it is close to 1, so that\nthe integral is dominated by the tails, and we have\nZ \u221e\n\u2212\u221e\nd\u03bd\u2212\n2\u03c0 |b\u2212(\u03bd\u2212)|\n\f\f\f\f1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\f\f\f\f = O\n\u0012\n2\nZ \u221e\n\u00b5\nd\u03bd\u2212\n2\u03c0 |b\u2212(\u03bd\u2212)|\n\u0013\n= O\n \ne\u2212\u03c32\u00b52\n16\n\u03c32\u00b5\n!\n(C44)\n22where in the second step we have used the fact that both |b\u2212| \u2264e\u2212(\u03c3\u03bd\u2212)2/16 for either choice b\u2212 = g\u2212, h\u2212.\nCombining Eq. (C43) and Eq. (C44), we have\nI1 = O\n \nt0e\u2212\u03c32\u00b52\n16\n\u03c32\u00b5\n!\n(C45)\nWe turn next to the long-time contributions. By integrating by parts twice, we observe that\nZ \u221e\n\u2212\u221e\nd\u03bd\u2212\n2\u03c0 b\u2212(\u03bd\u2212)\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\nei\u03bd\u2212t\u2212 = \u2212 1\nt2\n\u2212\nZ \u221e\n\u2212\u221e\nd\u03bd\u2212\n2\u03c0\nd2\nd\u03bd2\n\u2212\n\u0014\nb\u2212(\u03bd\u2212)\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\u0015\nei\u03bd\u2212t\u2212. (C46)\nWe can then use this result to obtain a useful bound on the long-time contributions,\nI2 \u2261\nZ\nR\\[\u2212t0,t0]\ndt\u2212\n\f\f\f\f\nZ \u221e\n\u2212\u221e\nd\u03bd\u2212\n2\u03c0 b\u2212(\u03bd\u2212)\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\nei\u03bd\u2212t\u2212\n\f\f\f\f\n\u2264 2\nt0\nZ \u221e\n\u2212\u221e\nd\u03bd\u2212\n2\u03c0\n\f\f\f\f\nd2\nd\u03bd2\n\u2212\n\u0014\nb\u2212(\u03bd\u2212)\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\u0015\f\f\f\f.\n(C47)\nUsing the product rule, we have that\nd2\nd\u03bd2\n\u2212\n\u0014\nb\u2212(\u03bd\u2212)\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\u0015\n=\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\nb\u2032\u2032\n\u2212(\u03bd\u2212) + 1\n\u00b5b\u2032\n\u2212(\u03bd\u2212)w\u2032\n\u0012\u03bd\u2212\n\u00b5\n\u0013\n+ 1\n\u00b52 b\u2212(\u03bd\u2212)w\u2032\u2032\n\u0012\u03bd\u2212\n\u00b5\n\u0013\n,\n(C48)\nso that on inserting Eq. (C48) into Eq. (C47) and using the triangle inequality that\nI2 \u2264 2\nt0\nZ \u221e\n\u2212\u221e\nd\u03bd\u2212\n2\u03c0\n\u0012\f\f\f\f\n\u0012\n1 \u2212 w\n\u0012\u03bd\u2212\n\u00b5\n\u0013\u0013\nb\u2032\u2032\n\u2212(\u03bd\u2212)\n\f\f\f\f +\n\f\f\f\f\n1\n\u00b5b\u2032\n\u2212(\u03bd\u2212)w\u2032\n\u0012\u03bd\u2212\n\u00b5\n\u0013\f\f\f\f +\n\f\f\f\f\n1\n\u00b5b\u2212(\u03bd\u2212)w\u2032\u2032\n\u0012\u03bd\u2212\n\u00b5\n\u0013\f\f\f\f\n\u0013\n(C49)\nLet us now consider the two choices b\u2212 = h\u2212, g\u2212 corresponding to the local driving and exact samplers.\nWe have that h\u2212(\u03bd\u2212) \u221d e\u2212(\u03c3\u03bd\u2212)2/16, whereas h\u2212(\u03bd\u2212) \u221d tanh\n\u0010\n\u03b2\u03bd\u2212\n4\n\u0011\ne\u2212(\u03c3\u03bd\u2212)2/16. Crucially, the tanh that\nappears in the latter case has a bounded derivative everywhere, and so is relatively innocuous when inserted\ninto Eq. (C49). Therefore, after a scaling analysis (that we do not reproduce here as it is straightforward but\ntedious) we can show that each term in Eq. (C49) takes the form Lpoly( \u03b2\u03bd, \u03c3\u03bd) \u00d7 e\u2212(\u03c3\u03bd\u2212)2/16, where Lpoly\nrepresents some Laurent polynomial of bounded negative and positive degrees. Meanwhile, for our choice of\nbump function, w\u2032(\u03bd\u2212/\u00b5) and w\u2032\u2032(\u03bd\u2212/\u00b5) are both O(1) around \u03bd\u2212/\u00b5 \u223c 1 or vanish otherwise, whereas as\nstated before 1 \u2212 w(\u03bd\u2212/\u00b5) is only nonzero outside the bounded domain [ \u2212\u00b5, \u00b5]. Combining these results, it\nis evident that we can bound the various terms in Eq. (C49) as\nI2 = O\n\u0012 \u03c3\nt0\nLpoly(\u03b2\u00b5, \u03c3\u00b5)e\u2212(\u03c3\u00b5)2/16\n\u0013\n(C50)\nwhich results from considering that the integrals only receive contributions either near \u00b1\u00b5 or ( \u00b1\u00b5, \u221e),\nrescaling the integrand, and estimating the weight due to the Gaussian factors, and the \u03b2-dependence will\nonly appear for the exact sampler. Combining the bound Eq. (C42) on the \u03bd+ integral with the bounds Eq.\n(C45) and Eq. (C50) respectively and setting \u03c3 = t0\n\u2225B \u2212 Bsec\u2225\u221e = O\n\u0010\nLpoly(\u03b2\u00b5, \u03c3\u00b5)e\u2212(\u03c3\u00b5)2/16\n\u0011\n(C51)\nAgain, for the case of multiple jump operators nB = |A| the errors add independently, so that we have\n\u2225B \u2212 Bsec\u2225\u221e = O\n\u0010\nnB Lpoly(\u03b2\u00b5, \u03c3\u00b5)e\u2212(\u03c3\u00b5)2/16\n\u0011\n(C52)\n23Now, substituting Eq. (C52) and Eq. (C34) into Eq. (C22) we obtain\n\u2225\u03c1\u221e \u2212 \u03c1\u03b2\u22251 = O\n\u0010\nnB\nh\n\u03b2\u00b5 + Lpoly(\u03b2\u00b5, \u03c3\u00b5)e\u2212(\u03c3\u00b5)2/16\ni\nmax(tmix(L\u221e), tmix(Ldiss)\n\u0011\n(C53)\nwhich for some choice of \u00b5 = \u02dcO(\u03c3\u22121) (the \u02dcO now hiding extra-log factors necessary to suppress the second\nterm above) yields the desired result in Eq. (C9).\ne. Choice of Filter Functions and Kernels.\nTo complete the proof, we must obtain the relevant functions b+, b\u2212 for the two samplers in question, for\na specific filter function, and confirm that they satisfy the properties assumed in the proof above. While this\ncan be done for arbitrary sufficiently quickly decaying f(t), for concreteness here we present results for the\nspecific choice Eq. (25) in the main text.\nWith this choice, we see that the Lamb shift kernel in Eq. (23f) is given by (taking T \u2192 \u221eas we have\nthroughout the secular approximation, and dropping the superscript T)\n\u22122ih\u03bd1,\u03bd2 =\nZ \u221e\n\u2212\u221e\ndt1\nZ \u221e\n\u2212\u221e\ndt2 sgn(t1 \u2212 t2)f(t1)f\u2217(t2)ei\u03bd1t1\u2212i\u03bd2t2\n=\nZ \u221e\n\u2212\u221e\ndt1\nZ \u221e\n\u2212\u221e\ndt2 sgn(t1 + t2)f(t1)f\u2217(\u2212t2)ei\u03bd1t1 ei\u03bd2t2\n= 2\nZ \u221e\n\u2212\u221e\ndt+\nZ \u221e\n\u2212\u221e\ndt\u2212 sgn(t+)f(t+ + t\u2212)f\u2217(\u2212t+ + t\u2212)ei\u03bd+t+ ei\u03bd\u2212t\u2212 (C54)\nWe can further simplify the integrand as follows:\nf(t+ + t\u2212)f\u2217(\u2212t+ + t\u2212) = 2\n\u03c0\u03c32 exp\n(\n\u2212 2\n\u03c32\n\"\u0012\u0012\nt+ \u2212 i\u03b2\n4\n\u0013\n+ t\u2212\n\u00132\n+\n\u0012\u0012\nt+ \u2212 i\u03b2\n4\n\u0013\n\u2212 t\u2212\n\u00132#)\n= 2\n\u03c0\u03c32 exp\n\"\n\u2212 4\n\u03c32\n\u0012\nt+ \u2212 i\u03b2\n4\n\u00132#\nexp\n\u0014\n\u2212 4\n\u03c32 t2\n\u2212\n\u0015\n. (C55)\nFrom this, we obtain\nh\u03bd1,\u03bd2 = h+(\u03bd+)h\u2212(\u03bd\u2212). (C56)\nHere,\nh+(\u03bd+) = \u2212 1\n2i\nZ \u221e\n\u2212\u221e\ndt+\n2\u221a\u03c0\u03c3 sgn(t+) exp\n\"\n\u2212 4\n\u03c32\n\u0012\nt+ \u2212 i\u03b2\n4\n\u00132#\ne\u2212i\u03bd+t+ (C57)\nwhich gives us that\n|cb+(t+)| =\n\f\f\f\f\f\ni\u221a\u03c0\u03c3 sgn(t+) exp\n\"\n\u2212 4\n\u03c32\n\u0012\nt+ \u2212 i\u03b2\n4\n\u00132#\f\f\f\f\f = 1\u221a\u03c0\u03c3 e\u03b22/4\u03c32\ne\u22124t2\n+/\u03c32\n(C58)\nand hence satisfies the assumption of Gaussian decay at large t+. Meanwhile, we have\nh\u2212(\u03bd\u2212) =\nZ \u221e\n\u2212\u221e\ndt\u2212\u221a\u03c0\u03c3 e\u2212 4\n\u03c32 t2\n\u2212e\u2212i\u03bd\u2212t\u2212 = 1\n2e\u2212 1\n16 \u03c32\u03bd2\n\u2212. (C59)\nwhich has the form of a bounded function (in this case a constant) multiplied by a Gaussian.\n24For the exact sampler, instead, we have that \u03b1\u03bd1,\u03bd2 = exp\n\u0002\n\u2212(\u03c3\u03bd1)2/8 \u2212 (\u03c3\u03bd2)2/8 \u2212 \u03b2\u03bd1/4 \u2212 \u03b2\u03bd2/4\n\u0003\n, which\nyields\ng\u03bd1,\u03bd2 = \u2212 1\n2i tanh \u03b2(\u03bd1 \u2212 \u03bd2)\n4 \u03b1\u03bd2,\u03bd2 = g+(\u03bd+)g\u2212(\u03bd\u2212) (C60)\nwith\ng+(\u03bd+) = i\n2e\u2212\u03b2\u03bd+/4e\u2212\n(\u03c3\u03bd+)2\n16 and g\u2212(\u03bd\u2212) = tanh \u03b2\u03bd\u2212\n4 e\u2212\n(\u03c3\u03bd\u2212)2\n16 (C61)\nEvidently, g\u2212 once again has the form of a bounded function multiplying a Gaussian. Turning finally to the\nFourier transform fo g+, we have\n|cg+(t+)| =\n\f\f\f\f\ni\n2\nZ \u221e\n\u2212\u221e\nd\u03bd+\n2\u03c0 e\u2212\u03b2\u03bd+/4e\u2212\n(\u03c3\u03bd+)2\n16 ei\u03bd+t+\n\f\f\f\f\n=\n\f\f\f\f\f\ni\u221a\u03c0\u03c3 exp\n\"\n\u2212 4\n\u03c32\n\u0012\nt+ \u2212 i\u03b2\n4\n\u00132#\f\f\f\f\f\n= 1\u221a\u03c0\u03c3 e\u03b22/4\u03c32\ne\u22124t2\n+/\u03c32\n(C62)\nwhich is identical to |ch+(t+)| and in particular of exactly the form assumed in Eq. (C41).\nAppendix D: Bounding the error in dropping the \u201crewinding\u201d\nIn this appendix, we show that dropping the rewinding procedure in our protocol adds to the total error\nbound in a controllable way, namely that the error bound for the original protocol increases by a factor of\n2\u03c4mix [cf. Eq. (37) in the main text].\nConcretely, we consider a channel K\u2032 defined as K\u2032[\u03c3] \u2261 V K[\u03c3]V \u2020 for some unitary V , and a state \u03c1\u03b2 that\ninvariant under conjugation with V : V \u03c1\u03b2V \u2020 = \u03c1. We assume that fixed point of K, \u03c1, (K[\u03c1] = \u03c1) is close to\n\u03c1\u03b2: \u2225\u03c1 \u2212 \u03c1\u03b2\u22251 \u2264 \u03b4 for some \u03b4 >0. We then show that\n\u2225\u03c1\u2032 \u2212 \u03c1\u03b2\u22251 \u2264 4\u03b4\u03c4mix (D1)\nwhere \u03c1\u2032 is the fixed-point of K\u2032 and \u03c4mix the mixing time of K, respectively.\nAs preliminaries, observe that since the channel K is contracting w.r.t. to the Schatten 1-norm,\n\u2225K[\u03c1\u03b2] \u2212 \u03c1\u22251 = \u2225K[\u03c1\u03b2] \u2212 K[\u03c1]\u22251 \u2264 \u2225\u03c1\u03b2 \u2212 \u03c1\u22251 \u2264 \u03b4. (D2)\nFurthermore, because the Schatten 1-norm is preserved under unitary conjugation,\n\u2225K\u2032[\u03c1\u03b2] \u2212 \u03c1\u03b2\u22251 =\n\r\rV K[\u03c1\u03b2]V \u2020 \u2212 \u03c1\u03b2\n\r\r\n1 \u2264\n\r\rV (K[\u03c1\u03b2] \u2212 \u03c1) V \u2020\r\r\n1 +\n\r\rV (\u03c1 \u2212 \u03c1\u03b2) V \u2020\r\r\n1 \u2264 2\u03b4. (D3)\nNow, for the fixed point \u03c1\u2032 of K\u2032, we have\n\u2225\u03c1\u2032 \u2212 \u03c1\u03b2\u22251 = \u2225K\u2032[\u03c1\u2032] \u2212 \u03c1\u03b2\u22251 \u2264 \u2225K\u2032[\u03c1\u2032] \u2212 K\u2032[\u03c1\u03b2]\u22251 + \u2225K\u2032[\u03c1\u03b2] \u2212 \u03c1\u03b2\u22251 \u2264 \u03b7\u2225\u03c1\u2032 \u2212 \u03c1\u03b2\u22251 + 2\u03b4 (D4)\nwhere \u03b7 \u2208 [0, 1) is the contraction factor\n\u03b7\u2032 \u2261 sup\n\u03c9\u0338=\u03b6\n\u2225K\u2032[\u03c9] \u2212 K\u2032[\u03b6]\u22251\n\u2225\u03c9 \u2212 \u03b6\u22251\n= sup\n\u03c9\u0338=\u03b6\n\u2225K[\u03c9] \u2212 K[\u03b6]\u22251\n\u2225\u03c9 \u2212 \u03b6\u22251\n= \u03b7 (D5)\nwhere the second inequality follows from the definition of K. Note that \u03b7 <1 under the assumption that K\nis primitive (that is irreducible and aperiodic), i.e. that any initial state converges to a unique fixed point.\nRearranging Eq. (D4) then gives\n\u2225\u03c1\u2032 \u2212 \u03c1\u03b2\u22251 \u2264 2\u03b4\n1 \u2212 \u03b7 . (D6)\n25What is left to show to obtain Eq. (D1) is then to relate the contraction factor \u03b7 to the mixing time \u03c4mix.\nTo this end, note that the contraction factor lower bounds the second-largest absolute value in the spectrum\nof K, which in turn lower bounds the mixing time (see e.g. Lemma 30 of Ref. 51):\n1\n1 \u2212 \u03b7 \u2264 1\n1 \u2212 |\u03bb1| \u2264 2\u03c4mix (D7)\nSubstituting this above then yields the final result\n\u2225\u03c1\u2032 \u2212 \u03c1\u03b2\u22251 \u2264 2\u03b4\n1 \u2212 \u03b7 \u2264 4\u03b4\u03c4mix (D8)\n[1] S. Lloyd, Universal quantum simulators, Science\n273, 1073 (1996).\n[2] B. M. Terhal and D. P. DiVincenzo, Problem of\nequilibration and the computation of correlation\nfunctions on a quantum computer, Phys. Rev. A\n61, 022301 (2000).\n[3] A. Riera, C. Gogolin, and J. Eisert, Thermalization\nin nature and on a quantum computer, Physical Re-\nview Letters 108, 10.1103/physrevlett.108.080402\n(2012).\n[4] Y. Ge, A. Moln\u00b4 ar, and J. I. Cirac, Rapid adia-\nbatic preparation of injective projected entangled\npair states and gibbs states, Physical Review Let-\nters 116, 10.1103/physrevlett.116.080503 (2016).\n[5] E. Mozgunov and D. Lidar, Completely positive\nmaster equation for arbitrary driving and small level\nspacing, Quantum 4, 227 (2020).\n[6] S. McArdle, T. Jones, S. Endo, Y. Li, S. C. Ben-\njamin, and X. Yuan, Variational ansatz-based quan-\ntum simulation of imaginary time evolution, npj\nQuantum Information 5, 10.1038/s41534-019-0187-\n2 (2019).\n[7] O. Shtanko and R. Movassagh, Preparing thermal\nstates on noiseless and noisy programmable quan-\ntum processors (2021), arXiv:2112.14688.\n[8] A. Schuckert, A. Bohrdt, E. Crane, and M. Knap,\nProbing finite-temperature observables in quantum\nsimulators of spin systems with short-time dynam-\nics, Phys. Rev. B 107, L140410 (2023).\n[9] X. Mi et al., Stable quantum-correlated many-body\nstates through engineered dissipation, Science 383,\n1332 (2024).\n[10] J. Lloyd, A. Michailidis, X. Mi, V. Smelyanskiy,\nand D. A. Abanin, Quasiparticle cooling algorithms\nfor quantum many-body state preparation, arXiv\n10.48550/arxiv.2404.12175 (2024).\n[11] A. Deshpande, M. Hinsche, S. Najafi, K. Sharma,\nR. Sweke, and C. Zoufal, Dynamic parameterized\nquantum circuits: expressive and barren-plateau\nfree (2024), arXiv:2411.05760 [quant-ph].\n[12] M. Consiglio, Variational quantum algorithms for\ngibbs state preparation, in Numerical Computa-\ntions: Theory and Algorithms (Springer Nature\nSwitzerland, 2025) p. 56\u201370.\n[13] D. Poulin and P. Wocjan, Sampling from the ther-\nmal quantum gibbs state and evaluating partition\nfunctions with a quantum computer, Phys. Rev.\nLett. 103, 220502 (2009).\n[14] E. Bilgin and S. Boixo, Preparing thermal states\nof quantum systems by dimension reduction, Phys.\nRev. Lett. 105, 170405 (2010).\n[15] K. Temme, T. J. Osborne, K. G. Vollbrecht,\nD. Poulin, and F. Verstraete, Quantum metropolis\nsampling, Nature 471, 87 (2011).\n[16] P. Rall, C. Wang, and P. Wocjan, Thermal State\nPreparation via Rounding Promises, Quantum 7,\n1132 (2023).\n[17] M.-H. Yung and A. Aspuru-Guzik, A quan-\ntum\u2013quantum metropolis algorithm, Proceedings of\nthe National Academy of Sciences 109, 754 (2012).\n[18] A. N. Chowdhury and R. D. Somma, Quantum algo-\nrithms for gibbs sampling and hitting-time estima-\ntion, Quantum Information and Computation 17,\n41 (2017).\n[19] M. Motta, C. Sun, A. T. K. Tan, M. J. O\u2019Rourke,\nE. Ye, A. J. Minnich, F. G. S. L. Brand\u02dc ao, and\nG. K.-L. Chan, Determining eigenstates and ther-\nmal states on a quantum computer using quantum\nimaginary time evolution, Nature Physics 16, 205\n(2020).\n[20] Z. Holmes, G. Muraleedharan, R. D. Somma,\nY. Subasi, and B. S \u00b8ahino\u02d8 glu, Quantum algorithms\nfrom fluctuation theorems: Thermal-state prepara-\ntion, Quantum 6, 825 (2022).\n[21] P. Wocjan and K. Temme, Szegedy walk unitaries\nfor quantum maps, Communications in Mathemat-\nical Physics 402, 3201 (2023).\n[22] D. Zhang, J. L. Bosse, and T. Cubitt, Dissipative\nquantum gibbs sampling (2023), arXiv:2304.04526.\n[23] J. Jiang and S. Irani, Quantum metropolis sampling\nvia weak measurement (2024), arXiv:2406.16023.\n[24] M. J. Kastoryano and F. G. S. L. Brand\u02dc ao, Quan-\n26tum gibbs samplers: The commuting case, Commu-\nnications in Mathematical Physics 344, 915 (2016).\n[25] C.-F. Chen and F. G. S. L. Brand\u02dc ao, Fast Ther-\nmalization from the Eigenstate Thermalization Hy-\npothesis, arXiv 10.48550/arxiv.2112.07646 (2021).\n[26] C.-F. Chen, M. J. Kastoryano, F. G. S. L. Brand\u02dc ao,\nand A. Gily\u00b4 en, Quantum Thermal State Prepara-\ntion, arXiv 10.48550/arxiv.2303.18224 (2023).\n[27] C.-F. Chen, M. J. Kastoryano, and A. Gily\u00b4 en, An\nefficient and exact noncommutative quantum Gibbs\nsampler, arXiv 10.48550/arxiv.2311.09207 (2023).\n[28] A. Gily\u00b4 en, C.-F. Chen, J. F. Doriguello, and M. J.\nKastoryano, Quantum generalizations of glauber\nand metropolis dynamics (2024), arXiv:2405.20322.\n[29] Z. Ding, B. Li, and L. Lin, Efficient quantum Gibbs\nsamplers with Kubo\u2013Martin\u2013Schwinger detailed\nbalance condition, arXiv 10.48550/arxiv.2404.05998\n(2024).\n[30] E. B. Davies, Markovian master equations, Commu-\nnications in Mathematical Physics 39, 91 (1974).\n[31] E. B. Davies, Markovian master equations. ii, Math-\nematische Annalen 219, 147 (1976).\n[32] J. Guo, O. Hart, C.-F. Chen, A. J. Friedman, and\nA. Lucas, Designing open quantum systems with\nknown steady states: Davies generators and beyond,\nQuantum 9, 1612 (2025).\n[33] G. S. Agarwal, Open quantum markovian systems\nand the microreversibility, Zeitschrift f\u00a8 ur Physik A\nHadrons and nuclei 258, 409 (1973).\n[34] R. Alicki, On the detailed balance condition for\nnon-hamiltonian systems, Reports on Mathematical\nPhysics 10, 249 (1976).\n[35] A. Kossakowski, A. Frigerio, V. Gorini, and\nM. Verri, Quantum detailed balance and kms condi-\ntion, Communications in Mathematical Physics 57,\n97 (1977).\n[36] F. Fagnola and V. Umanita, Generators of detailed\nbalance quantum markov semigroups, Infinite Di-\nmensional Analysis, Quantum Probability and Re-\nlated Topics 10, 335 (2007).\n[37] G. E. Santoro, R. Martonak, E. Tosatti, and R. Car,\nTheory of quantum annealing of an ising spin glass,\nScience 295, 2427 (2002).\n[38] B. Altshuler, H. Krovi, and J. Roland, Anderson\nlocalization makes adiabatic quantum optimization\nfail, Proceedings of the National Academy of Sci-\nences 107, 12446 (2010).\n[39] V. Bapst, L. Foini, F. Krzakala, G. Semerjian, and\nF. Zamponi, The quantum adiabatic algorithm ap-\nplied to random optimization problems: The quan-\ntum spin glass perspective, Physics Reports 523,\n127 (2013).\n[40] T. Rakovszky, B. Placke, N. P. Breuckmann, and\nV. Khemani, Bottlenecks in quantum channels\nand finite temperature phases of matter (2024),\narXiv:2412.09598.\n[41] B. Placke, T. Rakovszky, N. P. Breuckmann, and\nV. Khemani, Topological quantum spin glass or-\nder and its realization in qldpc codes (2024),\narXiv:2412.13248.\n[42] D. Gamarnik, B. T. Kiani, and A. Zlokapa,\nSlow mixing of quantum gibbs samplers (2024),\narXiv:2411.04300.\n[43] E. R. Anschuetz, Efficient learning implies quantum\nglassiness (2025), arXiv:2505.00087.\n[44] H. Chen, B. Li, J. Lu, and L. Ying, A randomized\nmethod for simulating lindblad equations and ther-\nmal state preparation (2024), arXiv:2407.06594.\n[45] E. Brunner, L. Coopmans, G. Matos,\nM. Rosenkranz, F. Sauvage, and Y. Kikuchi,\nLindblad engineering for quantum Gibbs state\npreparation under the eigenstate thermaliza-\ntion hypothesis, arXiv 10.48550/arxiv.2412.17706\n(2024).\n[46] H.-P. Breuer and F. Petruccione, The theory of open\nquantum systems (Oxford University Press, 2002).\n[47] L. A. Correa and J. Glatthard, Potential renormal-\nisation, lamb shift and mean-force gibbs state \u2013 to\nshift or not to shift? (2023), arXiv:2305.08941.\n[48] E. Schr\u00a8 odinger,Statistical Thermodynamics(Dover\nPublications, Mineola, NY, 1967).\n[49] G. Lindblad, On the generators of quantum dynam-\nical semigroups, Communications in Mathematical\nPhysics 48, 119 (1976).\n[50] K. Sharma and M. C. Tran, Hamiltonian simulation\nin the interaction picture using the magnus expan-\nsion (2024), arXiv:2404.02966 [quant-ph].\n[51] M. J. Kastoryano and K. Temme, Quantum loga-\nrithmic sobolev inequalities and rapid mixing, Jour-\nnal of Mathematical Physics 54, 052202 (2013).\n[52] K. Temme, M. J. Kastoryano, M. B. Ruskai, M. M.\nWolf, and F. Verstraete, The \u03c72 - divergence and\nmixing times of quantum markov processes, arXiv\n10.48550/arxiv.1005.2358 (2010).\n[53] M. \u02c7Znidari\u02c7 c, Relaxation times of dissipative many-\nbody quantum systems, Phys. Rev. E 92, 042143\n(2015).\n[54] I. Bardet, A. Capel, L. Gao, A. Lucia, D. P\u00b4 erez-\nGarc\u00b4 \u0131a, and C. Rouz\u00b4 e, Rapid thermalization of spin\nchain commuting hamiltonians, Phys. Rev. Lett.\n130, 060401 (2023).\n[55] J. Kochanowski, A. M. Alhambra, A. Capel, and\nC. Rouz\u00b4 e, Rapid thermalization of dissipative many-\nbody dynamics of commuting hamiltonians (2024),\narXiv:2404.16780.\n[56] Y. Tong and Y. Zhan, Fast mixing of weakly inter-\nacting fermionic systems at any temperature (2024),\narXiv:2501.00443.\n[57] Z. Ding, Z. Landau, B. Li, L. Lin, and\nR. Zhang, Polynomial-time preparation of low-\ntemperature gibbs states for 2d toric code (2024),\narXiv:2410.01206.\n[58] \u02c7St\u02c7 ep\u00b4 an\u02c7Sm\u00b4 \u0131d, R. Meister, M. Berta, and R. Bonde-\nsan, Polynomial time quantum gibbs sampling for\nfermi-hubbard model at any temperature (2025),\narXiv:2501.01412.\n[59] C. Rouz\u00b4 e, D. S. Fran\u00b8 ca, and \u00b4Alvaro M. Alham-\nbra, Efficient thermalization and universal quantum\n27computing with quantum gibbs samplers (2024),\narXiv:2403.12691.\n[60] C. Rouz\u00b4 e, D. S. Fran\u00b8 ca, and\u00b4Alvaro M. Alhambra,\nOptimal quantum algorithm for gibbs state prepa-\nration (2024), arXiv:2411.04885.\n[61] H. Kim and D. A. Huse, Ballistic spreading of entan-\nglement in a diffusive nonintegrable system, Phys.\nRev. Lett. 111, 127205 (2013).\n[62] J. Dormand and P. Prince, A family of embedded\nrunge-kutta formulae, Journal of Computational\nand Applied Mathematics 6, 19 (1980).\n[63] A. J. Daley, I. Bloch, C. Kokail, S. Flannigan,\nN. Pearson, M. Troyer, and P. Zoller, Practical\nquantum advantage in quantum simulation, Nature\n607, 667 (2022).\n[64] D. Hahn, S. A. Parameswaran, and B. Placke, in\npreparation (2025).\n[65] S. R. White, Minimally entangled typical quantum\nstates at finite temperature, Phys. Rev. Lett. 102,\n190601 (2009).\n[66] T. Kuwahara, T. Mori, and K. Saito, Flo-\nquet\u2013magnus theory and generic transient dynamics\nin periodically driven many-body quantum systems,\nAnnals of Physics 367, 96 (2016).\n[67] E. M. Stoudenmire and S. R. White, Minimally en-\ntangled typical thermal state algorithms, New Jour-\nnal of Physics 12, 055026 (2010).\n[68] M. Rigol, T. Bryant, and R. R. P. Singh, Numerical\nlinked-cluster approach to quantum lattice models,\nPhys. Rev. Lett. 97, 187202 (2006).\n[69] R. Bhatia, Variational principles for eigenvalues, in\nMatrix Analysis (Springer New York, New York,\nNY, 1997) pp. 57\u201383.\n28"
   ],
   "labels": [
      "manual",
      "manual",
      "manual",
      "manual",
      "manual",
      "manual",
      "report",
      "report",
      "report",
      "report",
      "report",
      "report",
      "research",
      "research",
      "research",
      "research",
      "research",
      "research"
   ]
}